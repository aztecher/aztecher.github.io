<!DOCTYPE HTML>
<html lang="ja" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>toyvmm book (ja)</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="introduction.html">Introduction</a></li><li class="chapter-item expanded "><a href="quickstart.html">QuickStart</a></li><li class="chapter-item expanded "><a href="01_running_tiny_code_in_vm.html">Running Tiny Code in VM</a></li><li class="chapter-item expanded "><a href="02_load_linux_kernel.html">Load Linux Kernel</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="02-1_overview_of_booting_linux.html">02-1. Overview of Booting Linux</a></li><li class="chapter-item expanded "><a href="02-2_elf_binary_format_and_vmlinux_structure.html">02-2. ELF binary format and vmlinux structure</a></li><li class="chapter-item expanded "><a href="02-3_load_initrd.html">02-3. Load initrd</a></li><li class="chapter-item expanded "><a href="02-4_setup_registers_of_vcpu.html">02-4. Setup registers of vcpu</a></li><li class="chapter-item expanded "><a href="02-5_serial_console_implementation.html">02-5. Serial Console implementation</a></li><li class="chapter-item expanded "><a href="02-6_toyvmm_implementation.html">02-6. ToyVMM implementation</a></li></ol></li><li class="chapter-item expanded "><a href="03_virtio.html">Virtio</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="03-1_virtio.html">03-1. Virtio</a></li><li class="chapter-item expanded "><a href="03-2_implement_virtio_in_toyvmm.html">03-2. Implement virtio in ToyVMM</a></li><li class="chapter-item expanded "><a href="03-3_virtio-net.html">03-3. Implement virtio-net device</a></li><li class="chapter-item expanded "><a href="03-4_virtio-blk.html">03-4. Implement virtio-blk device</a></li></ol></li><li class="chapter-item expanded "><a href="04_smp.html">SMP and CPU specific configuration</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="04-1_smp_symmetric_multiprocessing.html">04-1. SMP - Symmetric MultuProcessing</a></li><li class="chapter-item expanded "><a href="04-2_common_CPU_configuration.html">04-2. Common CPU configuration</a></li><li class="chapter-item expanded "><a href="04-3_Intel_CPU_specific_configuration.html">04-3. Intel CPU specific configuration</a></li><li class="chapter-item expanded "><a href="04-4_AMD_CPU_specific_configuration.html">04-4. AMD CPU specific configuration</a></li></ol></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">toyvmm book (ja)</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        <a href="https://github.com/aztecher/toyvmm" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                                                
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p><strong>Sorry, introdcution is currently available only in English</strong></p>
<h2 id="what-is-toyvmm"><a class="header" href="#what-is-toyvmm">What is ToyVMM?</a></h2>
<p>ToyVMM is a project being developed for the purpose of learning virtualization technology.
ToyVMM aims to accomplish the following</p>
<p>Code-based understanding of KVM-based virtualization technologies
Learn about the modern virtualization technology stack by using libraries managed by rust-vmm
The rust-vmm libraries are also used as a base for well-known OSS such as firecracker and provides the functionality needed to create custom VMMs.</p>
<h2 id="disclaimer"><a class="header" href="#disclaimer">Disclaimer</a></h2>
<p>While every effort has been made to provide correct information in this publication, the authors do not necessarily guarantee that all information is accurate.
Therefore, the authors cannot be held responsible for the results of development, prototyping, or operation based on this information.
If you find any errors in the contents of this document, please correct or report them as PR or Issue.</p>
<h2 id="whats-next"><a class="header" href="#whats-next">What's Next?</a></h2>
<p>If you would like to try ToyVMM first, please refer to <a href="./quickstart.html">QuickStart</a>.
To learn more about KVM-based virtualization through ToyVMM, please refer to <a href="./01_running_tiny_code_in_vm.html">01. Running Tiny Code in VM</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quickstart"><a class="header" href="#quickstart">QuickStart</a></h1>
<p><strong>Sorry, quickstart is currently available only in English</strong></p>
<p>This quickstart documents are based on the commit ID of <code>58cf0f68a561ee34a28ae4e73481f397f2690b51</code>.</p>
<h3 id="architecture--os"><a class="header" href="#architecture--os">Architecture &amp; OS</a></h3>
<p>ToyVMM only supports <strong>x86_64</strong> Linux for Guest OS.<br />
ToyVMM has been confirmed to work with Rocky Linux 8.6, 9.1 and Ubuntu 18.04, 22.04 as the Hypervisor OS.</p>
<h3 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h3>
<p>ToyVMM requires <a href="https://www.linux-kvm.org/page/Main_Page">the KVM Linux kernel module.</a></p>
<h3 id="run-virtual-machine-using-toyvmm"><a class="header" href="#run-virtual-machine-using-toyvmm">Run Virtual Machine using ToyVMM</a></h3>
<p>Following command builds toyvmm from source, downloads the kernel binary and rootfs needed to start the VM, and starts the VM.</p>
<pre><code class="language-bash"># download and build toyvmm from source.
git clone https://github.com/aztecher/toyvmm.git
cd toyvmm
mkdir build
CARGO_TARGET_DIR=./build cargo build --release

# Download a linux kernel binary.
wget https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/x86_64/kernels/vmlinux.bin

# Download a rootfs.
wget https://s3.amazonaws.com/spec.ccfc.min/ci-artifacts/disks/x86_64/ubuntu-18.04.ext4

# Run virtual machine based on ToyVMM!
sudo ./build/release/toyvmm vm run --config examples/vm_config.json
</code></pre>
<p>After the guest OS startup sequence is output, the login screen is displayed, so enter both username and password as 'root' to login.</p>
<h3 id="disk-io-in-virtual-machine"><a class="header" href="#disk-io-in-virtual-machine">Disk I/O in Virtual Machine.</a></h3>
<p>Since we have implemented virtio-blk, the virtual machine is capable of operating block devices.<br />
Now it recognizes the ubuntu18.04.ext4 disk image as a block device and mounts it as the root filesystem.</p>
<pre><code class="language-bash">lsblk
&gt; NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
&gt; vda  254:0    0  384M  0 disk /
</code></pre>
<p>Therefore, if you create a file in the VM and then recreate the VM using the same image, the file you created will be found.
This behavior is significantly different from a initramfs (rootfs that is extracted on RAM).</p>
<pre><code class="language-bash"># Create 'hello.txt' in VM.
echo &quot;hello virtual machine&quot; &gt; hello.txt
cat hello.txt
&gt; hello virtual machine

# Rebooting will cause the ToyVMM process to terminate.
reboot -f

# In the host, please restart VM and login again.
# Afterward, you can found the file you created in the VM during its previous run.
cat hello.txt
&gt; hello virtual machine
</code></pre>
<h3 id="network-io-in-virtual-mahcine"><a class="header" href="#network-io-in-virtual-mahcine">Network I/O in Virtual Mahcine.</a></h3>
<p>Since we have implemented virtio-net, the virtual machine is capable of operating network device.<br />
Now, it recognizes the <code>eth0</code> network interface.</p>
<pre><code class="language-bash">ip link show eth0
&gt; 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT group default qlen 1000
&gt;     link/ether 52:5f:7f:b3:f8:81 brd ff:ff:ff:ff:ff:ff
</code></pre>
<p>And toyvmm creates the host-side tap device named <code>vmtap0</code> that connect to the virtual machine interface.</p>
<pre><code class="language-bash">ip link show vmtap0
&gt; 334: vmtap0: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN mode DEFAULT group default qlen 1000
&gt;     link/ether 26:e9:5c:02:3c:19 brd ff:ff:ff:ff:ff:ff
</code></pre>
<p>Therefore, by assigning appropriate IP addresses to the interfaces on both the VM side and the Host side, communication can be established between the HV and the VM.</p>
<pre><code class="language-bash"># Assign ip address 192.168.0.10/24 to 'eth0' in vm.
ip addr add 192.168.0.10/24 dev eth0

# Assign ip address 192.168.0.1/24 to 'vmtap0' in host.
sudo ip addr add 192.168.0.1/24 dev vmtap0

# Host -&gt; VM. ping to VM interface ip from host.
ping -c 1 192.168.0.10

# VM -&gt; Host. Ping to Host interface ip from vm.
ping -c 1 192.168.0.1
</code></pre>
<p>Additionally, by setting the default route on the VM side, and configuring iptables and enabling IP forwarding on the host side, you can also allow the VM to access the Internet.<br />
However, this will not be covered in detail here.</p>
<h3 id="whats-next-1"><a class="header" href="#whats-next-1">What's next?</a></h3>
<p>If you are not familiar with KVM-based VMs, I suggest you start reading from <a href="./01_running_tiny_code_in_vm.html">01. Running Tiny Code in VM</a>.
If not, please read the topics that interest you.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="01-running-tiny-code-in-vm"><a class="header" href="#01-running-tiny-code-in-vm">01. Running Tiny Code in VM</a></h1>
<p><strong>Sorry, this contents is currently available only in English</strong></p>
<p>Tiny code execution is no longer supported in the current latest commit.</p>
<p>You may be able to verify it by checking out past commits, but please be aware that resolving package dependencies may be challenging.</p>
<p>This chapter is documented in a way that you can get a sense of its behavior without actually running it, so please feel reassured about that.</p>
<h3 id="deepdive-toyvmm-instruction-and-how-to-run-tiny-code-in-vm"><a class="header" href="#deepdive-toyvmm-instruction-and-how-to-run-tiny-code-in-vm">DeepDive ToyVMM instruction and how to run tiny code in VM</a></h3>
<p>This <code>main</code> function is a program that starts a VM using the KVM mechanism and executes the following small code inside the VM</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>code = &amp;[
	0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */
	0x00, 0xd8,       /* add %bl, %al */
	0x04, b'0',       /* add $'0', %al */
	0xee,             /* out %al, (%dx) */
	0xb0, '\n',       /* mov $'\n', %al */
	0xee,             /* out %al, (%dx) */
	0xf4,             /* hlt */
];
<span class="boring">}
</span></code></pre></pre>
<p>This code perform several register operations, but the initial state of the CPU regisers for this VM is set as follows.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>    regs.rip = 0x1000;
    regs.rax = 2;
    regs.rbx = 2;
    regs.rflags = 0x2;
    vcpu.set_sregs(&amp;sregs).unwrap();
    vcpu.set_regs(&amp;regs).unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>This will output the result of calculations (2 + 2) inside the VM from the IO Port, followed by a newline code as well.<br />
As you can see the result of running ToyVMM, hex value 0x34 (= '4') and 0xa (= New Line) are catched from I/O port</p>
<h3 id="hows-work-above-code-with-rust-vmm-libraries"><a class="header" href="#hows-work-above-code-with-rust-vmm-libraries">How's work above code with rust-vmm libraries</a></h3>
<p>Now, the following crate provided by rust-vmm is used to run these codes.</p>
<pre><code class="language-bash"># Please see Cargo.toml
kvm-bindings
kvm-ioctls
vmm-sys-util
vm-memory
</code></pre>
<p>I omit to describe about <a href="https://github.com/rust-vmm/vmm-sys-util">vmm-sys-util</a> because it is only used to create temporary files at this point, so there is nothing special to mention about it.</p>
<p>I will go through the code in order and describe how each crate is related to that.<br />
In this explanation, we will focus primary on the perspective of <strong>what ioctl is performed as a result of a function call</strong> (This is because the interface to manipulate KVM from the user space relies on the iocl system call)<br />
Also, please note that explanations of unimportant variables may be omitted.<br />
It should be noted that what is described here is not only the ToyVMM implementation, but also the firecracker implementation in a similaer form.</p>
<p>First, we need to open <code>/dev/kvm</code> and acquire the file descriptor. This can be done by <code>Kvm::new()</code> of <a href="https://github.com/rust-vmm/kvm-ioctls"><code>kvm_ioctls</code></a> crate. Following this process, the <a href="https://github.com/rust-vmm/kvm-ioctls/blob/d12f5776be0937a14da1ad8f9736653e8a2ad5ba/src/ioctls/system.rs#L69-L78"><code>Kvm::open_with_cloexec</code></a> function issues an <code>open</code> system call as follows, returns a file descriptor as <code>Kvm</code> structure</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let ret = unsafe { open(&quot;/dev/kvm\0&quot;.as_ptr() as *const c_char, open_flags) };
<span class="boring">}
</span></code></pre></pre>
<p>The result obtained from above is used to call the method <code>create_vm</code>, which results in the following <code>ioctl</code> being issued</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vmfd = ioctl(kvmfd, KVM_CREATE_VM, 0)

where
  vmfd: from /dev/kvm
<span class="boring">}
</span></code></pre></pre>
<p>Please keep in mind that the file descriptor returned from above function will be used later when preparing the CPU.<br />
Anyway, we finish to crete a VM but it has no memory, cpu.</p>
<p>Now, the next step is to prepare memory!
In <code>kvm_ioctls</code>'s example, memory is prepared as follows</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// First, setup Guest Memory using mmap
let load_addr: *mut u8 = unsafe {
    libc::mmap(
        null_mut(),
        mem_size, // 0x4000
        libc::PROT_READ | libc::PROT_WRITE,
        libc::MAP_ANONYMOUS | libc::MAP_SHARED | libc::MAP_NORESERVE,
        -1,
        0,
    ) as *mut u8
};

// Second, setup kvm_userspace_memory_region sructure using above memory
// kvm_userspace_memory_region is defined in kvm_bindings crate
let mem_region = kvm_userspace_memory_region {
    slot,
    guest_phys_addr: guest_addr,  // 0x1000
    memory_size: mem_size as u64, // 0x4000
    userspace_addr: load_addr as u64,
    flags: KVM_MEM_LOG_DIRTY_PAGES,
};
unsafe { vm.set_user_memory_region(mem_region).unwrap() };

// retrieve slice from pointer and length (slice::form_raw_parts_mut)
//   &gt; https://doc.rust-lang.org/beta/std/slice/fn.from_raw_parts_mut.html
// and write asm_code into this slice (&amp;[u8], &amp;mut [u8], Vec&lt;u8&gt; impmenent the Write trait!)
//   &gt; https://doc.rust-lang.org/std/primitive.slice.html#impl-Write
unsafe {
    let mut slice = slice::from_raw_parts_mut(load_addr, mem_size);
    slice.write(&amp;asm_code).unwrap();
}
<span class="boring">}
</span></code></pre></pre>
<p>Check <code>set_user_memory_region</code>. This function will issue the following ioctl as a result, attach the memory to VM</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ioctl(vmfd, KVM_SET_USER_MEMORY_REGION, &amp;mem_region)
<span class="boring">}
</span></code></pre></pre>
<p>ToyVMM, on the other hand, provides a utility functions for memory preparation.<br />
This difference is due to the fact that ToyVMM's implementation is similaer to firecracker's, but they are essentially doing the same thing.</p>
<p>Let's look at the whole implementation first</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// The following `create_region` functions operate based on file descriptor, so first, create a temporary file and write asm_code to it.
let mut file = TempFile::new().unwrap().into_file();
assert_eq!(unsafe { libc::ftruncate(file.as_raw_fd(), 4096 * 10) }, 0);
let code: &amp;[u8] = &amp;[
    0xba, 0xf8, 0x03, /* mov $0x3f8, %dx */
    0x00, 0xd8,       /* add %bl, %al */
    0x04, b'0',       /* add $'0', %al */
    0xee,             /* out %al, %dx */
    0xb0, b'\n',      /* mov $'\n', %al */
    0xee,             /* out %al, %dx */
    0xf4,             /* hlt */
];
file.write_all(code).expect(&quot;Failed to write code to tempfile&quot;);

// create_region funcion create GuestRegion (The details are described in the following)
let mut mmap_regions = Vec::with_capacity(1);
let region = create_region(
    Some(FileOffset::new(file, 0)),
    0x1000,
    libc::PROT_READ | libc::PROT_WRITE,
    libc::MAP_NORESERVE | libc::MAP_PRIVATE,
    false,
).unwrap();

// Vec named 'mmap_regions' contains the entry of GuestRegionMmap
mmap_regions.push(GuestRegionMmap::new(region, GuestAddress(0x1000)).unwrap());

// guest_memory represents as the vec of GuestRegion
let guest_memory = GuestMemoryMmap::from_regions(mmap_regions).unwrap();
let track_dirty_page = false;

// setup Guest Memory
vm.memory_init(&amp;guest_memory, kvm.get_nr_memslots(), track_dirty_page).unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>The <code>create_vm</code> consequently performs a mmap in the following way and returns a part of the structure (GuestMmapRegion) representing the GuestMemory</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn create_region(
    maybe_file_offset: Option&lt;FileOffset&gt;,
    size: usize,
    prot: i32,
    flags: i32,
    track_dirty_pages: bool,
) -&gt; Result&lt;GuestMmapRegion, MmapRegionError&gt; {

...

let region_addr = unsafe {
    libc::mmap(
        region_start_addr as *mut libc::c_void,
        size,
        prot,
        flags | libc::MAP_FIXED,
        fd,
        offset as libc::off_t,
    )
};
let bitmap = match track_dirty_pages {
    true =&gt; Some(AtomicBitmap::with_len(size)),
    false =&gt; None,
};
unsafe {
    MmapRegionBuilder::new_with_bitmap(size, bitmap)
        .with_raw_mmap_pointer(region_addr as *mut u8)
        .with_mmap_prot(prot)
        .with_mmap_flags(flags)
        .build()
}
<span class="boring">}
</span></code></pre></pre>
<p>Let's check the structure about Memory here.
In <code>src/kvm/memory.rs</code>, the following Memory structure is defined based on <a href="https://github.com/rust-vmm/vm-memory">vm-memory</a> crate</p>
<pre><code>pub type GuestMemoryMmap = vm_memory::GuestMemoryMmap&lt;Option&lt;AtomicBitmap&gt;&gt;;
pub type GuestRegionMmap = vm_memory::GuestRegionMmap&lt;Option&lt;AtomicBitmap&gt;&gt;;
pub type GuestMmapRegion = vm_memory::MmapRegion&lt;Option&lt;AtomicBitmap&gt;&gt;;
</code></pre>
<p>The <code>MmapRegionBuilder</code> is also defined in the <code>vm-memory</code> crate, and this <code>build</code> method creates the <code>MmapRegion</code>.</p>
<p>This time, since we have performed the mmap myself in advance and passed that address to <code>with_raw_mmap_pointer</code>, <a href="https://github.com/rust-vmm/vm-memory/blob/f6ef1b619b126324830c87a3554b7082a0490ae0/src/mmap_unix.rs#L145-L147">use that area to initialize</a>. Otherwise, <a href="https://github.com/rust-vmm/vm-memory/blob/f6ef1b619b126324830c87a3554b7082a0490ae0/src/mmap_unix.rs#L164-L173">mmap is performed in the <code>build</code> method</a>. In any case, this <code>build</code> method will get the <code>MmapRegion</code> structure, but defines a synonym as described above, which is returned as the <code>GuestMmapRegion</code>. By calling the <code>create_region</code> function once, you can allocate and obtain one region of GuestMemory based on the information(<code>size</code>, <code>flags</code>, ...etc) specified in the argument.</p>
<p>The region allocated here is only mmapped from the virtual address space of the VMM process, and no further information is available. To use this area as Guest Memory, a <code>GuestRegionMmap</code> structure is created from this area. This is simple, specify the corresponding <code>GuestAddress</code> for this region and initialize <code>GuestRegionMmap</code> with a tuple of mmapped area and GuestAddress. In following code, the initialized <code>GuestRegionMmap</code> is pushed to Vec for subsequent processing.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>map_regions.push(GuestRegionMmap::new(region, GuestAddress(0x1000)).unwrap());
<span class="boring">}
</span></code></pre></pre>
<p>Now, the <code>mmap_regions: Vec&lt;GuestRegionMmap&gt;</code> created as above represents the entire memory of the Guest VM, and each region that makes up the guest memory holds information on the area allocated by the VMM for the region and the top address of the Guest side.
The <code>GuestMemoryMmap</code> structure representing the Guest Memory is initialized from this Vec information and set to VM by the <code>memory_init</code> method.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let guest_memory = GuestMemoryMmap::from_regions(mmap_regions).unwrap();
vm.memory_init(&amp;guest_memory, kvm.get_nr_memslots(), track_dirty_page).unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>Next, let's check the operation of this <code>memory_init</code>. This calls <code>set_kvm_memory_regions</code> and the actual process is described there.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn set_kvm_memory_regions(
    &amp;self,
    guest_mem: &amp;GuestMemoryMmap,
    track_dirty_pages: bool,
    ) -&gt; Result&lt;()&gt; {
    let mut flags = 0u32;
    if track_dirty_pages {
        flags |= KVM_MEM_LOG_DIRTY_PAGES;
    }
    guest_mem
        .iter()
        .enumerate()
        .try_for_each(|(index, region)| {
            let memory_region = kvm_userspace_memory_region {
                slot: index as u32,
                guest_phys_addr: region.start_addr().raw_value() as u64,
                memory_size: region.len() as u64,
                userspace_addr: guest_mem.get_host_address(region.start_addr()).unwrap() as u64,
                flags,
            };
            unsafe { self.fd.set_user_memory_region(memory_region) }
        })
    .map_err(Error::SetUserMemoryRegion)?;
    Ok(())
}
<span class="boring">}
</span></code></pre></pre>
<p>Here we can see that <code>set_user_memory_region</code> is called using the necessary information while iterating the region.
In other words, it is processing the same as the example code except that there may be more than one region.</p>
<p>Now that we've gone through the explanation of memory preparation, let's take a look at the <code>vm-memory</code> crate!
The information presented here is only the minimum required, so please refer to <a href="https://github.com/rust-vmm/vm-memory/blob/main/DESIGN.md">Design</a> or other sources for more details.
This will also be related to the above iteration, where we were able to call methods such as <code>sart_addr()</code> and <code>len()</code> to construct the necessary information for <code>set_user_memory_region</code>.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>GuestAddress (struct) : Represent Guest Physicall Address (GPA)
FileOffset(struct) : Represents the start point within a 'File' that backs a 'GuestMemoryRegion'

GuestMemoryRegion(trait) : Represents a continuous region of guest physical memory / trait
GuestMemory(trait) : Represents a container for a immutable collection of GuestMemoryRegion object / trait

MmapRegion(struct) : Helper structure for working with mmaped memory regions
GuestRegionMmap(struct &amp; implement GuestMemoryRegion trait) : Represents a continuous region of the guest's physical memory that is backed by a mapping in the virtual address space of the calling process
GuestMemoryMmap(struct &amp; implement GuestMemory trait) : Represents the entire physical memory of the guest by tracking all its memory regions
<span class="boring">}
</span></code></pre></pre>
<p><a href="https://github.com/rust-vmm/vm-memory/blob/f6ef1b619b126324830c87a3554b7082a0490ae0/src/mmap.rs#L436">Since <code>GuestRegionMmap</code> implements the <code>GuestMemoryRegion</code> trait</a>, there are implementations of functions such as <code>start_addr()</code> and <code>len()</code>, which were used in the above interation.
The following figure briefly summarizes the relationship between these structures</p>
<img src="./01_figs/vm-memory_overview.svg" width="100%">
<p>As you can see, what is being done is essentially the same.</p>
<p>The final step is prepareing vCPU (vCPU is a CPU to be attached to a virtual machine).<br />
Currently, a VM has been created and memory containing instructions has been inserted, but these is no CPU, so the instructions can't be executed. Therefore, let's create a vCPU, associate it with the VM, and execute the instruction by running the vCPU!</p>
<p>Using the file descriptor obtained during VM creaion (<code>vmfd</code>), the resulting <code>ioctl</code> will be issued as follows.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vcpufd = ioctl(vmfd, KVM_CREATE_VCPU, 0)
<span class="boring">}
</span></code></pre></pre>
<p>The <code>create_vm</code> method that was just issued to obtain the <code>vmfd</code> is designed to return a <code>kvm_ioctls::VmFd</code> strucure as a result, and by execuing the <code>create_vcpu</code> method, which is a method of this structure, the above ioctl is consequently issued and returns the result as a <code>kvm_ioctls::VcpuFd</code> structure.</p>
<p><code>VcpuFd</code> provides utilities for getting and setting various CPU states.<br />
For example, if you want o get/set a register set from the vCPU, you would normally issue the following <code>ioctl</code></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ioctl(vcpufd, KVM_GET_SREGS, &amp;sregs);
ioctl(vcpufd, KVM_SET_SREGS, &amp;sregs);
<span class="boring">}
</span></code></pre></pre>
<p>For these, the following methods are available in <code>kvm_ioctls::VcpuFd</code></p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>get_sregs(&amp;self) -&gt; Result&lt;kvm_sregs&gt;
set_sregs(&amp;self, sregs: &amp;kvm_sregs) -&gt; Result&lt;()&gt;
<span class="boring">}
</span></code></pre></pre>
<p><code>VcpuFd</code> also provids a method called <code>run</code>, which issues the following insructions to actually run the vCPU.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>ioctl(vcpufd, KVM_RUN, NULL)
<span class="boring">}
</span></code></pre></pre>
<p>and then, we can aquire return values that has the type <code>Result&lt;VcpuExit&gt;</code> resulting this method.</p>
<p>When running vCPU, exit occurs for various reasons. This is an instruction that the CPU cannot handle, and the OS usually tries to deal with it by invoking the corresponding handler.<br />
If this type of exit comes back from the VM's vCPU, as in the case, it will be necessary to write the appropriate code to handle the situation.<br />
<code>VcpuExit</code> is defined in <code>kvm_ioctls::VcpuExit</code> as enum.<br />
When Exit are occurred on several reasons in running vCPU, the exit reasons that are defined in kvm.h in linux kernel are wrapped to <code>VcpuExit</code>.<br />
Therefore, it is sufficient to write a process that pattern matches this result and appropriately handles the error to be handled.</p>
<p>Now, there is a instruction that execute outputting values through I/O port and this will occur the <code>KVM_EXIT_IO_OUT</code>.<br />
<code>VcpuExit</code> wrap this exit reason as <code>IoOut</code>.</p>
<p>Originally (in C programm as example), we require to calculate appropriate offset to get output data from I/O port, but now, this process are implemented in <code>run</code> method and returned as VcpuExit that contains necessary values.<br />
So, we don't have to write these unsafe code (pointer offset calculation) and handle these exit as you will.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>loop {
    match vcpu.run().expect(&quot;vcpu run failed&quot;) {
        kvm_ioctls::VcpuExit::IoOut(addr, data) =&gt; {
            println!(
                &quot;Recieved I/O out exit. \
                Address: {:#x}, Data(hex): {:#x}&quot;,
                addr, data[0],
            );
        },
        kvm_ioctls::VcpuExit::Hlt =&gt; {
            break;
        }
        exit =&gt; panic!(&quot;unexpected exit reason: {:?}&quot;, exit),
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>In above, only handle <code>KVM_EXIT_IO_OUT</code> and <code>KVM_EXIT_HLT</code>, and the others will be processed as panic. (Although all exits should be handled, I want to focus on the description of KVM API example and keep it simply)</p>
<p>Since we are here, let's take a look at the processing of the <code>run</code> method in some detail.<br />
Let's check the processing of <code>KVM_EXIT_IO_OUT</code>.</p>
<p>If you look at the <a href="https://lwn.net/Articles/658511/">LWN article</a>, you will see that it calculates the offset and outputs the necessary information in the following way.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>case KVM_EXIT_IO:
    if (run-&gt;io.direction == KVM_EXIT_IO_OUT &amp;&amp;
	    run-&gt;io.size == 1 &amp;&amp;
	    run-&gt;io.port == 0x3f8 &amp;&amp;
	    run-&gt;io.count == 1)
	putchar(*(((char *)run) + run-&gt;io.data_offset));
    else
	errx(1, &quot;unhandled KVM_EXIT_IO&quot;);
    break;
<span class="boring">}
</span></code></pre></pre>
<p>On the other hand, <code>run</code> method implemented in <code>kvm_ioctl::VcpuFd</code> is like bellow</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>...
let run = self.kvm_run_ptr.as_mut_ref();
match run.exit_reason {
    ...
    KVM_EXIT_IO =&gt; {
        let run_start = run as *mut kvm_run as *mut u8;
        // Safe because the exit_reason (which comes from the kernel) told us which
        // union field to use.
        let io = unsafe { run.__bindgen_anon_1.io };
        let port = io.port;
        let data_size = io.count as usize * io.size as usize;
        // The data_offset is defined by the kernel to be some number of bytes into the
        // kvm_run stucture, which we have fully mmap'd.
        let data_ptr = unsafe { run_start.offset(io.data_offset as isize) };
        // The slice's lifetime is limited to the lifetime of this vCPU, which is equal
        // to the mmap of the `kvm_run` struct that this is slicing from.
        let data_slice = unsafe {
            std::slice::from_raw_parts_mut::&lt;u8&gt;(data_ptr as *mut u8, data_size)
        };
        match u32::from(io.direction) {
            KVM_EXIT_IO_IN =&gt; Ok(VcpuExit::IoIn(port, data_slice)),
            KVM_EXIT_IO_OUT =&gt; Ok(VcpuExit::IoOut(port, data_slice)),
            _ =&gt; Err(errno::Error::new(EINVAL)),
        }
    }
		...
<span class="boring">}
</span></code></pre></pre>
<p>Let me explain a little. The <code>kvm_run</code> is provided by the <a href="https://github.com/rust-vmm/kvm-bindings"><code>kvm-bindings</code></a> crate, which is a structure automatically generated from a header file using <a href="https://github.com/rust-lang/rust-bindgen"><code>bindgen</code></a>, so it is a structure like the linux kernel's <code>kvm_run</code> converted directory to Rust.<br />
First, <code>kvm_run</code> is obtained in the form of a pointer, a method of obtaining a pointer often used in Rust.<br />
This correspoinds to the first address of the <code>kvm_run</code> structure which is bound to <code>run_start</code> variable.<br />
And the information corresponding to <code>run-&gt;io(.member)</code> can be obtained from <code>run.__bindgen_anon_1.io</code>, although it is a bit tricky. The field named <code>__bindgen_anon_1</code> is the effect of automatic generation by <code>bindgen</code>.<br />
The data we want is at the first address of <code>kvm_run</code> plus <code>io.data_offset</code>. This process is performed in <code>run_start.offset(io.data_offset as isize)</code>. And the data size can be calculated from <code>io-&gt;size</code> and <code>io-&gt;count</code> (in the LWN example, it is 1byte, so it's taken directory from the offset by putchar). This part is calculated and stored in the value <code>data_size</code>, and <code>std::slice::from_raw_parts_mut</code> actually retrieves the data using this size.<br />
Finally, checking <code>io.direction</code>, we change the wrap type for <code>KVM_EXIT_IO_IN</code> or <code>KVM_EXIT_IO_OUT</code> respectively, and return the descired information such as <code>port</code> and <code>data_slice</code> together.</p>
<p>As can be seen from the above, what is being done is clear.<br />
However, it still contains many unsafe operations because it involves pointer manipuration.<br />
We can see that by using these libraries, we are able to implement VMM on a stable implementation.</p>
<p>Well, it's ben a long time comming, but let's take a look back at the rust-vmm crates we're using again.</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>kvm-bindings : Library that includes structures automatically generated from kvm.h by bindgen.
kvm-ioctls : Library that hides ioctl and unsafe processes related to kvm operations and provides user-friendly sructures, functions and methods.  
vm-memory : Library that provides structures and operations to the Memory
<span class="boring">}
</span></code></pre></pre>
<p>This knowledge will come up again and again in future discussion and is basic and important.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="load-linux-kernel"><a class="header" href="#load-linux-kernel">Load Linux Kernel</a></h1>
<p>このセクションではVMMのファーストステップとしてGuest VMを起動するための実装について触れることとする。<br />
VMMとしては必要最低限の機能である一方、Linux Kernelの起動にあたってさまざまな知識を要求される内容でもある。</p>
<p>このセクションではGuest VMを起動するための最低限の事項について説明し、ToyVMMでどのように実装しているかについても触れていく
そのため、いくつかの細かいチャプターに分割しトピックごとに説明をしていくことにしよう</p>
<p>トピックとしては次のようになっている。</p>
<ul>
<li><a href="./02-1_overview_of_booting_linux.html">02-1. Overview of Booting Linux</a></li>
<li><a href="./02-2_elf_binary_format_and_vmlinux_structure.html">02-2. ELF binary format and vmlinux structure</a></li>
<li><a href="./02-3_loading_initrd.html">02-3. Loading initrd</a></li>
<li><a href="./02-4_setup_registers_of_vcpu.html">02-4. Setup registers of vcpu</a></li>
<li><a href="./02-5_serial_console_implementation.html">02-5. Serial Console implementation</a></li>
<li><a href="./02-6_toyvmm_implementation.html">02-6. ToyVMM implementation</a></li>
</ul>
<p>また、本資料は以下のコミットナンバーをベースとしている</p>
<ul>
<li>ToyVMM : <code>27fdf196dfb31938f24785ca64e7233a6dc8fceb</code></li>
<li>Firecracker : <code>4bf121fc032cc2d94a20a3625f2af3918545154a</code></li>
</ul>
<p>本資料をToyVMMのコードを参照しながら確認する場合は参考にされたい。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview-of-booting-linux"><a class="header" href="#overview-of-booting-linux">Overview of Booting Linux</a></h1>
<h3 id="一般的なブートの仕組み"><a class="header" href="#一般的なブートの仕組み">一般的なブートの仕組み</a></h3>
<p>Linuxでは大まかに以下のようにプログラムが順番に動作していくことでOSが起動していく</p>
<ol>
<li>BIOS</li>
<li>ブートローダ (GRUB)</li>
<li>Linuxカーネル（vmlinuz）</li>
<li>init</li>
</ol>
<p>BIOSはマザーボード上のROMにプログラムが格納されている。<br />
我々が電源を投入すると、CPUはこの領域がマップされているアドレスから処理を実行するようになっている。<br />
BIOSはハードウェアの検出、初期化を実行し、その後OSのブートドライブ（HHD/SSD、USBフラッシュメモリなど）を探索する。<br />
この時、ブートドライブはMBR、もしくはGPTの形式でフォーマットされている必要があり、これらのフォーマットとBIOSの関係はそれぞれ以下のように対応する</p>
<table><thead><tr><th>BIOS \ DISK Format</th><th>MBR</th><th>GPT</th></tr></thead><tbody>
<tr><td>Legacy BIOS</td><td>◯</td><td>-</td></tr>
<tr><td>UEFI</td><td>◯ *</td><td>◯</td></tr>
</tbody></table>
<p>* UEFIはLegacy Boot Modeのサポートがあるため、MBRをサポートしている</p>
<p>以降ではMBRを利用する場合のOS探索について説明する<br />
詳しい説明に入る前に、MBRの構造について簡単に整理しておく。
以降で説明するMBRの構造はHDD／SSDやUSBフラッシュメモリなどの場合を想定し記載しており、後述するPartition Entryの存在を暗黙に仮定しているので注意されたい。
なお、本資料では<a href="https://en.wikipedia.org/wiki/Master_boot_record">Wikipedia</a>で記載されている名称を引用しているので注意されたい。</p>
<p>MBRはブートドライブの先頭セクタに512byte書き込まれており、大きく分けて3つの領域が存在している。</p>
<ol>
<li>Bootstrap code are (446 byte)</li>
<li>Partition Entry (64 byte = 16 byte * 4)</li>
<li>Boot Signature (2 byte)</li>
</ol>
<p>MBRについてここでは詳細に説明はしないが、Boot code areaにはOSをブートする機械語のプログラム（Boot Loader）が、Partition Entryにはそのディスクの論理パーティション情報が格納されている。<br />
（余談だが、Boot code areaは446byteしかないため、Boot Loaderを直接実装するのではなく、Boot Loaderは別の場所に格納しておき、そのブートローダをメモリに読み込むために最小限のプログラムを配置することもあるようだ）<br />
ここで重要なのは3つ目の「Boot Signature」であり、ここに格納されている2byteの値は、当該ドライブがブートドライブかどうかを担保するために利用される。<br />
具体的には、BIOSがOSのブートドライブを探索する時、先頭1セクタ（512byte）を読み込み、最後の2byte（Boot Signature）がブートドライブであることを示すシグネチャ（<code>0x55 0xaa</code>）であることを確認する。
このシグネチャが亜確認できた場合、当該ディスクをブートディスクと判定し、先頭1セクタ(512byte)をメインメモリの0x7c00から0x7fffに読み込んで、0x7c00からプログラムを実行していく。</p>
<p>さて、これまでの議論の簡単な裏付けとして、手元のマシンでBoot Signatureを確認してみる。<br />
仮想マシンなので、ブートディスクは<code>vda</code>と表示されている。通常のマシンなら<code>sda</code>などだろう。
この<code>vda</code>から先頭1セクタ分の内容をファイルに書き出し、<code>hexdump</code>で510byteオフセットした位置から2byte確認してみると、確かに<code>0x55</code> <code>0xaa</code>の値が確認できる。</p>
<pre><code class="language-bash">$ lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sr0     11:0    1    2M  0 rom
vda    252:0    0  300G  0 disk
├─vda1 252:1    0    1M  0 part
└─vda2 252:2    0  300G  0 part /

$ sudo dd if=/dev/vda of=mbr bs=512 count=1
1+0 records in
1+0 records out
512 bytes (512 B) copied, 0.000214802 s, 2.4 MB/s

$ hexdump -s 510 -C mbr
000001fe  55 aa                                             |U.|
00000200
</code></pre>
<p>話を戻すと、BIOSによってMBRの情報を元にブートディスク上に格納されていたブートローダがメモリ上に展開され実行されていくことになる。ブートローダはカーネルとinitramfsをDISKからメモリに読み込み、カーネル起動する役目を持ったプログラムであり、近年では一般的にGRUBが利用されることが多い。ブートローダの詳細の処理内容についてもここでは省略とする。</p>
<p>重要な点としては、ブートローダはDISK上に格納されたカーネルなどを読み込む必要があるという点である。
これを達成する素朴な方法は我々がDISK上のカーネルファイルの位置をブートローダに対して教えることであろう。
しかしgrub.cfgの内容を見てみると、カーネルやinitrdの位置をファイルパスの形でしか指定していないことを確認できるだろう。
これはブートローダがファイルシステムを解釈する能力を有している必要があることを意味する。
実際に、Boot Loaderいくつかのファイルシステムを解釈でき、ファイルシステム上のディレクトリパス情報からカーネルを探し出すことができる。
ただし、当然ながらBoot Loaderは特定のファイルシステムのみのサポートに留まるため、それ以外のフォーマットのものを解釈することはできないので注意されたい。
ブートローダによってgrub.cfgで指定されたカーネルとRAMディスクをメモリ上にロードし、カーネルの先頭アドレスにジャンプすることで、処理をカーネルへと引き渡し自身の処理を終える</p>
<p>カーネルの処理の話に入る前に、カーネルファイルについて少し整理しておく。
カーネルファイルは一般に<code>vmlinuz*</code>という名前がついているファイルである
我々にとって馴染みのあるカーネルファイルは<code>/boot/vmlinuz-*.img</code>と思われるが、このファイルは<code>bzImage</code>形式のファイルである。これは<code>file</code>コマンドで簡単に確認することができる。<br />
この<code>bzImage</code>はカーネル本体を含む圧縮バイナリファイルの他に、低レベルの初期化を行うためのファイルなどいくつかのファイルが含まれる形式になっている。
この<code>bzImage</code>を適切に解凍することでカーネルの実行バイナリを手に入れることもできる。
本資料では<code>bzImage</code>形式のカーネルを<code>vmlinuz</code>、実行バイナリ形式のカーネルを<code>vmlinux.bin</code>と表記する。</p>
<p>さてブートローダから<code>vmlinuz</code>に処理が引き渡されると<code>vmlinuz</code>は低レベルの初期化処理を実施後、カーネル本体を解凍、メモリにロードし、カーネルのエントリールーチンに処理を移す。
カーネルは全ての初期化処理を終えると、<code>tmpfs</code>ファイルシステムを作成し、ブートローダがRAM上に配置した<code>initramfs</code>をそこに展開し、ディレクトリルートにある<code>init</code>スクリプトを起動する。
この<code>init</code>スクリプトはDISK上に格納されている本命のファイルシステムをマウントするために必要な準備を整え、本命のファイルシステムやその他に重要なファイルシステムをマウントする。この時<code>initramfs</code>にはいくつかのデバイスドライバなども含まれているため、多様なフォーマットのルートファイルシステムをマウントすることが可能である。
さらにこれが完了すると、ルートを本命のルートファイルシステムに切り替え、そこに格納されている<code>/sbin/init</code> バイナリを起動する。</p>
<p><code>/sbin/init</code>はシステムで最初に起動されるプロセス（PID=1が付与されるプロセス）であり、他のプロセスを起動させる役割を持っている全てのプロセスの親となるものである。
initにはさまざまな実装（<code>SysVinit</code>, <code>Upstart</code>）があるが、最近のCentOSやUbuntuなどで利用されているのは<code>Systemd</code>である。
initの最終的な責務は、システムの更なる準備とブートプロセスが終わった時点で必要なサービスが実行されておりユーザがログイン可能な状態まで持っていくことである。</p>
<p>以上が、非常に大雑把ではあるが電源投入からOSが起動するまでの流れである。</p>
<h3 id="initrdとinitramfs"><a class="header" href="#initrdとinitramfs">initrdとinitramfs</a></h3>
<p>上記に記載したLinux起動処理の中に、メモリ上に展開するファイルシステムである<code>initramfs</code>を紹介したが、我々がよく目にするのは<code>/boot/initrd.img</code>であろうと思う。ここでは<code>initrd</code>と<code>initramfs</code>との違いについて説明する
<code>initrd</code>は「initial RAM <strong>disk</strong>」、<code>initramfs</code>「initial RAM <strong>File System</strong>」であり両者は別物であるが、提供したい機能は同じで「本命のルートファイルシステムのマウントに必要なコマンド、ライブラリ、モジュール」を提供し、本命のルートファイルシステム上に存在する<code>/sbin/init</code>スクリプトを起動することである。<br />
もともと本来起動したいシステムは何かしらの記憶装置に書き込まれているが、これを読み込むには適切なデバイスドライバの存在と、これをマウントするファイルシステムが存在していないといけないという問題がある。<br />
<code>initrd</code>/<code>initramfs</code>は両方ともこの問題を解決する。</p>
<p><code>initrd</code>と<code>initramfs</code>は上記の機能を提供するための<strong>方式</strong>が異なっており、名前の通りであるが<code>initrd</code>はブロックデバイス、<code>initramfs</code>は（<code>tmpfs</code>をもとにした）RAM filesystemの方式になっている。
従来は<code>initrd</code>を利用していたが、Kernel 2.6以降で<code>initramfs</code>が利用できるようになっており、現在はこちらの方式を利用することの方が一般的と思われる。
<code>initrd</code>から<code>initramfs</code>に移りわってきたのにはもちろん、<code>initrd</code>には問題があり、<code>initramfs</code>はそれの解決が測られたからである。
<code>initrd</code>には概ね以下のような問題が存在していた</p>
<ol>
<li>RAM diskはRAM上に擬似的なブロックデバイスを作成し、これをあたかも二次記憶のように取り扱う仕組みであるため、通常のブロックデバイスと同様にメモリキャッシュ機構が働いてしまうために不必要にキャッシュメモリを消費する。さらにはページングのような機構が働いてしまうことで一層メモリを逼迫してしまう。</li>
<li>RAM diskはそのデータをフォーマットし解釈するためのext2のようなファイルシステムドライバーが必要である。</li>
<li>RAM diskのブロックデバイスは固定サイズになるため、あまりに小さいと必要なスクリプトを全て収めることができず、大きすぎると無駄にメモリを利用する</li>
</ol>
<p>これを解決するために考案され、現在のデフォルトになっているのが<code>initramfs</code>である。<br />
<code>initramfs</code>はサイズを柔軟に設定できる軽量なメモリ内ファイルシステムである<code>tmpfs</code>をベースとして作られたfilesystemである。<br />
当然これはブロックデバイスではないので、キャッシュやページングでメモリを汚すこともなく、ブロックデバイスに対するファイルシステムドライバも不要で、さらに固定長という問題もうまく解決している。</p>
<p><code>initrd</code>/<code>initramfs</code>いずれの方式にせよ、その中に格納されているツールを利用して本命のルートファイルシステムをマウントしそちらにルートを切り替えた上で、そのファイルシステム上に存在しているスタートアップスクリプトである<code>/sbin/init</code>を起動する。</p>
<h4 id="initramfsの中身を確認する"><a class="header" href="#initramfsの中身を確認する">initramfsの中身を確認する</a></h4>
<p><code>initramfs</code>の内容を展開し中身を確認してみる。Ubuntu 20.04.2 LTSの<code>initrd</code>を展開してみる。<br />
（注意: <code>initrd</code>という命名のファイルだが、このファイルはれっきとした<code>initramfs</code>である）。<br />
<code>initramfs</code>はいくつかのファイルをCPIOの形式にしたものが連結されているため、そのまま<code>cpio</code>コマンドで解凍しても以下のように冒頭のファイル（AuthenticAMD.bin）のみしか出てこない</p>
<pre><code class="language-bash">$ mkdir initrd-work &amp;&amp; cd initrd-work
$ sudo cp /boot/initrd.img ./
$ cat initrd.img| cpio -idvm
.
kernel
kernel/x86
kernel/x86/microcode
kernel/x86/microcode/AuthenticAMD.bin
62 blocks
</code></pre>
<p><code>dd</code>/<code>cpio</code>の組み合わせで全てのファイルが展開できるが、<code>unmkinitramfs</code>という便利なコマンドがあるので今回はこちらを利用する</p>
<pre><code class="language-bash">$ mkdir extract
$ unmkinitramfs initrd.img extract
$ ls extract
early  early2  main
</code></pre>
<p>解凍した結果、<code>early</code>, <code>early2</code>, <code>main</code>というディレクトリが作成されていることがわかる
例えばこの<code>early</code>は先ほどCPIOで解凍した際に出てきたファイルが確認できる
重要なのは、<code>main</code>の配下で、その中のコンテンツとしてファイルシステムルートの内容が格納されている</p>
<pre><code class="language-bash">$ ls extract/early/kernel/x86/microcode
AuthenticAMD.bin
$ ls extract/early2/kernel/x86/microcode
GenuineIntel.bin
$ ls extract/main
bin  conf  cryptroot  etc  init  lib  lib32  lib64  libx32  run  sbin  scripts  usr  var
</code></pre>
<p>ここで解凍した内容に対してchrootすると、Linux起動時のRAM filesystemの内容を擬似的に操作でき、どのような操作ができるか把握することができる。</p>
<pre><code>$ sudo chroot extract/main /bin/sh

BusyBox v1.30.1 (Ubuntu 1:1.30.1-4ubuntu6.3) built-in shell (ash)
Enter 'help' for a list of built-in commands.

# ls
scripts    init       run        etc        var        usr        conf
lib64      bin        lib        libx32     lib32      sbin       cryptroot
# pwd
/
# which mount
/usr/bin/mount
# exit
</code></pre>
<p>上記に示す通り、ルートに<code>init</code>というスクリプトファイルが入っており、これが<code>initramfs</code>を展開したのちに起動されるスクリプトである
このスクリプトを全て解説することはしないが、<code>init</code>スクリプトの中で<code>/proc/cmdline</code>の中身を読んでおり、ここから本来のroot filesystemが格納しているディスク情報（<code>root=/dev/sda1</code>のような記載）を拾い、マウント処理を実施しているようであった。
一方、この辺りが空の場合、Ubuntu 20.04LTSのinitrdから抜き出したこの<code>init</code>ファイルではエラーになるようだった。</p>
<p>今回のToyVMMでは以降説明する<code>firecracker-initrd</code>をベースとした<code>initramfs</code>を利用しているためこの辺りの挙動は少し異なる。</p>
<h4 id="firecracker-initrdについて"><a class="header" href="#firecracker-initrdについて">firecracker-initrdについて</a></h4>
<p>ToyVMMでは、<a href="https://github.com/marcov/firecracker-initrd">firecracker-initrd</a>を利用させてもらっている。
firecracker-initrdはAlpineをベースとしてinitrd.img（<code>initramfs</code>）を作成してくれる。
上記でみたUbuntuのinitrdとは異なり、microcodeなど追加のCPIOファイルは含まれないため、単純に解凍するだけでroot filesystemが確認できる</p>
<pre><code>$ cat initrd.img | cpio -idv
$ ls
bin  dev  etc  home  init  initrd.img  lib  media  mnt  opt  proc  root  run  sbin  srv  sys  tmp  usr  var
</code></pre>
<p>Alpine Linuxは通常起動時にRAM上にファイルシステムが展開された上でOSが起動する。その後ニーズに応じて<code>setup-alpine</code>でDISKにOSを焼いたりするかなど決定する。
今回はこのAlpine Linuxのinitを使用しているため、この<code>initramfs</code>を利用して起動したVMは、デフォルトでは本命のルートファイルシステムをマウントせず、単純にRAM上にファイルシステムを展開しAlpine Linuxが起動することになる。<br />
これは、従来通りのOSのようにboot領域を二次記憶に置いた上で<code>/proc/cmdline</code>でinitスクリプトに伝えるという流れとは異なるものであるということを理解しておきたい。</p>
<h2 id="boot-sequence-of-linux-kernel-in-toyvmm"><a class="header" href="#boot-sequence-of-linux-kernel-in-toyvmm">Boot sequence of linux kernel in ToyVMM</a></h2>
<p>ここでこれまで議論してきた内容とToyVMMでのLinuxブートについての比較をしてみる</p>
<table><thead><tr><th>Boot process (on Linux)</th><th>ToyVMM</th></tr></thead><tbody>
<tr><td>BIOS</td><td>Not implemented yet</td></tr>
<tr><td>Boot Loader</td><td><strong>Require: vmlinux/initrd.img loading, basic setup required</strong></td></tr>
<tr><td>Linux Kernel</td><td>Processed by <code>vmlinux.bin</code></td></tr>
<tr><td>init</td><td>Processed by <code>init</code> scripts (from firecracker-initrd's <code>initrd.img</code>)</td></tr>
</tbody></table>
<p>現在のToyVMMの実装では<code>bzImage</code>の読み込みについてはサポートしておらず、ELFバイナリである<code>vmlinux.bin</code>を利用することとする。
現時点の実装ではBIOS関係については実装を省略している。<br />
BootLoaderが行う処理のうち、<code>vmlinux.bin</code>や<code>initrd.img</code>をメモリにロードするなどの処理を実装する必要がある。
Linux Kernel自体は<code>vmlinux.bin</code>が、<code>init</code>の処理は<code>initrd.img</code>内部の<code>init</code>スクリプトが担当するため、上記の処理を実装することで既存のLinux Kernelを起動すること自体は可能である。
より詳細の実装については<a href="./02-6_minimal_vmm_implementation.html">02-6_minimal_vmm_implementation</a>で説明する。</p>
<h2 id="references"><a class="header" href="#references">References</a></h2>
<ul>
<li><a href="https://www.hazymoon.jp/OpenBSD/arch/i386/stand/mbr/mbr_structure.html">MBR(Master Boot Records)の構造</a></li>
<li><a href="https://linux.die.net/man/4/initrd">Initrd(4) - Linux man page</a></li>
<li><a href="https://gihyo.jp/admin/serial/01/ubuntu-recipe/0384">Initramfsのしくみ</a></li>
<li><a href="https://wiki.gentoo.org/wiki/Initramfs/Guide/ja">Initramfs/ガイド</a></li>
<li><a href="https://0xax.gitbooks.io/linux-insides/content/Booting/">Kernel Boot Process</a></li>
<li><a href="https://www.baeldung.com/linux/initrd-vs-initramfs">What's the Difference Between initrd and initramfs</a></li>
<li><a href="https://wiki.bit-hive.com/linuxkernelmemo/pg/bzImage">bzImage</a></li>
<li><a href="http://www.seinan-gu.ac.jp/%7Eshito/old_pages/hacking/shell/sh/boot_shutdown.html">Initデーモンを理解する</a></li>
<li><a href="https://keichi.dev/post/linux-boot/">Linuxがブートするまで</a></li>
<li><a href="http://archive.linux.or.jp/JF/JFdocs/kernel-docs-2.6/filesystems/ramfs-rootfs-initramfs.txt.html">filesystems/ramfs-rootfs-initramfs.txt</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="elf-binary-format-and-vmlinux-structure"><a class="header" href="#elf-binary-format-and-vmlinux-structure">ELF binary format and vmlinux structure</a></h1>
<p>本稿執筆時、ToyVMMでVMを起動する際に利用するカーネルはELF形式の<code>vmlinux.bin</code>を前提としている。<br />
そのため、VMMの内部ではELF形式を解釈し、適切にカーネルをVM用に用意したメモリ領域にロードする必要がある。 
この処理は <a href="https://github.com/rust-vmm/linux-loader"><code>rust-vmm/linux-loader</code></a> crateで実装されており、ToyVMMではこのcrateを利用するため実装としては隠蔽されてしまうが、このcrateの中でどのように処理されているかを知ることは重要だと判断したため、本章を設けELFバイナリのロードに関する解説を記載することとした。</p>
<h2 id="elf-binary-format"><a class="header" href="#elf-binary-format">ELF Binary Format</a></h2>
<p>ELFのファイルフォーマットは以下のようになっている</p>
<img src="./02_figs/elf_format.svg" width="100%">
<p>上記の通り、ELFファイルフォーマットは基本的に<code>ELF Header</code>、<code>Program Header Table</code>、<code>Segument(Sections)</code>、<code>Section Header Table</code>からなる。<br />
ELFファイルはシステムローダが利用する場合は<code>Program Header Table</code>に記述された<code>Segment</code>の集合として取り扱われ、コンパイラ・アセンブラ・リンカは<code>Section Header Table</code>に記述された<code>Section</code>の集合として扱われる。</p>
<p><code>ELF Header</code>はこのELFファイルの全体的な情報を保持している。<br />
<code>Program Header Table</code>の各エントリである<code>Program Header</code>は、それぞれが対応する<code>Segument</code>についてのHeader情報を保持している。つまり、<code>Program Header</code>の数だけ、<code>Segment</code>が存在していることになる。<br />
また、この<code>Segument</code>はさらに複数の<code>Seciton</code>という単位に分割でき、この<code>Section</code>単位でヘッダ情報を保持しているのが<code>Section Header Table</code>である。</p>
<p><code>ELF Header</code>は常にファイルオフセットの先頭から始まっており、ELFデータを読み込むために必要となる情報を保持している。
以下に<code>ELF Header</code>の内容を一部抜粋する。全体の構成を知りたい場合は<a href="https://linuxjm.osdn.jp/html/LDP_man-pages/man5/elf.5.html">Man page of ELF</a>を参考にされたい</p>
<table><thead><tr><th>Attribute</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>e_entry</code></td><td>このELFプロセスを開始する際のエントリポイントとなる仮想アドレス</td></tr>
<tr><td><code>e_phoff</code></td><td><code>Program Header Table</code>が存在する場所のファイルオフセット値</td></tr>
<tr><td><code>e_shoff</code></td><td><code>Section Header Table</code>が存在する場所のファイルオフセット値</td></tr>
<tr><td><code>e_phentsize</code></td><td><code>Program Header Table</code>にある1エントリのサイズ</td></tr>
<tr><td><code>e_phnum</code></td><td><code>Program Header Table</code>中のエントリの個数</td></tr>
<tr><td><code>e_shentsize</code></td><td><code>Section Header Table</code>にある1エントリのサイズ</td></tr>
<tr><td><code>e_shnum</code></td><td><code>Section Header Table</code>中のエントリの個数</td></tr>
</tbody></table>
<p>上記で抜粋した内容から、<code>Program Header</code>や<code>Section Header</code>の各エントリの情報を取り出すことが可能であると分かるであろう。</p>
<p>ここで、<code>Program Header</code>の内容を一部抜粋する。</p>
<table><thead><tr><th>Attribute</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>p_type</code></td><td>この<code>Program Header</code>が指す<code>Segment</code>の種類を表現しており、解釈の方法についてのヒントを与える</td></tr>
<tr><td><code>p_offset</code></td><td>この<code>Program Header</code>が指す<code>Segment</code>のファイルオフセット値</td></tr>
<tr><td><code>p_paddr</code></td><td>物理アドレスが意味を持つシステムでは、この値は<code>Program Header</code>が指す<code>Segment</code>の物理アドレスを指す</td></tr>
<tr><td><code>p_filesz</code></td><td>この<code>Program Header</code>が指す<code>Segment</code>のファイルイメージのバイト数</td></tr>
<tr><td><code>p_memsz</code></td><td>この<code>Program Header</code>が指す<code>Segment</code>のメモリイメージのバイト数</td></tr>
<tr><td><code>p_flags</code></td><td>この<code>Program Header</code>が指す<code>Segment</code>の情報を示すフラグで、実行可能、書き込み可能、読み取り可能を表現している</td></tr>
</tbody></table>
<p>上述の通り、<code>Program Header</code>の中身を解釈することで、当該セグメントの位置やサイズ、どの様に解釈すべきかの情報を手に入れることができる。
今回の内容はこの<code>Program Header</code>の構造まで把握できていれば十分であるため、<code>Section Header</code>やそのほかの詳細については省略する。<br />
興味がある方は、<a href="https://linuxjm.osdn.jp/html/LDP_man-pages/man5/elf.5.html">Man page of ELF</a>等を参考に確認されたい。</p>
<p>さて、後述するが今回取り扱う<code>vmlinux.bin</code>は<code>Program Header</code>の数が5個で、内4つの<code>p_type</code>の値が<code>PT_LOAD</code>、最後の一つだけ<code>PT_NOTE</code>になっているという大変簡単な構造になっている。
ここで、<code>PT_LOAD</code>、<code>PT_NOTE</code>についてのみ、<a href="https://linuxjm.osdn.jp/html/LDP_man-pages/man5/elf.5.html">Man page of ELF</a>からその詳細内容を部分的に抜粋する。一部情報を削っているため、必要に応じて参考資料を確認されたい。</p>
<table><thead><tr><th><code>p_type</code></th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>PT_LOAD</code></td><td>この要素は<code>p_filesz</code>と<code>p_memsz</code>で記述される読み込み可能な<code>Segment</code>である。</td></tr>
<tr><td><code>PT_NOTE</code></td><td>この要素はロケーションとサイズのための補助情報が書き込まれている</td></tr>
</tbody></table>
<p><code>PT_LOAD</code>では、ファイルのバイト列はメモリセグメントの先頭に対応づけされているため、<code>p_offset</code>を利用して得られる、セグメントのメモリアドレスからサイズ分（基本的には<code>p_memsz</code>を利用する）をCOPYすることでセグメントの内容を読み込むことができる。</p>
<p>以上で必要最低限なELFの知識を身につけることができたので、次は実際に<code>vmlinux.bin</code>をダンプしてみて中身を確認してみる。</p>
<h2 id="vmlinxの解析"><a class="header" href="#vmlinxの解析">vmlinxの解析</a></h2>
<p>それではここで<code>vmlinux</code>の内容を少し解析してみよう。<br />
この解析内容の一部は今後重要な要素になってくるため是非把握してもらいたい。<br />
<code>readelf</code>コマンドはELFフォーマットのファイルを理解しやすい形でダンプしてくれる非常に心強いツールである。
ここではvmlinuxのELF Header(<code>-h</code>)、Program Header（<code>-l</code>）をそれぞれ表示してみる</p>
<pre><code class="language-bash">$ readelf -h -l vmlinux.bin
ELF Header:
  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00
  Class:                             ELF64
  Data:                              2's complement, little endian
  Version:                           1 (current)
  OS/ABI:                            UNIX - System V
  ABI Version:                       0
  Type:                              EXEC (Executable file)
  Machine:                           Advanced Micro Devices X86-64
  Version:                           0x1
  Entry point address:               0x1000000
  Start of program headers:          64 (bytes into file)
  Start of section headers:          21439000 (bytes into file)
  Flags:                             0x0
  Size of this header:               64 (bytes)
  Size of program headers:           56 (bytes)
  Number of program headers:         5
  Size of section headers:           64 (bytes)
  Number of section headers:         36
  Section header string table index: 35

Program Headers:
  Type           Offset             VirtAddr           PhysAddr
                 FileSiz            MemSiz              Flags  Align
  LOAD           0x0000000000200000 0xffffffff81000000 0x0000000001000000
                 0x0000000000b72000 0x0000000000b72000  R E    0x200000
  LOAD           0x0000000000e00000 0xffffffff81c00000 0x0000000001c00000
                 0x00000000000b0000 0x00000000000b0000  RW     0x200000
  LOAD           0x0000000001000000 0x0000000000000000 0x0000000001cb0000
                 0x000000000001f658 0x000000000001f658  RW     0x200000
  LOAD           0x00000000010d0000 0xffffffff81cd0000 0x0000000001cd0000
                 0x0000000000133000 0x0000000000413000  RWE    0x200000
  NOTE           0x0000000000a031d4 0xffffffff818031d4 0x00000000018031d4
                 0x0000000000000024 0x0000000000000024         0x4

 Section to Segment mapping:
  Segment Sections...
   00     .text .notes __ex_table .rodata .pci_fixup __ksymtab __ksymtab_gpl __kcrctab __kcrctab_gpl __ksymtab_strings __param __modver
   01     .data __bug_table .vvar
   02     .data..percpu
   03     .init.text .altinstr_aux .init.data .x86_cpu_dev.init .parainstructions .altinstructions .altinstr_replacement .iommu_table .apicdrivers .exit.text .smp_locks .data_nosave .bss .brk
   04     .notes
</code></pre>
<p>ELF Headerをみてみると、Entry point address (<code>e_entry</code>)の値としてProgram Headerの最初のセグメントの物理アドレスの値である(<code>0x0100_0000</code>)が格納されていることが分かる。この値は<code>rust-vmm/linux-loader</code>の実装としてkernelをロードした際の返り値として返却される値であり、かつvCPUの<code>eip</code>（命令アドレスレジスタ）に設定する値でもあるため重要である。<br />
また、ELF HeaderのNumber of program headers(<code>e_phnum</code>)の値である<code>5</code>と同じ数のProgram Headerが確認でき、Program Headerを出力をみると先頭4つはTypeが<code>LOAD</code>、最後は<code>NOTE</code>となっていることが確認できる。<br />
また、1つ目、および4つ目の<code>LOAD</code>セグメントはFlagを確認すると<code>E(xecutable)</code>がマークされており、この辺りに実行可能コードが存在していることも分かる。
特に1つめのエントリは実際にカーネルの実行バイナリのエントリポイントに該当する内容が配置されていることが期待される。<br />
今回はこれ以上の深追いは控えておくが、興味がある人はELFのSpecificationをもとにさらに解析をしてみるのも面白いかもしれない。</p>
<h2 id="toyvmmでの実装"><a class="header" href="#toyvmmでの実装">ToyVMMでの実装</a></h2>
<p>ToyVMMでは、<code>src/builder.rs</code>の中の<code>load_kernel</code>関数の中でvmlinuxの読み込みを実施している。<br />
この関数には、カーネルファイルへのパス情報などが含まれている<code>boot_config</code>とVM向けに確保したメモリ(<code>guest_memory</code>)を渡している。
<code>load_kernel</code>が実施していることは単純で、<code>boot_config</code>からカーネルファイルへのパスを取得し、<code>linux-loader</code>の<code>Elf</code>という構造体を<code>Loader</code>という名前で取り扱い、この構造体に実装されているELF形式のLinuxのローディング処理を適切な引数を伴って実行しているだけである。</p>
<pre><code>use linux_loader::elf::Elf as Loader;

let entry_addr = Loader::load::&lt;File, memory::GuestMemoryMmap&gt;(
    guest_memory,
    None,
    &amp;mut kernel_file,
    Some(GuestAddress(arch::x86_64::get_kernel_start())),
).map_err(StartVmError::KernelLoader)?;
</code></pre>
<p>さて、ここから<code>linux-loader</code>の実装について深掘りしてみよう。
<code>linux-loader</code>では、<code>KernelLoader</code> traitが定義されており、その定義は以下のようになっている</p>
<pre><code>/// Trait that specifies kernel image loading support.
pub trait KernelLoader {
    /// How to load a specific kernel image format into the guest memory.
    ///
    /// # Arguments
    ///
    /// * `guest_mem`: [`GuestMemory`] to load the kernel in.
    /// * `kernel_offset`: Usage varies between implementations.
    /// * `kernel_image`: Kernel image to be loaded.
    /// * `highmem_start_address`: Address where high memory starts.
    ///
    /// [`GuestMemory`]: https://docs.rs/vm-memory/latest/vm_memory/guest_memory/trait.GuestMemory    .html
    fn load&lt;F, M: GuestMemory&gt;(
        guest_mem: &amp;M,
        kernel_offset: Option&lt;GuestAddress&gt;,
        kernel_image: &amp;mut F,
        highmem_start_address: Option&lt;GuestAddress&gt;,
    ) -&gt; Result&lt;KernelLoaderResult&gt;
    where
        F: Read + Seek;
}
</code></pre>
<p>コメントから推測できるように、このtraitが実装しているべき<code>load</code>関数は、特定のカーネルイメージフォーマットをGuestMemoryに読み込むような実装になっていることを要求している。
linux-loaderではx86_64向けの実装として、ELF形式の他にbzImage形式のカーネルの読み込みについても実装が存在しているようであるが、ひとまず今回はELF向けの実装を利用する。</p>
<p>さて、先のToyVMM側のコードで利用していた<code>Elf</code>構造体（<code>Loader</code>と名前を変えてimportした構造体）はこの<code>KernelLoader</code> traitを実装しており、その<code>load</code>関数がELFファイルをロードする実装になっていることが期待できる。<br />
そのため、このload関数を見てみると以下のような処理になっていることがわかる。処理内容がすこし長いためコードの転載は控える。</p>
<ol>
<li>ELFファイルの先頭から、ELFヘッダー分のデータを抜き出す</li>
<li><code>loader_result</code>という変数名の<code>KernelLoaderResult</code>構造体のインスタンスを作成し、<code>kernel_load</code>メンバにELFヘッダの<code>e_entry</code>の値を格納しておく。この値はシステムが最初に制御を渡すアドレス、つまりプロセスを開始する仮想アドレスに該当する。</li>
<li>ELFファイルを先頭からプログラムヘッダテーブルが存在するアドレスまで（<code>e_phoff</code>分）シークし、プログラムヘッダテーブル数分（<code>e_phnum</code>分）ループしながら、ELFファイルに含まれているプログラムヘッダを全て抜き出す。</li>
<li>上記のプログラムヘッダをループしつつ以下の内容を行う
<ul>
<li>ELFファイルの先頭から今確認しているプログラムヘッダに対応するセグメントまで（<code>p_offset</code>分）シーク</li>
<li>Guestのメモリに対して、<code>mem_offset</code>から算出したmemory regionのアドレス位置を先頭に、<code>kernel_image</code>（<code>p_offset</code>分シーク済みなので、プログラムヘッダに対応するセグメントのデータの先頭）から、セグメントのサイズ分(<code>p_filesz</code>分)だけを書き込む</li>
<li><code>kernel_end</code>（GuestMemory上での読み込んだセグメントの末尾のアドレス）の値を更新し、<code>loader_result.kernel_end</code>（2回目以降のループでは前回の値が記録されている）と比較して大きい方の値を<code>loader_result.kernel_end</code>に格納しておく</li>
</ul>
</li>
<li>全てのプログラムヘッダをループ後、返り値として最終的な<code>loader_result</code>を返却する。</li>
</ol>
<p>これはまさに上記でみたELFフォーマットを解釈し読み込むコードになっていることがわかる。<br />
また当該関数呼び出しの結果返却される<code>KernelLoaderResult</code>の値には、最終的なGuestMemory上でのカーネルの開始位置、終了位置の情報が含まれており、特にこの開始位置の情報は<a href="./02-4_setup_registers_of_vcpu.html">Setup registers of vCPU</a>で利用する値になるため重要である。</p>
<h2 id="references-1"><a class="header" href="#references-1">References</a></h2>
<ul>
<li><a href="https://valinux.hatenablog.com/entry/20200910">vmlinux</a></li>
<li><a href="https://www.hazymoon.jp/OpenBSD/annex/elf.html">ELF Formatについて</a></li>
<li><a href="https://linuxhint.com/understanding_elf_file_format/">Understanding the ELF File Format</a></li>
<li><a href="https://qiita.com/niwaka_dev/items/b915d1ffc7a677c74959">ELF形式のヘッダ部分を解析する単純なプログラムを作ってみた</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="loading-initrd"><a class="header" href="#loading-initrd">Loading initrd</a></h1>
<p>本稿では、VMを起動するにあたり<code>initrd</code>(<code>initramfs</code>)のロードやこれにまつわる設定について記載する。<br />
以降の記載では、<code>initrd</code>と書いたときも暗黙に<code>initramfs</code>を指しているとする。<br />
initramfs自体の説明は<a href="./02-1_overview_of_booting_linux.html">Overview of booting linux</a>で既におこなっているのでそちらを確認されたい</p>
<h2 id="loading-initrd-and-setup-some-parameters-of-kernel-header"><a class="header" href="#loading-initrd-and-setup-some-parameters-of-kernel-header">Loading initrd and setup some parameters of kernel header</a></h2>
<p><code>initrd</code>をロードする関数は<code>load_initrd</code>に実装している。
引数としてはGuest用に確保したメモリと、<code>initrd</code>のファイルをOpenした<code>File</code>構造体(Read, Seekを実装している)の可変参照を渡している</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn load_initrd&lt;F&gt;(
    vm_memory: &amp;memory::GuestMemoryMmap,
    image: &amp;mut F,
) -&gt; std::result::Result&lt;InitrdConfig, StartVmError&gt;
where F: Read + Seek {
    let size: usize;
    // Get image size
    match image.seek(SeekFrom::End(0)) {
        Err(e) =&gt; return Err(StartVmError::InitrdRead(e)),
        Ok(0) =&gt; {
            return Err(StartVmError::InitrdRead(io::Error::new(
                io::ErrorKind::InvalidData,
                &quot;Initrd image seek returned a size of zero&quot;,
            )))
        }
        Ok(s) =&gt; size = s as usize,
    };
    // Go back to the image start
    image.seek(SeekFrom::Start(0)).map_err(StartVmError::InitrdRead)?;
    // Get the target address
    let address = arch::initrd_load_addr(vm_memory, size)
        .map_err(|_| StartVmError::InitrdLoad)?;

    // Load the image into memory
    //   - read_from is defined as trait methods of Bytes&lt;A&gt;
    //     and GuestMemoryMmap implements this trait.
    vm_memory
        .read_from(GuestAddress(address), image, size)
        .map_err(|_| StartVmError::InitrdLoad)?;

    Ok(InitrdConfig{
        address: GuestAddress(address),
        size,
    })
}
<span class="boring">}
</span></code></pre></pre>
<p>上記の処理でおこなっていること関数名の通りGuest用メモリへの<code>initrd</code>のロードであるが、内容としては以下の通り単純である</p>
<ol>
<li>initrdのサイズの取得する（SeekFrom::End(0)としてファイルの末尾にカーソルを指定することでoffset=size取得をしている）</li>
<li>1でサイズを取得するために動かしたカーソルを先頭に戻す</li>
<li>initrdをロードするべきGuestメモリのaddressを取得する</li>
<li>上記Guestメモリのaddress位置にinitrdの中身を読み込む</li>
<li><code>InitrdConfig</code>という構造体にGuestメモリのinitrd開始位置のアドレスとinitrdの値を詰めて返却する)</li>
</ol>
<p>さて、上記でGuestメモリ上に<code>initrd</code>をロードすることはできたが、実際にこの領域をカーネルがどのように把握するのかという疑問が残っている<br />
ブートローダの責務の一つにカーネルのセットアップヘッダを読み込み、いくつかのフィールドを埋めるというものがある。<br />
このセットアップヘッダの内容は<a href="https://docs.kernel.org/x86/boot.html#the-real-mode-kernel-header">Boot Protocol</a>として定義されており、上記のinitrdに関係する内容はこの値として格納されるべき値の一つになっている.</p>
<p>今回、ToyVMMではこれらの内容を主に<code>configure_system</code>関数で以下の通り設定している。<br />
以下の内容については<a href="https://docs.kernel.org/x86/boot.html#the-real-mode-kernel-header">Boot Protocol</a>を参照している。<br />
ここでは下記以外の設定項目については設定をしていないため説明を省略する。</p>
<table><thead><tr><th>Offset/Size</th><th>Name</th><th>Meaning</th><th>ToyVMM value</th></tr></thead><tbody>
<tr><td>01FE/2</td><td>boot_flag</td><td>0xAA55 magic number</td><td>0xaa55</td></tr>
<tr><td>0202/4</td><td>header</td><td>Magic signature &quot;HdrS&quot; (0x53726448)</td><td>0x5372_6448</td></tr>
<tr><td>0210/1</td><td>type_of_loader</td><td>Boot loader identifier</td><td>0xff (undefined)</td></tr>
<tr><td>0218/4</td><td>ramdisk_image</td><td>initrd load address (set by boot loader)</td><td>GUEST ADDRESS OF INITRD</td></tr>
<tr><td>021C/4</td><td>ramdisk_size</td><td>initrd size (set by boot loader)</td><td>SIZE OF INITRD</td></tr>
<tr><td>0228/4</td><td>cmd_line_ptr</td><td>32-bit pointer to the kernel command line</td><td>0x20000</td></tr>
<tr><td>0230/4</td><td>kernel_alignment</td><td>Physical addr alignment required for kernel</td><td>0x0100_0000</td></tr>
<tr><td>0238/4</td><td>cmdline_size</td><td>Maximum size of the kernel command line</td><td>SIZE OF CMDLINE STRING</td></tr>
</tbody></table>
<p>上記の内容をGuestMemoryの<code>0x7000</code>に書き込むコードになっている。<br />
この<code>0x7000</code>のアドレスは後述するvCPUのRSIの値として書き込んでおく値になる。<br />
vCPUのレジスタ設定関係については<a href="./02-4_setup_registers_of_vcpu">Setup registers of vCPU</a>に記載しているので、本稿読了後に参照されたい。</p>
<h2 id="setup-e820"><a class="header" href="#setup-e820">Setup E820</a></h2>
<p>Guest OSのE820のセットアップを行うことで、OSやBootLoaderに対して利用可能なメモリ領域の報告できるようにしたい。
この辺りの処理は基本的にFirecrackerの実装に合わせて実装している。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>add_e820_entry(&amp;mut params, 0, EBDA_START, E820_RAM)?;
let first_addr_past_32bits = GuestAddress(FIRST_ADDR_PAST_32BITS);
let end_32bit_gap_start = GuestAddress(MMIO_MEM_START);
let himem_start = GuestAddress(HIGH_MEMORY_START);
let last_addr = guest_mem.last_addr();
if last_addr &lt; end_32bit_gap_start {
    add_e820_entry(
        &amp;mut params,
        himem_start.raw_value() as u64,
        last_addr.unchecked_offset_from(himem_start) as u64 + 1,
        E820_RAM)?;
} else {
    add_e820_entry(
        &amp;mut params,
        himem_start.raw_value(),
        end_32bit_gap_start.unchecked_offset_from(himem_start),
        E820_RAM)?;
    if last_addr &gt; first_addr_past_32bits {
        add_e820_entry(
            &amp;mut params,
            first_addr_past_32bits.raw_value(),
            last_addr.unchecked_offset_from(first_addr_past_32bits) + 1,
            E820_RAM)?;
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>上記のコードはToyVMMで起動するGuest VMのアドレス全体の設計を見ながら理解した方が良いだろう。そのため以下に、現状の実装におけるGuestのメモリ設計を以下の通り一覧にしておく。
この内容は今後変更される可能性があるため注意されたい。</p>
<table><thead><tr><th>Guest Address</th><th>Contents</th><th>Note</th></tr></thead><tbody>
<tr><td>0x0 - 0x9FBFF</td><td>E820</td><td></td></tr>
<tr><td>0x7000 - 0x7FFF</td><td>Boot Params (Header)</td><td>ZERO_PAGE_START(=0x7000)</td></tr>
<tr><td>0x9000 - 0x9FFF</td><td>PML4</td><td>Now only 1 entry (8byte), maybe expand later</td></tr>
<tr><td>0xA000 - 0xAFFF</td><td>PDPTE</td><td>Now only 1 entry (8byte), maybe expand later</td></tr>
<tr><td>0xB000 - 0xBFFF</td><td>PDE</td><td>Now 512 entry (4096byte)</td></tr>
<tr><td>0x20000 -</td><td>CMDLINE</td><td>Size depends on cmdline parameter len</td></tr>
<tr><td>0x100000</td><td></td><td>HIGH_MEMORY_START</td></tr>
<tr><td>0x100000 - 0x7FFFFFF</td><td>E820</td><td></td></tr>
<tr><td>0x100000 - 0x20E3000</td><td>vmlinux.bin</td><td>Size depends on vmlinux.bin's size</td></tr>
<tr><td>0x6612000 - 0x7FFF834</td><td>initrd.img</td><td>Size depends on initrd.img's size</td></tr>
<tr><td>0x7FFFFFF</td><td>GuestMemory last address</td><td>based on (128 &lt;&lt; 20 = 128MB = 0x8000000) - 1</td></tr>
<tr><td>0xD0000000</td><td></td><td>MMIO_MEM_START（4GB - 768MB）</td></tr>
<tr><td>0xD0000000 - 0xFFFFFFFF</td><td></td><td>MMIO_MEM_START - FIRST_ADDR_PAST_32BIT</td></tr>
<tr><td>0x100000000</td><td></td><td>FIRST_ADDR_PAST_32BIT (4GB~)</td></tr>
</tbody></table>
<p>コードを確認すると、GuestMemoryのサイズに非依存で設計しているアドレス帯（大まかに<code>0x0 ~ HIGH_MEMORY_START</code>のレンジ）は<code>0~EBDA_START(0x9FBFF)</code>の領域を共通でE820にUsableで登録している。
その後、GuestMemoryをどの程度確保しているかに従ってE820に登録している範囲が変化する。
現在の実装では、GuestのMemoryはデフォルトで128MBのメモリを確保するように実装しているためGuest Memoryは全体で<code>0x0 ~ 0x7FF_FFFF</code>になる。今回はこのレンジに<code>vmlnux.bin</code>の内容や<code>initrd.img</code>がマップされている。
つまり<code>guest_mem.last_addr() = 0x7FF_FFFF &lt; 0xD000_0000 = end_32bit_gap_start</code>のロジックに該当するので、<code>HIGH_MEMORY_START ~ guest_mem.last_addr()</code>のレンジを追加で登録している。
今後拡張していく中で、GuestMemoryのサイズが4GB超える場合は、<code>0x10_0000 ~ 0xD000_0000</code>と<code>0x1_000_0000 ~ guest_mem.last_addr()</code>のレンジを登録することになる。</p>
<p>後ほどVM起動時のコンソール出力を確認できるようになるが、ここでは確認のために先取ってVM起動時の一部の出力を添付する。
以下のように上記で設定したE820エントリが登録できている。</p>
<pre><code class="language-bash">[    0.000000] e820: BIOS-provided physical RAM map:
[    0.000000] BIOS-e820: [mem 0x0000000000000000-0x000000000009fbff] usable
[    0.000000] BIOS-e820: [mem 0x0000000000100000-0x0000000007ffffff] usable
</code></pre>
<h2 id="references-2"><a class="header" href="#references-2">References</a></h2>
<ul>
<li><a href="https://nishidy.hatenablog.com/entry/2016/09/08/230637">Linuxのブートシーケンスの基礎まとめ</a></li>
<li><a href="https://doc.kusakata.com/admin-guide/initrd.html">Linuxカーネルユーザ・管理者ガイド - 初期RAMdディスクを使用する</a></li>
<li><a href="https://manpages.ubuntu.com/manpages/bionic/ja/man4/initrd.4.html">initrd</a></li>
<li><a href="https://www.gcd.org/blog/2007/09/129/">initramfs(initrd)のinitをbusyboxだけで書いてみた</a></li>
<li><a href="https://blog.goo.ne.jp/pepolinux/e/4d1f4b6e0f5b5ed389fcec1f711b1408">initramfsとinitrdについて</a></li>
<li><a href="https://qiita.com/akachochin/items/d38b538fcabf9ff80531">initramfsについて</a></li>
<li><a href="http://archive.linux.or.jp/JF/JFdocs/kernel-docs-2.6/filesystems/ramfs-rootfs-initramfs.txt.html">filesystem/ramfs-rootfs-initramfs.txt</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setup-registers-of-vcpu"><a class="header" href="#setup-registers-of-vcpu">Setup registers of vCPU</a></h1>
<p>本稿では、vCPUのレジスタの設定について記載する。<br />
一口にレジスタと言ってもその種類は多岐に渡るため、それぞれどのレジスタをどう設定すると良いかを判断するのはかなり煩雑である。<br />
本稿で説明するRegisterに関わる内容はVMを起動するという側面にのみ焦点を置いた内容になっているので注意されたい。<br />
また、64-bit modeでGuest OSを起動したいので、64-bit modeに移行するためのいくつかの設定やその設定にまつわるPagingについても簡単に解説する。</p>
<h2 id="setup-vcpu-general-purpose-registers"><a class="header" href="#setup-vcpu-general-purpose-registers">Setup vCPU general purpose registers</a></h2>
<p>vCPUのgeneral purose registersは、KVMの<code>set_regs</code> APIを通してセットアップが可能である。<br />
今回は以下のようにレジスタの値を設定する。（レジスタ自体の詳細な説明は省く)</p>
<table><thead><tr><th>Register</th><th>Value</th><th>Meaning</th></tr></thead><tbody>
<tr><td>RFLAGS</td><td>2</td><td>0x02のbitは予約ビットで立てておかないといけない</td></tr>
<tr><td>RIP</td><td>KERNEL START ADDRESS (<code>0x0100_0000</code>)</td><td>ELFから取得したkernelのentry pointのアドレス</td></tr>
<tr><td>RSP</td><td>BOOT STACK POINTER (<code>0x8ff0</code>)</td><td>BOOT時に利用するStack Pointerのアドレス</td></tr>
<tr><td>RBP</td><td>BOOT STACK POINTER (<code>0x8ff0</code>)</td><td>BOOT処理実施前なのでRSPの値に合わせておく</td></tr>
<tr><td>RSI</td><td><code>boot_params</code> ADDRESS (<code>0x7000</code>)</td><td><code>boot_param</code>の情報が格納されているアドレス</td></tr>
</tbody></table>
<p>RIPはvCPU起動時の命令開始アドレスを格納する必要があり、今回はKernelのEntry Pointのアドレスを記載する。<br />
後述するが、<code>x64 Long Mode</code>を設定したCPUで実行するため、RIPのアドレスも仮想メモリアドレスとして扱われることになるが、Paging機構をIdentity Mappingで実装するため、<code>仮想メモリアドレス = 物理メモリアドレス</code>となり辻褄が合うことになる。<br />
RSP、RBPにはBootに必要なStackを格納するためのアドレスを入れておく。この辺りの値は空いている領域を使えば良い<br />
RSIには<a href="https://docs.kernel.org/x86/boot.html#id1">64-bit Boot Protocol</a>にも記載がある通り、<code>boot_params</code>構造体が格納されているアドレスを渡しておく必要がある。
ToyVMMはFirecrackerの値を模倣して作成しているため、RSP、RBP、RSIに格納するaddress値はFirecrackerのものを模倣している。</p>
<h2 id="setup-vcpu-special-registers"><a class="header" href="#setup-vcpu-special-registers">Setup vCPU special registers</a></h2>
<p>vCPUのspecial registersは、KVMの<code>set_sregs</code> APIを通してセットアップが可能である。<br />
ここでは実際にセットアップをしているレジスタにのみ焦点を当てつつ、その背景についても簡単にではあるが触れながら確認していく。<br />
ここからの説明では、これまで話題に上げてこなかった単語なども出てくることになるだろう。これらを一つ一つ説明していてはキリがないため、知らない単語に遭遇したらご自身で確認してほしい。</p>
<h4 id="idtinterrupt-descriptor-table"><a class="header" href="#idtinterrupt-descriptor-table">IDT(Interrupt Descriptor Table)</a></h4>
<p><a href="https://wiki.osdev.org/Interrupt_Descriptor_Table">IDT(Interrupt Descriptor Table)</a>とは、Protected modeとLong Modeにおける割り込み、例外に関する情報を保持するデータ構造である。<br />
もともとReal ModeではIVT（Interrupt Vector Table）というものが存在しており、これはISR(Interrupt Service Routine)がどこにあるかをCPUに対して教える役割を持っていた。<br />
要するに各割り込みや例外に対するハンドラを保持しており、それらが発生したとき、どのハンドラを起動すればいいか決定できるテーブルであった。</p>
<p>Protected modeやLong modeになるとRead Modeとは異なるアドレス表現になるため、それに対応した同様の能力をもつ機構がIDTである。<br />
IDTは最大255 Entryのテーブルであり、IDTのアドレスをIDTRレジスタに設定する必要がある。割り込みが発生した際、CPUはIDTRの値からIDTを参照し、指定された割り込みハンドラを実行する。</p>
<p><a href="https://docs.kernel.org/x86/boot.html#id1">64-bit Boot Protocol</a>での要求を確認すると、Interruptの設定はDisabledでなくてはならないという。
それに伴い、IDTに関する設定はToyVMM(Firecracker)の実装の中では省略されている、IDTについての説明もここまでにとどめておく。</p>
<h4 id="segumentation-gdtglobal-descriptor-table-ldtlocal-descriptor-table"><a class="header" href="#segumentation-gdtglobal-descriptor-table-ldtlocal-descriptor-table">Segumentation, GDT(Global Descriptor Table), LDT(Local Descriptor Table)</a></h4>
<p>GDTの話を始める前に、まずはSegumentationについて軽く導入しておく。<br />
メモリセグメンテーションはメモリ管理方式の一つであり、プログラムやデータをセグメントと呼ばれる可変なまとまりで管理する方式である。<br />
セグメントはメモリ空間上で情報の属性などによって分類されたグループであり、仮想記憶やメモリ保護機能を実現する方式の一つである。<br />
Linuxではフラットメモリを前提としたセグメンテーションとPagingを併用しているため、以降ではそれを前提として話を進める。</p>
<p><a href="https://wiki.osdev.org/Interrupt_Descriptor_Table">GDT(Global Descriptor Table)</a>は、メモリセグメントを管理するためのデータ構造である。<br />
このデータ構造はIDTのものと非常によく似通っている。
GDTはSegment Descriptorと呼ばれる複数のEntryを持つテーブルであり、GDTのアドレスをGDTRレジスタに設定する必要がある。<br />
このTableのエントリは、Segment Selectorによってアクセスされ、該当するアドレス領域はどこかという情報や、その領域ではどの様な操作が許可されているかなどの情報を得ることができる。
Segument SelectorはSegumentation RegistersやIDTの各EntryのフォーマットであるGate Descriptor、Task State Segumentなどの中に現れるものである。<br />
詳細については本稿では説明を省略しているため、気になった場合は調べてみてほしい</p>
<p><a href="https://wiki.osdev.org/Local_Descriptor_Table">LDT(Local Descriptor Table)</a>は、GDTと同様にメモリにアクセスするためのセグメントを管理するデータ構造であるが、タスクやスレッド毎にLDTを保有できるという点で違いがある。<br />
タスク毎にGDTに相当するディスクリプタを持たせることは、自身のプログラム、タスク間ではセグメントを共有しつつ、異なるタスクとはセグメントを分離することができるため、タスク間のセキュリティを高めることに寄与する。
LDTも今回の実装では関わってこない話なので、この詳細についてもここでは省略する。</p>
<h3 id="gdt-setup-for-64-bit-mode"><a class="header" href="#gdt-setup-for-64-bit-mode">GDT setup for 64-bit mode</a></h3>
<p><a href="https://docs.kernel.org/x86/boot.html#id1">64-bit Boot Protocol</a>にも記載がある通り、64-bit modeの場合はそれぞれのSegment descriptorは4G flat segmentとしてセットアップする必要があり、Code Segument、Data Segumentはそれぞれ適切な権限を付与する必要がある。
その一方<a href="https://wiki.osdev.org/Global_Descriptor_Table">Gloabl Descriptor Table</a>を確認すると、64-bit modeの場合は基本的にbase, limitが無視され、各Descriptorは全体のリニアアドレススペースをカバーするという記載があるため、Flag以外についてはどの様な値を書いていてもよさそうではある。今回は念の為、明示的にflat segumentとしてセットアップを行った。
また、<code>DS</code>、<code>ES</code>、<code>SS</code>の値はDSと同一にする必要がある旨についても記載があるため、これに習って実装する。</p>
<p>以降では、ToyVMM(Firecrackerと読み替えていただいても差し支えない）の実装を参考にこれらがどの様に設定されているかを確認してみる。
この設定は<code>configure_seguments_and_sregs</code>関数で実施されている。説明をしやすくするために、一部コメントを追記している</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn configure_segments_and_sregs(sregs: &amp;mut kvm_sregs, mem: &amp;GuestMemoryMmap) -&gt; Result&lt;(), RegError&gt; {
    let gdt_table: [u64; BOOT_GDT_MAX as usize] = [
        gdt::gdt_entry(0, 0, 0),            // NULL
        gdt::gdt_entry(0xa09b, 0, 0xfffff), // CODE
        gdt::gdt_entry(0xc093, 0, 0xfffff), // DATA
        gdt::gdt_entry(0x808b, 0, 0xfffff), // TSS
    ];
    // &gt; https://wiki.osdev.org/Global_Descriptor_Table
    //
    //              55 52     47     40 39        31               16 15                0
    // CODE: 0b0..._1010_1111_1001_1011_0000_0000_0000_0000_0000_0000_1111_1111_1111_1111
    //              &lt;-f-&gt;     &lt;-Access-&gt;&lt;---------------------------&gt; &lt;----- limit -----&gt;
    // - Flags  : 1010      =&gt; G(limit is in 4KiB), L(Long mode)
    // - Access : 1001_1011 =&gt; P(must 1), S(code/data type), E(executable), RW(readable/writable), A(CPU access allowed)
    //   - 0xa09b of A,9,B represents above values
    //
    // DATA: 0b0..._1100_1111_1001_0011_0000_0000_0000_0000_0000_0000_1111_1111_1111_1111
    // - Flags  : 1100      =&gt; G(limit is in 4KiB), DB(32-bit protected mode)
    // - Access : 1001_0011 =&gt; P(must 1), S(code/data type), RW(readable/writable), A(CPU access allowed)
    //
    // TSS
    // - Flags  : 1000      =&gt; G(limit is in 4KiB)
    // - Access : 1000_1011 =&gt; P(must 1), E(executable), RW(readable/writable), A(CPU access allowed)
    //    - TSS requires to support Intel VT
    let code_seg = gdt::kvm_segment_from_gdt(gdt_table[1], 1);
    let data_seg = gdt::kvm_segment_from_gdt(gdt_table[2], 2);
    let tss_seg = gdt::kvm_segment_from_gdt(gdt_table[3], 3);

    // Write seguments
    write_gdt_table(&amp;gdt_table[..], mem)?;
    sregs.gdt.base = BOOT_GDT_OFFSET as u64;
    sregs.gdt.limit = mem::size_of_val(&amp;gdt_table) as u16 - 1;

    write_idt_value(0, mem)?;
    sregs.idt.base = BOOT_IDT_OFFSET as u64;
    sregs.idt.limit = mem::size_of::&lt;u64&gt;() as u16 - 1;

    sregs.cs = code_seg;
    sregs.ds = data_seg;
    sregs.es = data_seg;
    sregs.fs = data_seg;
    sregs.gs = data_seg;
    sregs.ss = data_seg;
    sregs.tr = tss_seg;

    // 64-bit protected mode
    sregs.cr0 |= X86_CR0_PE;
    sregs.efer |= EFER_LME | EFER_LMA;
    Ok(())
}
<span class="boring">}
</span></code></pre></pre>
<p>上記ではセットアップするGDTとして4 Entryを持つテーブルを作成している。
最初のEntryはGDTの要求としてNullでなければならないため、そのようなエントリを作成している。
それ以外は全体のメモリ領域に対して、CODE Segment、DATA Segment、TSS Segmentの設定を行なっていることが分かるだろう。<br />
TSSの設定はIntel VTの要求を満たすために設定されており、本資料の範疇では実質使用しない内容である。</p>
<p>さて、このGDTを作成する際に各エントリを作成する関数<code>gdt_entry</code>を呼び出しているが、この内容を以下に転載する。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn gdt_entry(flags: u16, base: u32, limit: u32) -&gt; u64 {
    ((u64::from(base) &amp; 0xff00_0000u64) &lt;&lt; (56 - 24))
        | ((u64::from(flags) &amp; 0x0000_f0ffu64) &lt;&lt; 40)
        | ((u64::from(limit) &amp; 0x000f_0000u64) &lt;&lt; (48 - 16))
        | ((u64::from(base) &amp; 0x00ff_ffffu64) &lt;&lt; 16)
        | (u64::from(limit) &amp; 0x0000_ffffu64)
}
<span class="boring">}
</span></code></pre></pre>
<p>この関数の引数として、全てのエントリがbaseに<code>0x0</code>、limitに<code>0xFFFFF</code> (<code>2^5 = 32bit = 4GB</code>)を指定しているためフラットなセグメンテーションになっている。第一引数であるflagsについてはEntry毎に設定を行なっており、これが翻ってGDTの<code>Flags</code>や<code>AccessByte</code>の値に対応するようになる。<br />
実際にそれぞれのEntryを<code>gdt_entry</code>に与えた結果返却される値と、その値を解析した内容が上記コード上のコメントになっている。
コメントを確認すると、<a href="https://docs.kernel.org/x86/boot.html#id1">64-bit Boot Protocol</a>で要求されていた通り、CODE SegumentにはExecute / Read permissionと、さらに<code>long mode (64-bit code segment)</code>のフラグが、DATA SegumentにはRead / Write permissionが付与されていることが分かる。
上記の通り作成したGDTを、<code>write_gdt_table</code>関数でGuestMemory上に書き込み、その先頭アドレスを<code>sregs.gdt.base</code>に残している。</p>
<p>後続するIDTの設定だが、上述した通りここはdisabledとなるようだ。そのためか、特に何もメモリ上に書き込んでない。ただしGuestMemory上のどの位置を利用するかについては決めてあり、そのアドレスを<code>sregs.idt.base</code>に残している。</p>
<p>引き続き、そのほかのレジスタ値を設定する。
上述した通り<code>CS</code>にはCODE segumentの情報を、<code>DS</code>, <code>ES</code>, <code>SS</code>にはData Segumentの情報を、<code>TR</code>にはTSS Segumentの情報を格納しておく。<br />
上記のコードでは<code>FS</code>, <code>GS</code>にもDATA Segumentの情報を書いているが、これらのセグメントの値はおそらく設定しなくても良い。</p>
<p>最後に、CR0やEFERレジスタの設定をしているがこの説明は後述する。</p>
<h3 id="64-bit-protected-mode"><a class="header" href="#64-bit-protected-mode">64-bit protected mode</a></h3>
<p><code>Long mode</code>とはx86_64プロセッサ用のネイティブモードであり、従来（<code>x86</code>）に比べていくつかの追加機能がサポートされているが、ここではこれらについて詳細には記載しない<br />
<code>Long mode</code>はさらに<code>64-bit mode</code>と<code>互換モード</code>の2つのサブモードから構成される。</p>
<p>64-bitモードに切り替えるには、以下の処理が必要になる</p>
<ul>
<li>CR4.PAEを設定し、物理アドレス拡張機構を有効化する</li>
<li>Page Tableの作成、CR3レジスタへトップレベルページテーブルのアドレスを読み込む</li>
<li>CR0.PGを設定し、Pagingの有効化する</li>
<li>EFER.LMEを設定し、Long Modeの有効化する</li>
</ul>
<p>レジスタ値の設定は<code>kvm_sregs</code>構造体のうち対応するものを更新し<code>set_sregs</code>で設定するだけであり、既に説明済みであるため同様に実施すれば良い。<br />
それ以外に重要な作業としてPage Tableの作成がある。
特に64-bit modeに移行するためには、4-Level Page Tableを構築する必要があるため、これに焦点をしぼって以降簡単にPagingについて説明をする。</p>
<h4 id="4-level-page-table-for-entering-64-bit-mode"><a class="header" href="#4-level-page-table-for-entering-64-bit-mode">4-Level Page Table for entering 64-bit mode</a></h4>
<p>これまで特に言及をしてこなかったが、Linux Kernelの起動に関わる処理は、利用できるメモリアドレス空間の違いによって何段階かに名称分けされている。
起動直後、物理メモリアドレスを直接触ってセットアップを進める処理は、<code>x16 Real-Mode</code>と呼ばれ、その名の通り16bitのメモリアラインメントで処理が進んでいく。<br />
一方、読者もよく知っている通り、我々の馴染みがあるOSは<code>32bit</code>であったり、<code>64bit</code>である。<br />
これらはCPUのモード切り替えと呼ばれる機能により、<code>x32 Protected Mode</code>、<code>x64 Long Mode</code>と呼ばれるモードに切り替えられるが、これらのモードに切り替えられた途端、CPUは仮想メモリアドレスしか利用できない状態になる。<br />
また、特にx64 CPUアーキテクチャでは基本的に<code>4-level page table</code>によって、64bit 仮想アドレスが物理アドレスに変換されることが期待される.
つまり、<code>x64 Long Mode</code>に切り替える前に<code>4-level page table</code>を構成してCPUに伝える必要があり、この処理はBootLoaderの機能の一部として実装される。</p>
<p>さてもう一つ重要な点としては、今<code>RIP</code>の値にはカーネルのエントリーポイントを示す<strong>物理アドレスの値</strong>が格納されているが、<code>x64 Long Mode</code>で取り扱う際にこのアドレスが<strong>仮想アドレスの値として利用される</strong>ため、別の物理アドレスに変換されてしまうとOSが起動できなくなってしまう。<br />
したがって、ここではまず仮想メモリアドレスが同じ物理メモリアドレスにマッピングされる簡単なページテーブル（これは特にIdentity Mappingと呼ばれる）を作成すること上記の問題に対応する</p>
<p><strong>Note</strong><br />
ここでBootLoaderが作成するPage Tableはx64でカーネルを実行するために一時的に必要な処理であることに注意されたい。<br />
通常我々が仮想メモリアドレスやPage Tableと聞いた時に多くの場合に思い浮かべるのは、ユーザスペースのプロセスに対してのアドレスの話であるが、このユーザプロセスに対するPagingの仕組みはカーネル内部に実装があり、カーネルの起動とともに構成されるものであるため、今回の話とは切り離して考えるべきである。<br />
つまり、このBootLoaderのpage tableの変換の仕組みがIdentity Mappingであろうがなかろうが、OS起動後の各プロセスに対するPagingの仕組みには影響がないということである。</p>
<h4 id="page-table-implementation-in-toyvmm"><a class="header" href="#page-table-implementation-in-toyvmm">Page Table implementation in ToyVMM</a></h4>
<p>ここではToyVMMの実装を具体的に見ていきながら、Page Tableの構成について理解を深める。<br />
この実装はFirecrackerの実装を模倣しているため、実質的にFirecrackerの実装と読み替えていただいて問題ない。</p>
<p>まずは、簡単に4-Level Page Tableの構造について議論しておく。基本的には以下の名称でLevel毎にTableが存在しそれぞれ名称分けされている。</p>
<ul>
<li>Level 4: Page Map Level 4 (PML4)</li>
<li>Level 3: Page Directory Pointer Table (PDPT)</li>
<li>Level 2: Page Directory Table (PDT)</li>
<li>Level 1: Page Tables (PT)</li>
</ul>
<p>また、各Tableはそれぞれ512個のEntityを格納可能であり、一つのEntityは8byte（64bit）からなるため、Table全体としては<code>512(entity) * 8(byte/entity) = 4096(byte)</code>となる。これは1つのPage（<code>4KB</code>）にちょうど収まるサイズになっている。
それぞれのLevelのEntiryは以下のような構造になっている<br />
（引用元: <a href="https://alessandropellegrini.it/didattica/2017/aosv/1.Initial-Boot-Sequence.pdf">x86 Initial Boot Sequence</a>、<a href="https://wiki.osdev.org/Paging">OSdev/Paging</a>。64bitのうちHigh bitについては今回あまり重要ではないので紙面の都合上省略している）</p>
<img src="./02_figs/cr3_and_paging.svg" width="100%">
<p>上記から、以下の内容を満たしながらセットアップすればよさそうである。</p>
<ul>
<li>CR3のなかで、PML4のアドレスとして利用されるデータは12~32+bitになるため、これを考慮してPML4のアドレスを設計する</li>
<li>PML4は有効化のために0bit目は1にセットし、12~32+bitにPDPTのアドレスを設計する</li>
<li>PDPTE page directoryのレイアウトを利用するために、PDPTEの7bit目は立てず、12~32+bitのレンジにPDのアドレスを設計する</li>
<li>PDEでは2MB pageを許可するため7bit目を立て、21~32+bitのレンジにPhysical Addressを設計する
<ul>
<li>Firecrackerでは、Level1 Page Tableを利用せず(= 4KiB pageを利用せず)、2MiB Pagingで実装しているようである。ToyVMMの実装もこれに倣う（2MiBページングは基本的に多くのCPUでサポートされている上、4KiBページングによるPage Tableの肥大化を伏せるためかと思われる）</li>
</ul>
</li>
</ul>
<p>さて、上記をもとに実際の実装部分のコードを抜粋する</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn setup_page_tables(sregs: &amp;mut kvm_sregs, mem: &amp;GuestMemoryMmap) -&gt; Result&lt;(), RegError&gt; {
    let boot_pml4_addr = GuestAddress(PML4_START);
    let boot_pdpte_addr = GuestAddress(PDPTE_START);
    let boot_pde_addr = GuestAddress(PDE_START);

    // Entry converting VA [0..512GB)
    mem.write_obj(boot_pdpte_addr.raw_value() as u64 | 0x03, boot_pml4_addr)
        .map_err(|_| RegError::WritePdpteAddress)?;
    // Entry covering VA [0..1GB)
    mem.write_obj(boot_pde_addr.raw_value() as u64 | 0x03, boot_pdpte_addr)
        .map_err(|_| RegError::WritePdpteAddress)?;
    // 512 MB entries together covering VA [0..1GB).
    // Note we are assuming CPU support 2MB pages (/proc/cpuinfo has 'pse').
    for i in 0..512 {
        mem.write_obj((i &lt;&lt; 21) + 0x83u64, boot_pde_addr.unchecked_add(i * 8))
            .map_err(|_| RegError::WritePdeAddress)?;
    }
    sregs.cr3 = boot_pml4_addr.raw_value() as u64;
    sregs.cr4 |= X86_CR4_PAE;
    sregs.cr0 |= X86_CR0_PG;
    Ok(())

}
<span class="boring">}
</span></code></pre></pre>
<p>見た通り実装はかなりシンプルである。<br />
<code>PML4_START</code>、<code>PDPTE_START</code>、<code>PDE_START</code>にはそれぞれアドレス値がハードコードされており、それぞれ<code>PML4_START=0x9000</code>、<code>PDPTE_START=0xa000</code>、<code>PDE_START=0xb000</code>となっていて、これは上記したそれぞれのアドレス設計の要求を満たしている。<br />
上記を見ると分かる通り、<code>PML4</code>及び<code>PDPT</code> Table自体は1つずつであり、Entryとしても最初のものしかセットアップしていない。これはこのページテーブルで変換されるカーネルのアドレスが、<code>0x0100_0000</code>であり、これを仮想アドレスとして扱った場合、詳しくは後述するが、<code>PML4</code>、<code>PDPT</code>は必ず最初のEntryを見ることになるためこの実装で十分である。<br />
PML4には、PDPTの先頭アドレス情報に0x03の論理和を取ったものを書き込み、PDPTにも同様にPDの先頭アドレス情報に0x03の論理和を取ったものを書き込む。
ここで0x03で論理和を取っている理由は、<code>PML4E</code>、<code>PDPTE</code>の0, 1bit目のフラグを立てるためであり、1bit目は共にR/W許可に関するフラグ、0bitは共に当該Entryの存在性に関してのフラグに該当するため今回のケースでは必須の処理である。<br />
PDは512 Entry分作成するためにループし、<strong>ループのindexの値を</strong>21bit shiftさせたものと0x83の論理和を取ったものを、PDの先頭アドレスから8byte毎(=1 Entry size毎）に書き込んでいる。<br />
ここで0x83で論理和を取っている理由は、前述しているR/W許可フラグ、存在性確認のフラグに加えて、2MBのpage frameとして扱うかどうかに関わるフラグを立てるためである。このフラグを立てることによって、21bit目からの値をアドレスとして取り扱う（図の「PDE 2MB page」のレイアウトを利用することになる）。したがって、<strong>上述したループで格納したindex値を21bitオフセット（2^21 = 2MB）させた値がそのまま変換後の物理アドレス値に対応するとになる</strong>ため、PDEとしてはindex=0のEntryは0を21bit offsetした値(= <code>0x0000_0000</code>)、index=1のEntryは1を21bit offsetした値(= <code>0x0010_0000</code>)というように変換されることになる。</p>
<p>さて、以降では上記で作成したPage Tableで、EIPに格納したkernelのアドレス（<code>0x0100_0000</code>）は正しく変換されるのか実際に計算して確かめてみよう！<br />
前述した通り、<code>x64 Long Mode</code>へ移行すると、このカーネルのアドレスは64bit仮想アドレスとして取り扱われることになる。今、ToyVMM（およびFirecracker）ではカーネルを物理アドレスの<code>0x0100_0000</code>に読み込んでおり、その値が<code>eip</code>レジスタに格納されている。
したがって、<code>0x0100_0000</code>を仮想アドレスとして扱い、上述した変換テーブルを用いてアドレス変換を行った結果、<code>0x0100_0000</code>になることを期待したい。</p>
<p>では具体的に計算してみよう。64bit仮想アドレスを4-Level Page Tableで変換する場合は以下の図の様に仮想アドレスの下位48bitを<code>9 + 9 + 9 + 9 + 12</code>bit毎に分割し、4つの9bitを先頭からそれぞれ各Page tableのEntryのindex値として利用する。この方法で特定したEntryのレイアウトを確認して、次のPage Tableの物理アドレスを確認し、同様にその物理アドレスと仮想アドレスから得たEntryのindex値を元に次のPage Tableの対象のEntryを割り出す。これを続けると最終的に目的の物理アドレスを得ることができる。Pageは少なくとも4KB毎のサイズになっているため、アドレス値としても4KB毎の値になるため、仮想アドレスの最後の12bitはそのオフセット(<code>2^12 = 4KB</code>)として利用される。</p>
<img src="./02_figs/4-level-page-table_and_address_translation.svg" width="100%">
<p>今回はPDEで2MB page frameとして取り扱うフラグを立てていることを思い出してほしい。この場合はPDEから得られる結果をそのまま物理アドレスへのマッピングとして利用する。この時使われないPTEの9bit分はオフセットとして取り扱われ、元々の12bitと合わせて合計21bit分のoffsetを加えることになる。この21bitオフセットが2MBに対応していることになる。同様にPDPTEでフラグを立てていると1GB page frameとして扱われるという仕組みになっている。</p>
<p>上記の話をもとに、<code>0x0100_0000</code>を変換してみる。この値はわかりやすさのためにbitで表現すると<code>0b0..._0000_0001_0000_0000_0000_0000_0000_0000</code>である。これを仮想アドレス変換の方式に倣いbitを分解すると、以下の通りになる。</p>
<table><thead><tr><th>Entry index for</th><th>Range of Virtual Address</th><th>Value</th></tr></thead><tbody>
<tr><td>Page Map Level4 (PML4)</td><td>47 ~ 39 bit</td><td><code>0b0_0000_0000</code></td></tr>
<tr><td>Page Directory Pointer Table (PTPT)</td><td>38 ~ 30 bit</td><td><code>0b0_0000_0000</code></td></tr>
<tr><td>Page Directory Table (PDT)</td><td>29 ~ 21 bit</td><td><code>0b0_0000_1000</code></td></tr>
<tr><td>Page Tables (PT)</td><td>20 ~ 12 bit</td><td><code>0b0_0000_0000</code></td></tr>
<tr><td>-</td><td>11 ~ 0  bit (offset)</td><td><code>0b0_0000_0000</code></td></tr>
</tbody></table>
<p>これを見ると分かる通り、<code>PML4E</code>、<code>PDPTE</code>用のindex値は<code>0</code>になるため、それぞれのTableの先頭アドレスから64bit確認することになる。
実装で確認した通り、index=0の<code>PML4E</code>には<code>PDPT</code>のアドレスが書かれており、index=0の<code>PDPTE</code>にはPDTのアドレスが書かれているため、<code>PDT</code>までは順当に辿り着く。
さて、今回PDEのIndex値は上記の仮想メモリアドレスから<code>0b0_0000_1000</code>なので<code>PDT</code>の中で8番目のEntryを確認することになるが、当該Entryの2MB Page frameの領域に書かれている値は実装から<code>0b0...0000_1000</code>であることが分かる。
したがって、この値に21bit offsetを加えた値である<code>0b1_0000_0000_0000_0000_0000_0000 = 0x100_0000</code>が変換により得られる物理アドレスであり、これは入力した仮想アドレスと一致している。
したがって、変換後もカーネルのエントリーポイントを指すことになり、64-bit long modeにシフトしてもカーネルを起動から処理が開始されることになる</p>
<img src="./02_figs/virt_phys_address_translation_example.svg" width="100%">
<p><strong>Note</strong><br />
今回作成したPage Tableを再考してみると、PML4、PDPT用のEntryは1つしか作っていないので、そもそも対象となる仮想メモリアドレス範囲はMaxでも<code>2^31 - 1</code>までの範囲になる(この領域を超える場合、PML4E、PDPTEとしてindexが0以外を指す場合が存在してしまう)
加えて、PDのEntryでは2MB page frameを有効化しているため、仮想メモリアドレスの下位21bitはOffsetとして取り扱われる。
その上で、PDEのアドレス設計をindexに対応付けているため、このPage Tableは <strong><code>2^21 ~ 2^30-1</code>の範囲でIdentity Mapping</strong> になっている。</p>
<h2 id="what-to-do-next"><a class="header" href="#what-to-do-next">What to do next?</a></h2>
<p>実はここまでの話を組み合わせるだけで、Guest VMを起動すること自体は可能である。<br />
しかし、この状態ではGuest VMは起動できてもそれを操作することはできないという何とも片手落ちな状態になってしまう。<br />
起動したGuest VMが我々の想定した通りの設定になっているかなどを確認するためにも、Guest VMを操作するようなインターフェイスを作りたいところである。<br />
次の章ではこれを達成するために<code>Serial</code>について議論し、ToyVMMの中に実装することでGuest VMを起動後にキーボード操作できるようにする！</p>
<h2 id="references-3"><a class="header" href="#references-3">References</a></h2>
<ul>
<li><a href="https://docs.kernel.org/x86/boot.html#id1">The Linux/x86 Boot Protocol - 64-bit Boot Protocol</a></li>
<li><a href="https://postd.cc/linux-bootstrap-4/">Linux Insides: カーネル起動プロセス part4</a></li>
<li><a href="https://wiki.osdev.org/Interrupt_Descriptor_Table">Global Descriptor Table (wiki)</a></li>
<li><a href="https://wiki.osdev.org/Interrupt_Descriptor_Table">Interrupt Descriptor Tabke (wiki)</a></li>
<li><a href="https://wiki.osdev.org/Segmentation">Segmentation (wiki)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Control_register#CR3">Control register (wiki)</a></li>
<li><a href="https://wiki.osdev.org/Segmentation">Long mode (wiki)</a></li>
<li><a href="https://alessandropellegrini.it/didattica/2017/aosv/1.Initial-Boot-Sequence.pdf">x86 initial boot sequence</a></li>
<li><a href="https://back.engineering/23/08/2020/">Virtual Memory - Intro to Paging Tables</a></li>
<li><a href="https://os.phil-opp.com/paging-introduction/">Writing an OS in Rust - Introduction to Paging</a></li>
<li><a href="https://www.intel.com/content/dam/support/us/en/documents/processors/pentium4/sb/25366821.pdf">Intel 64 and IA-32 Architectures Software Developer's Manual</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="serial-console-implementation"><a class="header" href="#serial-console-implementation">Serial Console implementation</a></h1>
<h2 id="about-serial-uart-and-ttys0"><a class="header" href="#about-serial-uart-and-ttys0">About Serial UART and ttyS0</a></h2>
<p>UART（Universal Asynchronous Receiver/Transmitter）はコンピュータやマイコンと周辺機器を繋ぐ非同期式シリアル通信規格である。
UARTによってシリアル・パラレル信号の相互変換を行えるため、入力されるパラレルデータをシリアルデータへ変換し通信回線越しに相手に送信することができる。
これを実装するために設計された集積回路として8250 UARTと呼ばれるデバイスが製造され、その後さまざまなファミリが登場してきた。</p>
<p>さて、今回Guest OS（Linux）を起動させようとしているわけであるが、デバッグ等の用途としてシリアルコンソールが存在すると便利なケースが多い。<br />
シリアルコンソールはGuestの全てのコンソール出力をシリアルポートに送付するため、シリアルターミナルが適切に設定されていればリモートターミナルとしてシステムの起動状況を確認したり、シリアルポート経由でシステムにログオンしたりすることができる。<br />
今回は、ToyVMM上で起動したGuest VMの状態確認やGuestの操作をするためにこの方式を利用することにする。</p>
<p>コンソールメッセージをシリアルポートに出力させるために、カーネルの起動時パラメータとして<code>console=ttyS0</code>を設定する必要がある。<br />
現在のToyVMMの実装ではこの値をデフォルト値として与えている。</p>
<p>問題はこれを受け取るシリアルターミナル側である。
シリアルポートに該当するIO Portのアドレスは決まっているため、ToyVMMのレイヤからは当該アドレス付近に対して<code>KVM_EXIT_IO</code>の命令を受けることになる。
つまりGuest OS側から発行されるシリアルコンソールへの出力情報、またそれ以外にも必要なセットアップ要求などを適切に処理する必要があり、これはUARTデバイスをエミュレートすることで成立させる必要がある。
その上で、デバイスをエミュレートした結果として、標準出力に対してコンソール出力を出力したり、逆に我々の標準入力をGuest VM側に反映させることができれば、ToyVMMからVMを起動した際に、その起動情報の確認やGuestの操作を手元のターミナルから実施することが可能になる。</p>
<p>以上をまとめると、大まかに下記のような概念図のものを作成することになる。</p>
<img src="./02_figs/overview_serial_device.svg" width="100%">
<p>以降では順を追って説明していく。</p>
<h2 id="serial-uart"><a class="header" href="#serial-uart">Serial UART</a></h2>
<p>Serial UARTについては、以下のLammet Biesの資料とWikibooksに大変詳細な情報が記載されているため基本的にはこれを確認すれば良い。</p>
<ul>
<li><a href="https://www.lammertbies.nl/comm/info/serial-uart">Serial UART information (Lammet Bies)</a></li>
<li><a href="https://en.wikibooks.org/wiki/Serial_Programming/8250_UART_Programming">Serial Programming / 8250 UART Programming (Wikibooks)</a></li>
</ul>
<p>以下の図は、<a href="https://www.lammertbies.nl/comm/info/serial-uart">Lammetの資料に記載のある図</a>を引用しつつそれぞれのRegisterの各bitに関して簡単に説明を加えた図である。本資料の執筆において個人的に作成した図であるが、読者の理解の手助けになることを期待して添付しておく。
ただし、それぞれのregisterやbitの意味については本資料では説明はしないため、上記の資料を参考に確認してもらいたい。</p>
<img src="./02_figs/serial_uart_registers.svg" width="100%">
<p>基本的には上記のregisters/bitsを操作することで必要な処理を行うのがUARTの仕組みになっている。<br />
今回はこれをSoftwareでエミュレートする必要があるが、この実装に関しては <a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>を利用することで代替とする。<br />
以降でこの <a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a> の実装と上記のSpecificationとを比較しながら簡単にではあるが確認していこうと思う。</p>
<h2 id="rust-vmmvm-superioによるserial-deviceのsoftware実装"><a class="header" href="#rust-vmmvm-superioによるserial-deviceのsoftware実装">rust-vmm/vm-superioによるSerial DeviceのSoftware実装</a></h2>
<h3 id="初期値設定rwの実装"><a class="header" href="#初期値設定rwの実装">初期値設定／RWの実装</a></h3>
<p>ここからは<a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>を利用したserial deviceの実装を、上記のSpecificationと比較しながら確認していく。
是非、上記からコードを取得して自分で確認してみてほしい。
なお、以下の内容は <code>vm-superio-0.6.0</code> に準拠しているので、最新のコードでは変更されているかもしれないため留意されたい。</p>
<p>まず、いくつかの値の初期値について以下の通りに整理する。<br />
<a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>はもともとVMMでの利用を前提にしているため、いくつかのレジスタの値を初期設定していたり、書き換え想定をしていなかったりする。</p>
<table><thead><tr><th>Variable</th><th>DEFAULT VALUE</th><th>Meaning</th><th>REGISTER</th></tr></thead><tbody>
<tr><td>baud_divisor_low</td><td>0x0c</td><td>baud rate 9600 bps</td><td></td></tr>
<tr><td>baud_divisor_high</td><td>0x00</td><td>baud rate 9600 bps</td><td></td></tr>
<tr><td>interrupt_enable</td><td>0x00</td><td>No interrupts enabled</td><td>IER</td></tr>
<tr><td>interrupt_identification</td><td>0b0000_0001</td><td>No pending interrupt</td><td>IIR</td></tr>
<tr><td>line_control</td><td>0b0000_0011</td><td>8 bit word length</td><td>LCR</td></tr>
<tr><td>line_status</td><td>0b0110_0000</td><td>(1)</td><td>LSR</td></tr>
<tr><td>modem_control</td><td>0b0000_1000</td><td>(2)</td><td>MCR</td></tr>
<tr><td>modem_status</td><td>0b1011_0000</td><td>(3)</td><td>MSR</td></tr>
<tr><td>scratch</td><td>0b0000_0000</td><td></td><td>SCR</td></tr>
<tr><td>in_buffer</td><td>Vec::new()</td><td>vector values (buffer)</td><td>-</td></tr>
</tbody></table>
<ol>
<li>THR empty関係のbitを立てている。このbitを立てているといつでもデータを受信可能であることを表現していることになるが、これは仮想デバイスとしての利用が前提の設定になっている。</li>
<li>多くのUARTはAuxiliary Output 2の値を1に設定し、interruptを有効にするたデフォルトで有効にしている</li>
<li>Connectedの状態、かつハードウェアデータフローの初期化</li>
</ol>
<p>さて、次にwriteのリクエストが来た場合の処理内容について簡単に記載する。
<code>KVM_EXIT_IO</code>の結果として、IOが発生したaddressと、書き込むべきデータが渡される。
ToyVMM側ではこれらの値から適切なデバイス（今回はSerial UART device）とそのベースアドレスからのOffsetを計算し、<code>vm-superio</code>で定義されている<code>write</code>関数を呼び出す。
以下の内容は、<code>Serial::write</code>の処理を簡単に表に起こしたものである。基本的には素直にレジスタ値の書き換えになるが一部ちょっとロジックが入る。</p>
<table><thead><tr><th>Variable</th><th>OFFSET(u8)</th><th>Additional Conditions</th><th>write</th></tr></thead><tbody>
<tr><td>DLAB_LOW_OFFSET</td><td>0</td><td>is_dlab_set = true</td><td><code>self.baud_divisor_low</code>を書き換え</td></tr>
<tr><td>DLAB_HIGH_OFFSET</td><td>1</td><td>is_dlab_set = true</td><td><code>self.baud_divisor_high</code>を書き換え</td></tr>
<tr><td>DATA_OFFSET</td><td>0</td><td>- (is_dlab_set = false)</td><td>(1)</td></tr>
<tr><td>IER_OFFSET</td><td>1</td><td>- (is_dlab_set = false)</td><td>(2)</td></tr>
<tr><td>LCR_OFFSET</td><td>3</td><td>-</td><td><code>self.line_control</code>を書き換え</td></tr>
<tr><td>MCR_OFFSET</td><td>4</td><td>-</td><td><code>self.modem_control</code>を書き換え</td></tr>
<tr><td>SCR_OFFSET</td><td>7</td><td>-</td><td><code>self.scratch</code>を書き換え</td></tr>
</tbody></table>
<ol>
<li>現在のSerialの状態として、LOOP_BACK_MODE（MCR bit 4）が有効になっている場合とそうでない場合で場合分け
<ul>
<li>有効の場合、送信レジスタに書かれたものをそのまま受信レジスタに書き込まれる(loopbackする）ようにシミュレート（今回は重要ではない）</li>
<li>有効でない場合、書き込むべきデータをそのまま出力に書き出し、既存の設定状態に依存して割り込みを入れる。
<ul>
<li>上記の表をみてわかる通り、外部からのwriteによるIIRの変更はサポートしておらず、デフォルト値が<code>0b0000_0001</code>で設定されている。</li>
<li>もし、IER_OFFSETに対してIERのTHR empty bitのフラグが立っている場合は、IIRのTHR emptyに対応するフラグを立てて割り込みをトリガする。</li>
</ul>
</li>
</ul>
</li>
<li>IERのbitのうち0-3bit以外はMaskした結果（Interruptに関連するbitのみそのままにして）で <code>self.interrupt_enable</code>を書き換え</li>
</ol>
<p>次に、readのリクエストが来た場合の処理内容について簡単に記載する<br />
同様に <code>Serial::read</code>の処理を表に起こしたものが下記である。readの場合はwriteと異なり基本的には返り値としてデータを返すロジックになっている</p>
<table><thead><tr><th>Variable</th><th>OFFSET(u8)</th><th>Additional Conditions</th><th>write</th></tr></thead><tbody>
<tr><td>DLAB_LOW_OFFSET</td><td>0</td><td>is_dlab_set = true</td><td><code>self.baud_divisor_low</code>を読み込み</td></tr>
<tr><td>DLAB_HIGH_OFFSET</td><td>1</td><td>is_dlab_set = true</td><td><code>self.baud_divisor_high</code>を読み込み</td></tr>
<tr><td>DATA_OFFSET</td><td>0</td><td>- (is_dlab_set = false)</td><td>(1)</td></tr>
<tr><td>IER_OFFSET</td><td>1</td><td>- (is_dlab_set = false)</td><td><code>self.inerrupt_enable</code>を読み込み</td></tr>
<tr><td>IIR_OFFSET</td><td>2</td><td>-</td><td>(2)</td></tr>
<tr><td>LCR_OFFSET</td><td>3</td><td>-</td><td><code>self.line_control</code>を読み込み</td></tr>
<tr><td>MCR_OFFSET</td><td>4</td><td>-</td><td><code>self.modem_control</code>を読み込み</td></tr>
<tr><td>LSR_OFFSET</td><td>5</td><td>-</td><td><code>self.line_status</code>を読み込み</td></tr>
<tr><td>MSR_OFFSET</td><td>6</td><td>-</td><td>(3)</td></tr>
<tr><td>SCR_OFFSET</td><td>7</td><td>-</td><td><code>self.scratch</code>を読み込み</td></tr>
</tbody></table>
<ol>
<li>Serial構造体が持つbufferのデータを読み出したりするが、現実装ではこのbufferはloopback modeでのwriteでしかデータが積まれない実装になっているため、今回の内容では省略。OSの起動シーケンスでもこの領域に関する<code>read</code>は発行されていなかった。</li>
<li><code>self.interrupt_identification</code> | <code>0b1100_0000(FIFO enabled)</code>の結果を返却しデフォルト値に戻す</li>
<li>現在の状態がloopback modeかどうかで場合分けを行う
<ul>
<li>loopbackの場合は適切に調整する（今回は重要ではないので省略）</li>
<li>loopbackで無い場合は素直に<code>self.modem_status</code>の値を返却する</li>
</ul>
</li>
</ol>
<h2 id="toyvmmでのrust-vmmvm-superioの利用"><a class="header" href="#toyvmmでのrust-vmmvm-superioの利用">ToyVMMでのrust-vmm/vm-superioの利用</a></h2>
<p>ToyVMMでは上記の<a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>を利用し<code>KVM_EXIT_IO</code>の内容をハンドルする。
加えて考えなければならないのは以下の2点である。</p>
<ul>
<li>Guestからシリアルポートに当てたコンソール出力を標準出力に書き出すことで、起動シーケンスやGuest内部の状態を確認できるようにする。</li>
<li>標準入力の内容をGuest VMに引き渡す</li>
</ul>
<p>以降、それぞれ順番に確認していく。　</p>
<h3 id="シリアルポート当てのコンソール出力を標準出力に書き出す"><a class="header" href="#シリアルポート当てのコンソール出力を標準出力に書き出す">シリアルポート当てのコンソール出力を標準出力に書き出す</a></h3>
<p>起動シーケンスやGuest VMの内部状態を確認するために、シリアルポート宛のコンソール出力を標準出力に書き出すようにしてみよう。<br />
「シリアルポート当てのコンソール出力」はまさに、<code>KVM_EXIT_IO</code>で<code>Serial向けのIO Portアドレス</code>に対しての<code>KVM_EXIT_IO_OUT</code>のケースに該当する。
以下のコード部が該当処理になる。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>...
loop {
  match vcpu.run() {
      Ok(run) =&gt; match run {
          ...
          VcpuExit::IoOut(addr, data) =&gt; {
              io_bus.write(addr as u64, data);
          }
      ...  
      }
    }
}
...
<span class="boring">}
</span></code></pre></pre>
<p>さて、ここでは<code>KVM_EXIT_IO_OUT</code>で受け取ったアドレスと書き込むべきデータを伴って、<code>io_bus.write</code>を呼び出すのみになっている。
この<code>io_bus</code>は以下のような形で設定を行ったものである。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let mut io_bus = IoBus::new();
let com_evt_1_3 = EventFdTrigger::new(EventFd::new(libc::EFD_NONBLOCK).unwrap());
let stdio_serial = Arc::new(Mutex::new(SerialDevice {
    serial: serial::Serial::with_events(
        com_evt_1_3.try_clone().unwrap(),
        SerialEventsWrapper {
            buffer_read_event_fd: None,
        },
        Box::new(std::io::stdout()),
    ),
}));
io_bus.insert(stdio_serial.clone(), 0x3f8, 0x8).unwrap();
vm.fd().register_irqfd(&amp;com_evt_1_3, 4).unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>上記のセットアップについては少し説明が必要なため以降に順を追って話していくが、大まかに押さえておくと以下のようなことをおこなっている。</p>
<ul>
<li>I/O Busを表現している<code>IoBus</code>構造体、割り込みを表現する<code>eventfd</code>を初期化する。</li>
<li>Serial Deviceの初期化をおこなう。その際にGuestへ割り込みを発生させるための<code>eventfd</code>と標準出力のためのFD（<code>std::io::stdout</code>）を渡している。</li>
<li>初期化した<code>IoBus</code>に上記のSerial Deviceを登録している。この時、<code>0x3f8</code>をベースアドレス、<code>0x8</code>をレンジとして登録している。
<ul>
<li>これにより、<code>0x3f8</code>を基底として<code>0x8</code>のレンジはこのSerial Deviceが利用するアドレス領域ということを表現している。</li>
</ul>
</li>
</ul>
<h4 id="io-busの取り回し"><a class="header" href="#io-busの取り回し">I/O Busの取り回し</a></h4>
<p><code>KVM_EXIT_IO</code>で渡されるアドレス値は、アドレス空間全体におけるアドレスの値になる。<br />
一方で、<a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>の<code>read/write</code>の実装は、Serial Deviceのベースアドレスからのオフセット値をとって処理を実施するような実装になっているため、このギャップを埋めるための処理が必要になる。<br />
単純にオフセットを計算するだけでも良いが、Firecrackerではその後の拡張性（Serialデバイス以外のIO Portを利用するデバイス）も考慮してか、I/O Busを表現する構造体である<code>Bus</code>という構造体が存在する。
これは<code>BusRange</code>（バスにおけるデバイスのベースアドレスと利用するアドレスレンジを表現している構造体）とともにデバイスを登録できるようなものになっている。<br />
さらに、あるアドレスへのIOが発生しときに、そのアドレスを確認し、対応するアドレスレンジに登録されているデバイスを取り出して、そのデバイスに対して、ベースアドレスからのオフセット値でIOを実行するような仕組みが提供されている。
例えば<code>write</code>関数は以下のような実装になっており、<code>get_device</code>でアドレス情報から対応する登録済みデバイスと、そのデバイスのベースアドレスからのオフセットを取得し、それを利用してデバイスに実装されている<code>write</code>関数を呼び出している。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn write(&amp;self, addr: u64, data: &amp;[u8]) -&gt; bool {
    if let Some((offset, dev)) = self.get_device(addr) { : u64 : &amp;Mutex&lt;dyn BusDevice&gt;
        // OK to unwrap as lock() failing is a serious error condition and should panic.
        dev.lock() Result&lt;MutexGuard&lt;dyn BusDevice&gt;, …&gt;
            .expect(&quot;Failed to acquire device lock&quot;) MutexGuard&lt;dyn BusDevice&gt; msg:
            .write(offset, data);
        true
    } else {
        false
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>具体的にSerialデバイスの話を例に挙げて考えてみる。
前述した通りGuest VMからのserialに対する<code>KVM_EXIT_IO_OUT</code>は<code>0x3f8</code>をベースとして8 byteのアドレスレンジで発生する。
ToyVMMのIoBusでも同様のアドレスベース、レンジ情報でSerial Deviceの登録をしているため、例えば<code>KVM_EXIT_IO_OUT</code>として<code>0x3fb</code>へ<code>0b1001_0011</code>を書き込むという命令をトラップした場合、登録したSerial Deviceに対して、ベースアドレス(<code>0x3f8</code>)からのオフセット(<code>0x3</code>)の位置、つまり<code>LCR</code>に<code>0b1001_0011</code>を書く、という命令に解釈される。</p>
<h4 id="eventfdirqfdによるguest-vmへの割り込み通知"><a class="header" href="#eventfdirqfdによるguest-vmへの割り込み通知">eventfd/irqfdによるGuest VMへの割り込み通知</a></h4>
<p>ここからは少しKVMと割り込みに関する話をしたい。
いくつかLinuxのソースコードを引用することになるが以降では<code>v4.18</code>のコードから引用する。</p>
<p>:warning: 以降の話は基本的にソースコードを元に記載しているが、詳細な状態遷移を逐一確認して記載したものではないため多少間違っている可能性があります。もし間違いを発見した場合はコメントをいただけると幸いです。</p>
<p><a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>では、Serialの初期化時に第一引数に<a href="https://docs.rs/vmm-sys-util/0.6.1/vmm_sys_util/eventfd/struct.EventFd.html"><code>EventFd</code></a>を要求する。<br />
これはLinuxにおける<a href="https://man7.org/linux/man-pages/man2/eventfd.2.html">eventfd</a>のWrapperになっているものである。
eventfdの詳細は<a href="https://man7.org/linux/man-pages/man2/eventfd.2.html">man</a>を確認してほしいが、簡単にいうとプロセス間やプロセスとカーネルの間などでのイベントの通知を実現することができる仕組みである。</p>
<p>次にirqfdである。irqfdはeventfdをベースとしたVMに対して割り込みを入れることのできる仕組みである。
イメージとしてはeventfdの一端をKVMが保持し、もう片方からの通知をGuest VMへの割り込みとして解釈するというものである。
このirqfdによる割り込みは、Guest VMの外の世界からGuest VMへの割り込み、つまり通常のシステムで言うところの周辺デバイスからの割り込みをエミュレートするものである。逆方向の割り込みは<code>ioeventfd</code>の仕組みを利用するがここでは一旦省略する。</p>
<p>実際にソースコードを見ながら、このirqfdがどのようにGuestへの割り込みにつながっていくかを確認してみよう。<br />
KVMに対して<code>KVM_IRQFD</code>を伴ってioctlを実施すると、渡されたデータを元に<code>kvm_irqfd</code>、<code>kvm_irqfd_assign</code>の流れでKVMの処理が実行され、この<code>kvm_irqfd_assign</code>関数で<code>kvm_kernel_irqfd</code>構造体のインスタンスが作成される。<br />
この時、ioctl時に渡した追加情報を元に設定を行うが、特に<code>kvm_kernel_irqfd</code>構造体は<code>gsi</code>というフィールドを持っており、これがioctlで渡した引数の値によって設定される。<br />
この<code>gsi</code>は、<code>irqfd</code>に対応するGuestの割り込みテーブルのインデックスに該当するものになるため、ioctlを呼ぶ際にはeventfdに加えて、Guestのどの割り込みテーブルのエントリに対して割り込みを入れるかということも指定する。
ToyVMMではこの設定を行なっているのが以下の一行である。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vm.fd().register_irqfd(&amp;com_evt_1_3, 4).unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>これは<code>kvm_ioctl::VmFd</code>構造体のメソッドとして定義されている</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn register_irqfd(&amp;self, fd: &amp;EventFd, gsi: u32) -&gt; Result&lt;()&gt; {
    let irqfd = kvm_irqfd {
        fd: fd.as_raw_fd() as u32,
        gsi,
        ..Default::default()
    };
    // Safe because we know that our file is a VM fd, we know the kernel will only read      the
    // correct amount of memory from our pointer, and we verify the return result.
    let ret = unsafe { ioctl_with_ref(self, KVM_IRQFD(), &amp;irqfd) }; : i32 fd: req: arg:
    if ret == 0 {
        Ok(())
    } else {
        Err(errno::Error::last())
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>つまり上記では、これまで話してきたSerial deviceに利用しているeventfd（<code>com_evt_1_3</code>）をGSI=4（Guest VM上のCOM1ポートへの割り込みテーブルインデックス）を伴って設定しているため、この<code>com_evt_1_3</code>に対して実行した<code>write</code>は、COM1からの割り込みとしてGuest VMに渡り（つまり、Guestから見るとCOM1の先のserial deviceから割り込みが発生したことになり）、Guest VMのCOM1の割り込みハンドラを起動することになる。</p>
<p>さて、今Guest側の割り込みテーブル（GSI: Global System Interrupt）の話が出たが、これらはいつ、どのようにセットアップされるかいついて以降で説明していくこととする。<br />
端的に言えばこれらは<code>KVM_CREATE_IRQCHIP</code>を伴ってKVMにioctlを発行することで設定される。これを実施すると割り込みコントローラである<code>PIC</code>と<code>IOAPIC</code>の２種類が作成される（内部的には<code>kvm_pic_init</code>でPICの初期化とread/write opsの登録などを行い、<code>kvm-&gt;arch.vpic</code>に設定。<code>kvm_ioapic_init</code>でIOAPICの初期化とread/write opsの登録などを行い、<code>kvm-&gt;arch.vioapic</code>に設定している）<br />
この<code>PIC</code>や<code>IOAPIC</code>などのハードウェアは高速化の目的でKVMに実装があるため、独自にエミュレートする必要がない。もちろんqemuなどに任せることができるが、ここでは利用しないため省略する。
さらにその後、<code>kvm_setup_default_irq_routing</code>関数の中でデフォルトのIRQ Routingの設定がなされている。
この処理によって、どのGSIに対する割り込みによってどのハンドラが起動するか、という部分がセットアップされる。</p>
<p>さて、もう少し<code>kvm_setup_default_irq_routing</code>の中身を見てみよう。この関数の中ではさらに<code>kvm_set_irq_routing</code>関数を呼びだしており、本質的な処理はそこに記載がある。
ここでは<code>kvm_irq_routing_table</code>を作成し、これに対してGSIからIRQへの対応を表現している<code>kvm_kernel_irq_routing_entry</code>を設定していく形になる。
この<code>kvm_kernel_irq_routing_entry</code>はデフォルトのエントリ（<code>default_routing</code>）が存在しており、これをループしながら登録していくような形の実装が存在する。
この<code>default_routing</code>は以下のような定義になっている。関係するマクロの実装も記しておく。</p>
<pre><code class="language-C">#define SELECT_PIC(irq) \
	((irq) &lt; 8 ? KVM_IRQCHIP_PIC_MASTER : KVM_IRQCHIP_PIC_SLAVE)

#define IOAPIC_ROUTING_ENTRY(irq) \
	{ .gsi = irq, .type = KVM_IRQ_ROUTING_IRQCHIP,	\
	  .u.irqchip = { .irqchip = KVM_IRQCHIP_IOAPIC, .pin = (irq) } }

#define ROUTING_ENTRY1(irq) IOAPIC_ROUTING_ENTRY(irq)

#define PIC_ROUTING_ENTRY(irq) \
	{ .gsi = irq, .type = KVM_IRQ_ROUTING_IRQCHIP,	\
	  .u.irqchip = { .irqchip = SELECT_PIC(irq), .pin = (irq) % 8 } }

#define ROUTING_ENTRY2(irq) \
	IOAPIC_ROUTING_ENTRY(irq), PIC_ROUTING_ENTRY(irq)

static const struct kvm_irq_routing_entry default_routing[] = {
	ROUTING_ENTRY2(0), ROUTING_ENTRY2(1),
	ROUTING_ENTRY2(2), ROUTING_ENTRY2(3),
	ROUTING_ENTRY2(4), ROUTING_ENTRY2(5),
	ROUTING_ENTRY2(6), ROUTING_ENTRY2(7),
	ROUTING_ENTRY2(8), ROUTING_ENTRY2(9),
	ROUTING_ENTRY2(10), ROUTING_ENTRY2(11),
	ROUTING_ENTRY2(12), ROUTING_ENTRY2(13),
	ROUTING_ENTRY2(14), ROUTING_ENTRY2(15),
	ROUTING_ENTRY1(16), ROUTING_ENTRY1(17),
	ROUTING_ENTRY1(18), ROUTING_ENTRY1(19),
	ROUTING_ENTRY1(20), ROUTING_ENTRY1(21),
	ROUTING_ENTRY1(22), ROUTING_ENTRY1(23),
};
</code></pre>
<p>見ての通り、0-15までのIRQ番号は<code>ROUTING_ENTRY2</code>に、16-23までのをIRQ番号は<code>ROUTING_ENTRY1</code>に引き渡しており、<code>ROUTING_ENTRY2</code>は<code>IOAPIC_ROUTING_ENTRY</code>と<code>PIC_ROUTING_ENTRY</code>を、<code>ROUTING_ENTRY1</code>は<code>IOAPIC_ROUTING_ENTRY</code>のみを呼び出して、必要な情報を埋めた構造体を作っている。</p>
<p>この構造体の情報を使いながら後続する<code>kvm_set_routing_entry</code>関数で以下の通りそれぞれの<code>.u.irqchip.irqchip</code>の値(<code>KVM_IRQCHIP_PIC_SLAVE</code>、<code>KVM_IRQCHIP_PIC_MASTER</code>、<code>KVM_IRQCHIP_IOAPIC</code>)ごとにコールバック（<code>kvm_set_pic_irq</code>、<code>kvm_set_ioapic_irq</code>）や必要な設定をおこなっている。このコールバックは割り込み発生時に呼ばれる関数に該当し、後ほど触れるため少し覚えておいてほしい。</p>
<pre><code class="language-C">int kvm_set_routing_entry(struct kvm *kvm,
			  struct kvm_kernel_irq_routing_entry *e,
			  const struct kvm_irq_routing_entry *ue)
{
	/* We can't check irqchip_in_kernel() here as some callers are
	 * currently inititalizing the irqchip. Other callers should therefore
	 * check kvm_arch_can_set_irq_routing() before calling this function.
	 */
	switch (ue-&gt;type) {
	case KVM_IRQ_ROUTING_IRQCHIP:
		if (irqchip_split(kvm))
			return -EINVAL;
		e-&gt;irqchip.pin = ue-&gt;u.irqchip.pin;
		switch (ue-&gt;u.irqchip.irqchip) {
		case KVM_IRQCHIP_PIC_SLAVE:
			e-&gt;irqchip.pin += PIC_NUM_PINS / 2;
			/* fall through */
		case KVM_IRQCHIP_PIC_MASTER:
			if (ue-&gt;u.irqchip.pin &gt;= PIC_NUM_PINS / 2)
				return -EINVAL;
			e-&gt;set = kvm_set_pic_irq;
			break;
		case KVM_IRQCHIP_IOAPIC:
			if (ue-&gt;u.irqchip.pin &gt;= KVM_IOAPIC_NUM_PINS)
				return -EINVAL;
			e-&gt;set = kvm_set_ioapic_irq;
			break;
		default:
			return -EINVAL;
		}
		e-&gt;irqchip.irqchip = ue-&gt;u.irqchip.irqchip;
		break;
...
</code></pre>
<p>さて、ここまで見てきたところで再度<code>irqfd</code>の話に立ち戻ろう。
先ほどは説明していなかったが、実は<code>kvm_irqfd_assign</code>関数の中では、<code>init_waitqueue_func_entry(&amp;irqfd-&gt;wait, irqfd_wakeup)</code>という処理が呼び出され、<code>&amp;irqfd-&gt;wait-&gt;func</code>に<code>irqfd_wakeup</code>を登録している。
割り込みが発生した際にこの関数が呼び出され、この中で<code>schedule_work(&amp;irqfd-&gt;inject)</code>が呼ばれる。
この<code>inject</code>フィールドも<code>kvm_irqfd_assign</code>関数の中で初期化されており、結果として<code>irqfd_inject</code>関数が呼び出されることになり、さらにこの関数の中で<code>kvm_set_irq</code>関数が呼び出される。<br />
<code>kvm_set_irq</code>の処理の中では、割り込みがきたIRQ番号を持つエントリをリストアップし、その<code>set</code>コールバックを呼び出していく。これはつまり上記で説明した<code>kvm_set_pic_irq</code>や<code>kvm_set_ioapic_irq</code>などの関数が呼ばれることを意味する。
以上の流れが割り込みとGSI、IRQ間Routingの実際の処理の部分である。</p>
<img src="./02_figs/kvm_irq_gsi_routing.svg" width="100%">
<p>ここからの話は割り込み処理に対してもう少し深掘りした内容になるが、ToyVMMを理解する上で必ずしも必要ではない内容なので、<a href="02-5_serial_console_implementation.html#toyvmm-serial-console">ToyVMM serial console</a>まで読み飛ばしてもらっても構わない。</p>
<p>折角なので、割り込みハンドラである<code>kvm_set_pic_irq</code>の処理についてもう少し見てみることにしよう。やや話が脱線してきたが、せっかくなのでキリのいいところまで深掘りして確認してみる。<br />
<code>kvm_set_pic_irq</code>は、<code>KVM_CREATE_IRQCHIP</code>で初期化した<code>kvm-&gt;arch.vpic</code>を利用して、<code>kvm_pic_set_irq</code>を呼び出しているのみである。</p>
<pre><code class="language-C">static int kvm_set_pic_irq(struct kvm_kernel_irq_routing_entry *e,
			   struct kvm *kvm, int irq_source_id, int level,
			   bool line_status)
{
	struct kvm_pic *pic = kvm-&gt;arch.vpic;
	return kvm_pic_set_irq(pic, e-&gt;irqchip.pin, irq_source_id, level);
}
</code></pre>
<p>さて、<code>kvm_pic_set_irq</code>の実装を見にいくと以下のようになっている。</p>
<pre><code class="language-C">int kvm_pic_set_irq(struct kvm_pic *s, int irq, int irq_source_id, int level)
{
	int ret, irq_level;

	BUG_ON(irq &lt; 0 || irq &gt;= PIC_NUM_PINS);

	pic_lock(s);
	irq_level = __kvm_irq_line_state(&amp;s-&gt;irq_states[irq],
					 irq_source_id, level);
	ret = pic_set_irq1(&amp;s-&gt;pics[irq &gt;&gt; 3], irq &amp; 7, irq_level);
	pic_update_irq(s);
	trace_kvm_pic_set_irq(irq &gt;&gt; 3, irq &amp; 7, s-&gt;pics[irq &gt;&gt; 3].elcr,
			      s-&gt;pics[irq &gt;&gt; 3].imr, ret == 0);
	pic_unlock(s);

	return ret;
}
</code></pre>
<p><code>pic_set_irq1</code>関数でIRQ Levelの設定を行い、<code>pic_update_irq</code>関数で<code>pic_irq_request</code>関数を呼び出し、<code>kvm-&gt;arch.vpic</code>に格納されている<code>kvm_pic</code>構造体の中を以下のように書き換えている。</p>
<pre><code class="language-C">/*
 * raise irq to CPU if necessary. must be called every time the active
 * irq may change
 */
static void pic_update_irq(struct kvm_pic *s)
{
	int irq2, irq;

	irq2 = pic_get_irq(&amp;s-&gt;pics[1]);
	if (irq2 &gt;= 0) {
		/*
		 * if irq request by slave pic, signal master PIC
		 */
		pic_set_irq1(&amp;s-&gt;pics[0], 2, 1);
		pic_set_irq1(&amp;s-&gt;pics[0], 2, 0);
	}
	irq = pic_get_irq(&amp;s-&gt;pics[0]);
	pic_irq_request(s-&gt;kvm, irq &gt;= 0);
}


/*
 * callback when PIC0 irq status changed
 */
static void pic_irq_request(struct kvm *kvm, int level)
{
	struct kvm_pic *s = kvm-&gt;arch.vpic;

	if (!s-&gt;output)
		s-&gt;wakeup_needed = true;
	s-&gt;output = level;
}
</code></pre>
<p>さらにこの上で<code>pic_unlock</code>関数を呼び出しているが、この関数の中身が地味に重要である。
<code>wakeup_needed</code>がtrueの場合、vCPUに対して<code>kvm_vcpu_kick</code>を実行している。</p>
<pre><code class="language-C">static void pic_unlock(struct kvm_pic *s)
	__releases(&amp;s-&gt;lock)
{
	bool wakeup = s-&gt;wakeup_needed;
	struct kvm_vcpu *vcpu;
	int i;

	s-&gt;wakeup_needed = false;

	spin_unlock(&amp;s-&gt;lock);

	if (wakeup) {
		kvm_for_each_vcpu(i, vcpu, s-&gt;kvm) {
			if (kvm_apic_accept_pic_intr(vcpu)) {
				kvm_make_request(KVM_REQ_EVENT, vcpu);
				kvm_vcpu_kick(vcpu);
				return;
			}
		}
	}
}

/*
 * Kick a sleeping VCPU, or a guest VCPU in guest mode, into host kernel mode.
 */
void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
{
	int me;
	int cpu = vcpu-&gt;cpu;

	if (kvm_vcpu_wake_up(vcpu))
		return;

	me = get_cpu();
	if (cpu != me &amp;&amp; (unsigned)cpu &lt; nr_cpu_ids &amp;&amp; cpu_online(cpu))
		if (kvm_arch_vcpu_should_kick(vcpu))
			smp_send_reschedule(cpu);
	put_cpu();
}
</code></pre>
<p><code>smp_send_reschedule</code>を呼び出した結果として、<code>native_smp_send_reschedule</code>関数が呼ばれる。</p>
<pre><code class="language-C">/*
 * this function sends a 'reschedule' IPI to another CPU.
 * it goes straight through and wastes no time serializing
 * anything. Worst case is that we lose a reschedule ...
 */
static void native_smp_send_reschedule(int cpu)
{
	if (unlikely(cpu_is_offline(cpu))) {
		WARN_ON(1);
		return;
	}
	apic-&gt;send_IPI(cpu, RESCHEDULE_VECTOR);
}
</code></pre>
<p>コメントにもある通り、この関数を呼び出すと別のCPUに対してIPI（Inter Process Interrupt）を発行し再スケジュールを促す。
vCPUに対して割り込みを送り、仮想マシンから強制的に<code>VMExit</code>する。そしてvCPUにスケジュールする際に割り込みが挿入されることになる。</p>
<p>さて、では実際に割り込みが挿入される処理も簡単に確認して割り込みの話を一旦終えることにしよう。
<code>KVM_RUN</code>が実行されると、以下のような処理が実行されていくようである（あくまで割り込みの挿入のみに着目しているため、そのほか膨大な処理は省略している）</p>
<pre><code>kvm_arch_vcpu_ioctl_run
 -&gt; vcpu_run
 -&gt; vcpu_enter_guest
 -&gt; inject_pending_event
 -&gt; kvm_cpu_has_injectable_intr
</code></pre>
<p><code>kvm_cpu_has_injectable_intr</code>の中で<code>kvm_cpu_has_extint</code>関数が呼ばれる。この処理は今回の場合はおそらく<code>pic_irq_request</code>で設定した<code>s-&gt;output</code>の値でreturnするため、この関数は全体として<code>return 1</code>を返す。<br />
そのため、<code>inject_pending_event</code>関数の以下の処理に差し掛かる。</p>
<pre><code class="language-C">	} else if (kvm_cpu_has_injectable_intr(vcpu)) {
		/*
		 * Because interrupts can be injected asynchronously, we are
		 * calling check_nested_events again here to avoid a race condition.
		 * See https://lkml.org/lkml/2014/7/2/60 for discussion about this
		 * proposal and current concerns.  Perhaps we should be setting
		 * KVM_REQ_EVENT only on certain events and not unconditionally?
		 */
		if (is_guest_mode(vcpu) &amp;&amp; kvm_x86_ops-&gt;check_nested_events) {
			r = kvm_x86_ops-&gt;check_nested_events(vcpu, req_int_win);
			if (r != 0)
				return r;
		}
		if (kvm_x86_ops-&gt;interrupt_allowed(vcpu)) {
			kvm_queue_interrupt(vcpu, kvm_cpu_get_interrupt(vcpu),
					    false);
			kvm_x86_ops-&gt;set_irq(vcpu);
		}
	}
</code></pre>
<p>結果的に<code>kvm_x86_ops-&gt;set_irq(vcpu)</code>が呼ばれる。これはコールバック関数として<code>vmx_inject_irq</code>が実行されることになる。
この処理で、<code>VMCS</code>（<code>Virtual Machine Control Structure</code>）に<code>VMX_ENTRY_INTR_INFO_FIELD</code>を設定することで割り込みを挿入している。
<code>VMCS</code>についての説明を行なっていないが、この話を始めるとハイパーバイザーの実装についての話が必要になってくるのでここでは省略する。
将来的に、補足情報としてドキュメントに追記するかもしれない。</p>
<p>長くなったが以上がPICを例にとった割り込み処理の流れである。</p>
<h4 id="toyvmm-serial-console"><a class="header" href="#toyvmm-serial-console">ToyVMM serial console</a></h4>
<p>さて、この辺りで割り込みに関する探索を一旦切り上げて、ToyVMMの実装の話に戻ろう。
これまでの話を踏まえながら、ToyVMM側ではどのような処理を実行しており、それが裏ではどのような処理として実行されているかを整理する。
ToyVMMの中で、先に紹介した<code>register_irqfd</code>を実施する前に実は以下のような処理を行なっていた。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>vm.setup_irqchip().unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>この関数は薄いwrapperになっており、内部的には<code>create_irq_chip</code>の呼び出しと<code>create_pit2</code>の呼び出しを行う処理になっている。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[cfg(target_arch = &quot;x86_64&quot;)]
pub fn setup_irqchip(&amp;self) -&gt; Result&lt;()&gt; {
    self.fd.create_irq_chip().map_err(Error::VmSetup)?;
    let pit_config = kvm_pit_config {
        flags: KVM_PIT_SPEAKER_DUMMY,
        ..Default::default()
    };
    self.fd.create_pit2(pit_config).map_err(Error::VmSetup)
}
<span class="boring">}
</span></code></pre></pre>
<p>ここで重要なのは、<code>create_irq_chip</code>関数である。これは内部的には<code>KVM_CREATE_IRQCHIP</code>のAPIを叩いており、これによって先に話した通り、割り込みコントローラの初期化、IRQ Routingの初期化などを実施する。
上記セットアップ済みのGuest VMに対して、先に説明した<code>register_irqfd(&amp;com_evt_1_3, 4)</code>を実行することで<code>KVM_IRQFD</code> APIを叩き、先に説明した<code>kvm_irqfd_assign</code>などの処理が実施されるため、割り込みハンドラの設定などが行われる。
これで、ToyVMMからKVM APIを利用した割り込み関係のセットアップは完了である。</p>
<p>さて、改めて<code>com_evt_1_3</code>のからの割り込みについて確認してみよう。割り込みの一端は上記で見た通り<code>register_irqfd</code>によって<code>GSI=4</code>とともにKVM側に受け渡しているため、もう一端から発行されたwriteをGuest VMへのCOM1ポートへのinterruptとして取り扱うことになる。一方、問題の<code>com_evt_1_3</code>のもう一端はSerial Deviceに渡してあるためSerial Device側で実行されるeventfdへのwrite（<code>Serial::write</code>による書き込み処理後や、後述する<code>Serial::enqueue_raw_byte</code>による呼び出しで発生する）が具体的な割り込みのトリガである。
つまり、通常のサーバとSerial Deviceが行うやりとりと同じような形で、Guest VMとSoftware実装のSerial Deviceがやりとりをするような構成を構築できる。</p>
<p>また、Serial Consoleを表現するために、今回はSerial Deviceのoutputに該当する書き込み先はstdoutに設定している。そのため、<code>KVM_EXIT_IO_OUT</code>をハンドルした際の<code>THR</code>への書き込みがstdoutへの書き込みへと渡っていき、結果として標準出力にコンソールメッセージが出力され、目的のSerial Consoleとしての機能が実現される。</p>
<h4 id="標準入力でguest-vmを操作する"><a class="header" href="#標準入力でguest-vmを操作する">標準入力でGuest VMを操作する。</a></h4>
<p>最後に標準入力の内容をGuest VMに反映させてGuest VMを操作できるようにしたい。
<a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a>が提供する<code>Serial</code>構造体には、<a href="https://docs.rs/vm-superio/0.1.1/vm_superio/serial/struct.Serial.html#method.enqueue_raw_bytes"><code>enqueue_raw_bytes</code></a>というヘルパー関数が存在し、これを利用するとあまり細かいレジスタ操作や割り込みを考えずにGuest VMに対してデータを送信することができる（関数の実装がこれらの処理をうまく実施してくれる）
あとはこれを標準入力から受け取った入力をプログラムで読み取って、このメソッドにそのまま引き渡してあげるようにすると目的の操作が達成できる。</p>
<p>標準入力をraw modeへと切り替えて、メインスレッドでpollingしながら標準入力を受け取ったら<code>enqueue_raw_bytes</code>でGuest VMに対してその入力内容を送信する。
Guest VMはvCPUごとに別スレッドを起動して処理させているため、メインスレッドで標準入力をpollingすることによるGuest VMの処理への影響はない。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>let stdin_handle = io::stdin();
let stdin_lock = stdin_handle.lock();
stdin_lock
    .set_raw_mode()
    .expect(&quot;failed to set terminal raw mode&quot;);
let ctx: PollContext&lt;Token&gt; = PollContext::new().unwrap();
ctx.add(&amp;exit_evt, Token::Exit).unwrap();
ctx.add(&amp;stdin_lock, Token::Stdin).unwrap();
'poll: loop {
    let pollevents: PollEvents&lt;Token&gt; = ctx.wait().unwrap();
    let tokens: Vec&lt;Token&gt; = pollevents.iter_readable().map(|e| e.token()).collect();
    for &amp;token in tokens.iter() {
        match token {
            Token::Exit =&gt; {
                println!(&quot;vcpu requested shutdown&quot;);
                break 'poll;
            }
            Token::Stdin =&gt; {
                let mut out = [0u8; 64];
                tx.send(true).unwrap();
                match stdin_lock.read_raw(&amp;mut out[..]) {
                    Ok(0) =&gt; {
                        println!(&quot;eof!&quot;);
                    }
                    Ok(count) =&gt; {
                        stdio_serial
                            .lock()
                            .unwrap()
                            .serial
                            .enqueue_raw_bytes(&amp;out[..count])
                            .expect(&quot;failed to enqueue bytes&quot;);
                    }
                    Err(e) =&gt; {
                        println!(&quot;error while reading stdin: {:?}&quot;, e);
                    }
                }
            }
            _ =&gt; {}
        }
    }
}
<span class="boring">}
</span></code></pre></pre>
<p>ナイーブな実装でありあまり特別なことは行っていないので特に説明すべきことはない。<br />
単純だが以上で目的の動作が達成できる。</p>
<h2 id="check-uart-request-in-booting-linux-kernel"><a class="header" href="#check-uart-request-in-booting-linux-kernel">Check UART request in booting linux kernel.</a></h2>
<p>これまでSerial UARTのソフトウェア実装の内容とToyVMM内部での利用方法について記載した。<br />
これは実際にうまく動作するが、やはりLinux Kernel起動時のUARTのやりとりを確認してみたいところである。
幸い、VMMの仕組み上、<code>KVM_EXIT_IO</code>をハンドルする必要があるため、このハンドリング処理にデバッグコードを仕込むことでシリアルポートに送付された全てのリクエストを盗み見ることができる。<br />
ただし全ての内容を確認するのは現実的ではないので、一部だけ簡単に確認してみようと思う。</p>
<p>デバッグするために差し込んだコードの説明はここでは行わない。適切な処理箇所にデバッグコードを入れるだけのため非常に単純である。<br />
以降では、シリアルポートへのリクエストに対して、分かりやすいように以下のような3種類の書式に従って注釈をつけている。</p>
<pre><code class="language-bash">[書式1 - Read]
r($register) = $data
  - Description

・ r           = Read operation
・ $register   = デバイスのアドレス(0x3f8)を利用して計算したoffsetに対応するレジスタ
・ $data       = $registerから読み出したデータ
・ Description = 説明文

[書式2 - Write]
w($register = $data)
  - Description

・ w           = Write operation
・ $register   = デバイスのアドレス(0x3f8)を利用して計算したoffsetに対応するレジスタ
・ $data       = $registerの値に書き込むデータ
・ Description = 説明文


[書式3 - Write (character)]
w(THR = $data = 0xYY) -&gt; 'CHAR'

・ w(THR ...)  = Write operation to THR
・ $data       = $registerの値に書き込むbinaryデータ
・ 0xYY        = $dataをhexに変換したもの
・ 'CHAR'      = 0xYYをASCIIコード表に基づきcharacterに変換したもの
</code></pre>
<p>さて、以下は少し長いがOS起動時の<code>0x3f8 (COM1)</code>のレジスタに対してのリクエストを上記の書式に従ってわかりやすく書き表したものである。</p>
<pre><code class="language-bash"># 最初はbaud lateなどの初期設定を行なっている
w(IER = 0)
w(LCR = 10010011)
  - DLAB         = 1   (DLAB: DLL and DLM accessible)
  - Break signal = 0   (Break signal disabled)
  - Parity       = 010 (No parity)
  - Stop bits    = 0   (1 stop bit)
  - Data bits    = 11  (8 data bit)
w(DLL = 00001100)
w(DLM = 0)
  - DLL = 0x0C, DLM = 0x00 (Speed = 9600 bps)
w(LCR = 00010011)
  - DLAB         = 0   (DLAB : RBR, THR, and IER accessible)
  - Break signal = 0   (Break signal disabled)
  - Parity       = 010 (No parity)
  - Stop bits    = 0   (1 stop bit)
  - Data bits    = 11  (8 data bit)
w(FCR = 0)
w(MCR = 00000001)
  - Reserved            = 00
  - Autoflow control    = 0
  - Loopback mode       = 0
  - Auxiliary output 2  = 0
  - Auxiliary output 1  = 0
  - Request to send     = 0
  - Data terminal ready = 1
r(IER) = 0
w(IER = 0)

# この辺りから実際にコンソール出力をシリアルで受け取り、write（今回の場合はstdoutへのwrite）をしている様子である

# r(LSR)の内容をみて、次の文字を書いていいか判別していると思われる
r(LSR) = 01100000
  - Errornous data in FIFO         = 0
  - THR is empty, and line is idle = 1
  - THR is empty                   = 1
  - Break signal received          = 0
  - Framing error                  = 0
  - Parity error                   = 0
  - Overrun error                  = 0
  - Data available                 = 0
    - 5, 6 bitはcharacter transmitterに関わるもので、UARTが次のcharacterを受付可能かの識別に利用する
    - 5, 6 bitが立っていれば、新たなcharacterを受け付けることができる
      - Bit 6 = '1' means that all characters have been transmitted
      - Bit 5 = '1' means that UARTs is capable of receiving more characters

# 上記で、次の文字のwriteを受け付けているので、outputしたい文字を書く。
w(THR = 01011011 = 0x5b) -&gt; '['

# 以降、これの繰り返しを行っている
r(LSR) = 01100000
w(THR = 00100000 = 0x20) -&gt; ' '
# 上記の処理があと3回続く
r(LSR) = 01100000
w(THR  = 00110000 = 0x30) -&gt; '0'
r(LSR) = 01100000
w(THR  = 00101110 = 0x2e) -&gt; '.'
r(LSR) = 01100000
w(THR  = 00110000 = 0x30) -&gt; '0'
# 上記の処理があと5回続く
r(LSR) = 01100000
w(THR  = 01011101 = 0x5d) -&gt; ']'
r(LSR) = 01100000
w(THR  = 00100000 = 0x20) -&gt; ' '
r(LSR) = 01100000
w(THR  = 01001100 = 0x4c) -&gt; 'L'
r(LSR) = 01100000
w(THR  = 01101001 = 0x69) -&gt; 'i'
r(LSR) = 01100000
w(THR  = 01101110 = 0x6e) -&gt; 'n'
r(LSR) = 01100000
w(THR  = 01110101 = 0x75) -&gt; 'u'
r(LSR) = 01100000
w(THR  = 01111000 = 0x78) -&gt; 'x'
r(LSR) = 01100000
w(THR  = 00100000 = 0x20) -&gt; ' '
r(LSR) = 01100000
w(THR  = 01110110 = 0x76) -&gt; 'v'
r(LSR) = 01100000
w(THR  = 01100101 = 0x65) -&gt; 'e'
r(LSR) = 01100000
w(THR  = 01110010 = 0x72) -&gt; 'r'
r(LSR) = 01100000
w(THR  = 01110011 = 0x73) -&gt; 's'
r(LSR) = 01100000
w(THR  = 01101001 = 0x69) -&gt; 'i'
r(LSR) = 01100000
w(THR  = 01101111 = 0x6f) -&gt; 'o'
r(LSR) = 01100000
w(THR  = 01101110 = 0x6e) -&gt; 'n'
r(LSR) = 01100000
w(THR  = 00100000 = 0x20) -&gt; ' '
r(LSR) = 01100000
w(THR  = 00110100 = 0x34) -&gt; '4'
r(LSR) = 01100000
w(THR  = 00101110 = 0x2e)-&gt; '.'
r(LSR) = 01100000
w(THR  = 00110001 = 0x31) -&gt; '1'
r(LSR) = 01100000
w(THR  = 00110100 = 0x34) -&gt; '4'
r(LSR) = 01100000
w(THR  = 00101110 = 0x2e) -&gt; '.'
r(LSR) = 01100000
w(THR  = 00110001 = 0x31) -&gt; '1'
r(LSR) = 01100000
w(THR  = 00110111 = 0x37) -&gt; '7'
r(LSR) = 01100000
w(THR  = 00110100 = 0x34) -&gt; '4'
r(LSR) = 01100000
w(THR  = 00100000 = 0x20) -&gt; ' '
r(LSR) = 01100000
w(THR  = 00101000 = 0x28) -&gt; '('
r(LSR) = 01100000
w(THR  = 01000000 = 0x40) -&gt; '@'
w(LSR) = 01100000
r(THR  = 00110101 = 0x35) -&gt; '5'
r(LSR) = 01100000
w(THR  = 00110111 = 0x37) -&gt; '7'
r(LSR) = 01100000
w(THR  = 01100101 = 0x65) -&gt; 'e'
r(LSR) = 01100000
w(THR  = 01100100 = 0x64) -&gt; 'd'
r(LSR) = 01100000
w(THR  = 01100101 = 0x65) -&gt; 'e'
r(LSR) = 01100000
w(THR  = 01100010 = 0x62) -&gt; 'b'
r(LSR) = 01100000
w(THR  = 01100010 = 0x62) -&gt; 'b'
r(LSR) = 01100000
w(THR  = 00111001 = 0x39) -&gt; '9'
r(LSR) = 01100000
w(THR  = 00111001 = 0x39) -&gt; '9'
r(LSR) = 01100000
w(THR  = 01100100 = 0x64) -&gt; 'd'
r(LSR) = 01100000
w(THR  = 01100010 = 0x62) -&gt; 'b'
r(LSR) = 01100000
w(THR  = 00110111 = 0x37) -&gt; '7'
r(LSR) = 01100000
w(THR  = 00101001 = 0x29) -&gt; ')'

# 上記の出力を並べると以下のようになる。  
[    0.000000] Linux version 4.14.174 (@57edebb99db7)

# これはOSブート時に出力される一行目の内容と一致していることがわかる
</code></pre>
<p>当然ながらLinux起動時のUART requestはまだまだ続き、かつ上記に示すような単純な出力以外の処理も行われるが、ここではこれ以上の確認は行わないので気になる方は各々確認してほしい。</p>
<h2 id="reference"><a class="header" href="#reference">Reference</a></h2>
<ul>
<li><a href="https://www.lammertbies.nl/comm/info/serial-uart">Serial UART information</a></li>
<li><a href="https://en.wikibooks.org/wiki/Serial_Programming/8250_UART_Programming">Wikibooks : Serial Programming / 8250 UART Programming</a></li>
<li><a href="https://github.com/rust-vmm/vm-superio">rust-vmm/vm-superio</a></li>
<li><a href="https://en.wikipedia.org/wiki/Interrupt_request_(PC_architecture)">Interrupt request(PC architecture)</a></li>
<li><a href="https://www.kernel.org/doc/html/v4.15/admin-guide/serial-console.html">Linux Serial Console</a></li>
<li><a href="https://xzpeter.org/htmls/2017_12_07_kvm_irqfd/kvm_irqfd_implementation.html">KVM IRQFD Implementation</a></li>
<li><a href="https://rkx1209.hatenablog.com/entry/2016/01/01/101456">KVMのなかみ（KVM internals）</a></li>
<li><a href="https://syuu1228.github.io/howto_implement_hypervisor/part2.html">ハイパーバイザーの作り方~ちゃんと理解する仮想化技術~ 第2回 intel VT-xの概要とメモリ仮想化</a></li>
<li><a href="https://habr.com/en/post/446312/">External Interrupts in the x86 system. Part1. Interrupt controller evolution</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="toyvmm-implememtation"><a class="header" href="#toyvmm-implememtation">ToyVMM implememtation</a></h1>
<p>これまでの話を合わせると、必要最低限の機能を保有したVMMが作成できる。<br />
このToyVMMは以下の機能を持っているシンプルなVMMである</p>
<ul>
<li>vmlinuz, initrdを利用して、Guest OSを起動できる</li>
<li>Guest OS起動後はSerial Terminalとして入出力を受け、Guestの状態確認や操作を実施することができる。</li>
</ul>
<h2 id="run-linux-kernel-"><a class="header" href="#run-linux-kernel-">Run linux kernel !</a></h2>
<p>実際にLinux Kernelを起動する</p>
<p>まずは、<code>vmlinux.bin</code>と<code>initrd.img</code>を用意します。いずれもtoyvmmのリポジトリルートに配置する。
<code>vmlinux.bin</code>は以下を参考にダウンロードする</p>
<pre><code class="language-bash"># Download vmlinux.bin
wget https://s3.amazonaws.com/spec.ccfc.min/img/quickstart_guide/x86_64/kernels/vmlinux.bin
cp vmlinux.bin &lt;TOYVMM WORKING DIRECTORY&gt;
</code></pre>
<p><code>initrd.img</code>は、<a href="https://github.com/marcov/firecracker-initrd.git">marcov/firecracker-initrd</a>を利用し、alpineのrootfsを含むinitrd.imgを作成する。</p>
<pre><code class="language-bash"># Create initrd.img
# Using marcov/firecracker-initrd (https://github.com/marcov/firecracker-initrd)
git clone https://github.com/marcov/firecracker-initrd.git
cd firecracker-initrd
bash ./build.sh
# After above commands, initrd.img file wil be located on build/initrd.img.
# So, please move it to the working directory of toyvmm.
cp build/initrd.img &lt;TOYVMM WORKING DIRECTORY&gt;
</code></pre>
<p>以上で準備完了ある。以下のコマンドを実行しGuest VMを起動してみよう！</p>
<pre><code class="language-bash">$ make run_linux
</code></pre>
<p>ここではブートシーケンスの出力については省略する。Guest VMの起動シーケンスが標準出力に表示されていくだろう。
起動が完了すると、以下のようにAlpine Linuxの画面とともにloginが要求されるため、root/rootでログインしよう.</p>
<pre><code class="language-bash">Welcome to Alpine Linux 3.15
Kernel 4.14.174 on an x86_64 (ttyS0)

(none) login: root
Password:
Welcome to Alpine!

The Alpine Wiki contains a large amount of how-to guides and general
information about administrating Alpine systems.
See &lt;http://wiki.alpinelinux.org/&gt;.

You can setup the system with the command: setup-alpine

You may change this message by editing /etc/motd.

login[1058]: root login on 'ttyS0'
(none):~#
</code></pre>
<p>素晴らしい！確かにGuest VMを起動し操作できています！
もちろん、Guest VM内部でコマンドを実行することもできます。<br />
例えば最も基本的なlsコマンドを実行した結果は以下のようになります。</p>
<pre><code class="language-bash">(none):~# ls -lat /
total 0
drwx------    3 root     root            80 Sep 23 06:44 root
drwxr-xr-x    5 root     root           200 Sep 23 06:44 run
drwxr-xr-x   19 root     root           400 Sep 23 06:44 .
drwxr-xr-x   19 root     root           400 Sep 23 06:44 ..
drwxr-xr-x    7 root     root          2120 Sep 23 06:44 dev
dr-xr-xr-x   12 root     root             0 Sep 23 06:44 sys
dr-xr-xr-x   55 root     root             0 Sep 23 06:44 proc
drwxr-xr-x    2 root     root          1780 May  7 00:55 bin
drwxr-xr-x   26 root     root          1040 May  7 00:55 etc
lrwxrwxrwx    1 root     root            10 May  7 00:55 init -&gt; /sbin/init
drwxr-xr-x    2 root     root          3460 May  7 00:55 sbin
drwxr-xr-x   10 root     root           700 May  7 00:55 lib
drwxr-xr-x    9 root     root           180 May  7 00:54 usr
drwxr-xr-x    2 root     root            40 May  7 00:54 home
drwxr-xr-x    5 root     root           100 May  7 00:54 media
drwxr-xr-x    2 root     root            40 May  7 00:54 mnt
drwxr-xr-x    2 root     root            40 May  7 00:54 opt
drwxr-xr-x    2 root     root            40 May  7 00:54 srv
drwxr-xr-x   12 root     root           260 May  7 00:54 var
drwxrwxrwt    2 root     root            40 May  7 00:54 tmp
</code></pre>
<p>お疲れ様でした。ひとまずここまで来ればminimal VMMと言って良いものではないかと思います。
とはいえ現状以下のような問題点があります</p>
<ul>
<li>serialコンソール経由でしか操作できない -&gt; virtio-netを実装したい</li>
<li>virtio-blkを実装したい</li>
<li>PCIデバイスを扱えていない</li>
</ul>
<p>実は、ToyVMMを作成し始めた個人的な目的の一つとしては以下のような目標がありました。</p>
<ul>
<li>virtualizationの理解を深める</li>
<li>virtioについて理解を深める</li>
<li>pci passthroughについて理解を深める
<ul>
<li>vfioなどの技術について詳細を知る</li>
<li>mdev/libvfio/vdpaなどの周辺技術について知る</li>
</ul>
</li>
</ul>
<p>MinimalなVMMの作成は完了しましたが、この先VMMとしてどのように成長させていくかというのは人それぞれで多くの選択肢があると思います。
ToyVMMでは今後上記のようなtopicについて扱っていきたいと考えています！<br />
ToyVMMをベースにして別の方向に拡張するというのも面白いでしょう。もしこれを読んでくれているGeekがいればぜひ挑戦してください。そしてもしよければToyVMMにもfeedbackしてくれると大変嬉しく思います。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtual-io-device-virtio"><a class="header" href="#virtual-io-device-virtio">Virtual I/O Device (Virtio)</a></h1>
<p>このセクションではVMMのセカンドステップとして、Virtioの実装について触れることにする。<br />
VirtioについてはOASISが仕様の策定とメンテナンスを行なっている。<br />
現在は2022/07/01に公開された<a href="http://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html">version 1.2</a>が最新のようである。<br />
本資料におけるVirtioに関する用語はversion 1.2の記載に則っているため、用語の意味を確認する際はOASISのページを参照されたい。</p>
<p>このセクションではVirtioに関する基礎的な知識の習得とその実装について触れる。<br />
また、Virtioをベースとした具体的なデバイスの実装として、<code>virtio-net</code>、<code>virtio-blk</code>についても実装していく。<br />
<code>virtio-net</code>が実装できると、起動したGuest VMにたいしてネットワーク経由で疎通することになるため、sshログインできたり、インターネットと繋いだりすることができるようになる。<br />
また、<code>virtio-blk</code>が実装できると、仮想マシン上でブロックデバイス、つまりDISK I/Oを取り扱うことができるようになる。
この二つの機能により、一般的に想定される「仮想マシン」の要件が概ね揃うことになるため、Virtioの実装は極めて重要である。</p>
<p>本セクションの各トピックは次のようになっている</p>
<ul>
<li><a href="./03-1_virtio.html">03-1. Virtio</a></li>
<li><a href="./03-2_implement_virtio_in_toyvmm.html">03-2. Implement virtio in ToyVMM</a></li>
<li><a href="./03-3_virtio-net.html">03-3. Implement virtio-net</a></li>
<li><a href="./03-4_virtio-blk.html">03-4. Implement virtio-blk</a></li>
</ul>
<p>また、本資料は以下のコミットナンバーをベースとしている</p>
<ul>
<li>ToyVMM: 58cf0f68a561ee34a28ae4e73481f397f2690b51</li>
<li>Firecracker: cfd4063620cfde8ab6be87ad0212ea1e05344f5c</li>
</ul>
<p>以降では実装されているソースコードをファイル名を用いて説明することになる。
説明の中で出てくるファイル名が指している実際のファイルパスをまとめておく。</p>
<table><thead><tr><th>説明の中で触れるファイル名</th><th>ファイルパス</th></tr></thead><tbody>
<tr><td><code>mod.rs</code></td><td>src/devices/virtio/mod.rs</td></tr>
<tr><td><code>queue.rs</code></td><td>src/devices/virtio/queue.rs</td></tr>
<tr><td><code>mmio.rs</code></td><td>src/devices/virtio/mmio.rs</td></tr>
<tr><td><code>status.rs</code></td><td>src/devices/virtio/status.rs</td></tr>
<tr><td><code>virtio_device.rs</code></td><td>src/devices/virtio/virtio_device.rs</td></tr>
<tr><td><code>net.rs</code></td><td>src/devices/virtio/net.rs</td></tr>
<tr><td><code>block.rs</code></td><td>src/devices/virtio/block.rs</td></tr>
</tbody></table>
<p>なお、今後ソースコードに修正がかかることが想定されるため上記のファイルパスは変更される可能性がある。<br />
あくまでこのファイルパスは上記のコミット番号におけるファイルパスだと認識してほしい　</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="virtio"><a class="header" href="#virtio">Virtio</a></h1>
<h3 id="virtual-io-device-virtio-とは"><a class="header" href="#virtual-io-device-virtio-とは">Virtual I/O Device (Virtio) とは</a></h3>
<p>Virtioは、<a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=virtio">OASIS</a> によって標準化されている仮想デバイスに関する仕様であり、ホストシステムとゲストシステム（仮想マシン）間で効率的なデータ転送や通信を行うための仮想デバイスインターフェイスを提供している。</p>
<p>Virtioをベースとした実装に<code>virtio-net</code>（仮想ネットワークデバイス）、<code>virtio-blk</code>（仮想ブロックデバイス）などが存在し、その名の通りそれぞれがネットワークデバイス／ブロックデバイスとしての振る舞いを実装している。<br />
これによりゲストOSはあたかも実際のネットワークデバイスやブロックデバイスを利用するかのようにI/Oを実施することができるようになっている。</p>
<p>Virtioは、KVMをはじめとする主要な仮想化技術と互換性があり、Linux、Windows、FreeBSDなど多くのゲストOSに対応しているため、仮想化環境において広く採用されており、業界標準の仕様となっている。</p>
<h3 id="なぜvirtioが必要か"><a class="header" href="#なぜvirtioが必要か">なぜVirtioが必要か?</a></h3>
<p>そもそもVM上でI/Oを発生させたい時、Hypervisorの観点からはどう処理すればいいだろうか？
何よりもまず、VM起動時にデバイスを認識させなければならず、素朴には各種PCIデバイスのエミュレーションを行う必要があるだろう。<br />
それ以外にも、そのデバイスに対してI/Oを発生させた際に、そのデバイスの振る舞いを模倣する必要がある。
この類のハードウェアエミュレータとして有名で広く利用されるソフトウェアとして<a href="https://www.qemu.org/">QEMU</a>が存在する。</p>
<p>実ハードウェアを完全にソフトウェアでエミュレーションできれば、ゲストOSに付属している実ハードウェア向けのデバイスドライバをそのまま利用できるというメリットがある。<br />
しかしこの方式は、VM内部でのI/O要求が発生する度にVMExitが発生し、Hypervisor側でエミュレーション処理を実施した後、VMに処理を戻す必要があるため、大きなオーバーヘッドを伴うことになる。</p>
<p>上記のようなデバイスI/Oにおける仮想化のオーバーヘッドを低減する方法の一つとして提案され標準化されているフレームワークが「Virtio」である。
VirtioではHypervisorとVMの間で共有されたメモリ上にVirtqueueと呼ばれるキュー構造を作成し、これを用いてデータの入出力を行う方式を採用することで、VMExitによるモード遷移回数を抑制するような仕組みになっている。
一方で、Virtio向けのデバイスドライバは別途必要になり、これはカーネルビルド時の設定に依存することになるが、現在の多くのOSではデフォルトでVirtioデバイスドライバはデフォルトでインストールされている。</p>
<h3 id="virtioの構成要素"><a class="header" href="#virtioの構成要素">Virtioの構成要素</a></h3>
<p>Virtioは概ね以下の構成要素からなる</p>
<ul>
<li>Virtqueue : ホスト・ゲスト間で共有されたメモリ領域上に構築されるキュー。これを介してデータの入出力を実施する。</li>
<li>Virtio driver : Virtioをベースとしたデバイスに対するゲスト側のドライバ。</li>
<li>Virtio device : ホスト側でエミュレーションするデバイスの実装。</li>
</ul>
<img src="./03_figs/virtio-overview.svg" width="100%">
<p>図にもある通り、ゲストから発行されたI/OリクエストはVirtqueueを仲介してホストに渡り、ホストからの返答も同様にVirtqueueを仲介してゲストに戻ってくる。
より詳細な挙動や実装についての解説は次節で行う。</p>
<p>また、Virtioデバイスをゲストに対してさらす際、特定のTransport methodを選択して提供することが可能である。
代表的なものとしてはPCI（Peripheral Component Interconnect）を利用した「Virtio Over PCI Bus」方式と、MMIO（Memory Mapped I/O）を利用した「Virtio Over MMIO Bus」である。
ゲストには、特定のTransportに紐づくドライバである<code>virtio-pci</code>や<code>virtio-mmio</code>などが存在し、更に特定のデバイスタイプ向けのvirtioドライバ（<code>virtio-net</code>, <code>virtio-blk</code>）が存在する。</p>
<p>ToyVMMではひとまず<code>virtio-mmio</code>をTransportとして採用し、その上で<code>virtio-net</code>向けのNetworkデバイス、及び<code>virtio-blk</code>向けのBlockデバイスを実装していくこととする。</p>
<h3 id="references-4"><a class="header" href="#references-4">References</a></h3>
<ul>
<li><a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=virtio">OASIS</a></li>
<li><a href="https://www.cs.cmu.edu/%7E412/lectures/Virtio_2015-10-14.pdf">Virtio: An I/O virtualization framework for Linux</a></li>
<li><a href="https://ozlabs.org/%7Erusty/virtio-spec/virtio-paper.pdf">virtio: Towards a De-Facto Standard For Virtual I/O Devices</a></li>
<li><a href="https://blogs.oracle.com/linux/post/introduction-to-virtio">Introduction to VirtIO</a></li>
<li><a href="https://docs.kernel.org/driver-api/virtio/virtio.html">Virtio on Linux</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implement-virtio-in-toyvmm"><a class="header" href="#implement-virtio-in-toyvmm">Implement virtio in ToyVMM</a></h1>
<p>本節では、ToyVMMにおけるVirtioの具体的な実装について記載する。<br />
以下の3項目が本説における主要なトピックである</p>
<ul>
<li>Virtqueueの実装</li>
<li>irqfd, ioeventfdによるゲスト・ホスト間の軽量な通知の実装</li>
<li>MMIO Transportの実装</li>
</ul>
<p><a href="./03-1_virtio.html">前節</a>でも述べた通りToyVMMではTransportとしてひとまずMMIOを利用する。<br />
具体的な説明に入る前に、以下に今回実装したVirtioの実装の全体像を図示する。</p>
<div align="center">
<img src="./03_figs/virtio_implementation.svg", width="100%">
</div>
<p>この図を適宜見返しつつ、以降の説明とコードを読み進めることで理解が深まるはずである。</p>
<h3 id="実装方針"><a class="header" href="#実装方針">実装方針</a></h3>
<p>実装においては、VirtioDevice自体は抽象概念 (<code>Trait</code>) として実装し、これを満たす構造体として具体的に<code>Net</code>や<code>Block</code>といったデバイスを作成していく。
また、Transportについても<code>PCI</code>と<code>MMIO</code>で選択肢があるためこれも抽象概念とし、それぞれの実体（今回は<code>MMIO</code>）に合わせて実装する形にする。</p>
<div align="center">
<img src="./03_figs/virtio_related_structs.svg" width="50%">
</div>
<p>最後にVirtqueueの実装が必要になるが、これは実装するVirtioデバイスによって個数や利用方法は異なるが、その構造自体は変化しないことを前提に素直に実装する。詳細は後述する。</p>
<h3 id="virtqueueの実装"><a class="header" href="#virtqueueの実装">Virtqueueの実装</a></h3>
<h4 id="virtqueue-deep-dive"><a class="header" href="#virtqueue-deep-dive">Virtqueue Deep-Dive</a></h4>
<p>Virtqueueの実装に入る前、一般的なVirtQueueの構造についてより詳細に把握しておく。
Virtqueueは以下のように、<code>Descriptor Table</code>、<code>Available Ring</code>、<code>Used Ring</code>の3つの要素で構成される。
それぞれの役割は以下のようなものである。</p>
<ul>
<li><code>Descriptor Table</code> : ホストとゲストの間で共有したいデータの位置やサイズなどの情報を格納しているエントリ（<code>Descriptor</code>）を保持しているテーブル</li>
<li><code>Available Ring</code> : ゲストからホストに対して通知したい情報を格納したDescriptorについて管理している構造</li>
<li><code>Used Ring</code> : ホストからゲストに対して通知したい情報を格納したDescriptorについて管理している構造</li>
</ul>
<div align="center">
<img src="./03_figs/virtqueue.svg" width="80%">
</div>
<p>以降、それぞれの要素について詳細に確認していきつつ、どのように協調動作するかを把握していく。
まず、<code>Descriptor Table</code>は具体的には以下のようなデータ構造（図中には<code>Descriptor</code>と記載したエントリ）を集めたものになっている。</p>
<blockquote>
<pre><code class="language-C">struct virtq_desc { 
        /* Address (guest-physical). */ 
        le64 addr; 
        /* Length. */ 
        le32 len; 
 
/* This marks a buffer as continuing via the next field. */ 
#define VIRTQ_DESC_F_NEXT   1 
/* This marks a buffer as device write-only (otherwise device read-only). */ 
#define VIRTQ_DESC_F_WRITE     2 
/* This means the buffer contains a list of buffer descriptors. */ 
#define VIRTQ_DESC_F_INDIRECT   4 
        /* The flags as indicated above. */ 
        le16 flags; 
        /* Next field if flags &amp; NEXT */ 
        le16 next; 
};
</code></pre>
<p>Source: <a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-430005">2.7.5 The Virtqueue Descriptor Table</a></p>
</blockquote>
<p><code>Descriptor</code>は転送するデータについてと、次のデータのありかについて表現されている構造になっている。</p>
<ul>
<li><code>addr</code>が実際のデータのアドレス（ゲストの物理アドレス）であり、そのデータ長の情報は<code>len</code>から取得できる。</li>
<li><code>flags</code>は、次のdescriptorがあるかどうかや、write onlyかどうかなどの情報を提供するフラグである。</li>
<li><code>next</code>は次のdescriptor番号を示しており、この値を元にDescriptor Tableを順番に処理していけるようになっている。</li>
</ul>
<p>基本的には、1つのデータを送信するために一つのDescriptorが利用されるが、データのアドレスがゲスト上の物理アドレスであり、連続している範囲に限られることに注意されたい。もし仮想アドレス上で連続する領域を確保していてたとしても、物理アドレス上で連続していない場合は、物理ページ毎にDescriptorが1つ必要になり複数のDescriptorを連続して送信することになる。</p>
<p>次に<code>Avaialble Ring</code> である。<code>Available Ring</code>は以下のようなデータ構造になっている。</p>
<blockquote>
<pre><code class="language-C">struct virtq_avail { 
#define VIRTQ_AVAIL_F_NO_INTERRUPT      1 
        le16 flags; 
        le16 idx; 
        le16 ring[ /* Queue Size */ ]; 
        le16 used_event; /* Only if VIRTIO_F_EVENT_IDX */ 
}
</code></pre>
<p>Source: <a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-490006">2.7.6 The Virtqueue Available Ring</a></p>
</blockquote>
<p><code>Avaiable Ring</code>は「ゲストからホストへ」通知したいDescriptorを指定するために利用する。</p>
<ul>
<li><code>flags</code>は割り込みの一時的な抑制などに利用されるフラグ</li>
<li><code>idx</code>は<code>ring</code>で一番新しいエントリのインデックス</li>
<li><code>ring</code>はRingの本体。Descriptorの番号を保持している</li>
<li><code>used_event</code>も割り込みの抑制に利用されるデータであるが、こちらは後述する<code>Used Ring</code>のエントリがこの値で指定したindexに書き込まれるまで通知が不要であることをデバイスに通知するできるというもの</li>
</ul>
<p>ゲストは実際のデータのありかについてはDescriptorに書き込み、Descriptorのindex情報をこの<code>Available Ring</code>（実際にはそのフィールドである<code>ring</code>）に書き込む。
一つ重要な事実として、<strong>ホストは最後に処理した<code>ring</code>のエントリ番号を記憶しておく必要がある</strong>。
ゲストから与えられ得る情報はあくまで、現在のringの状態、及びringの最新index情報（<code>idx</code>フィールド）であるため、ホストは最後に処理したエントリの番号と最新のindex情報を比較し、差分がある(=新しいエントリがある)場合に、ringを参照してDescriptorのindexを取得し、Descriptorから対応するデータを取得する。そして取得したデータをもとにどのような処理をするかは、実際のDeviceの実装に依存する。</p>
<p>最後に<code>Used Ring</code>である。<code>Userd Ring</code>は以下のようなデータ構造になっている。</p>
<blockquote>
<pre><code class="language-C">struct virtq_used { 
#define VIRTQ_USED_F_NO_NOTIFY  1 
        le16 flags; 
        le16 idx; 
        struct virtq_used_elem ring[ /* Queue Size */]; 
        le16 avail_event; /* Only if VIRTIO_F_EVENT_IDX */ 
}; 
 
/* le32 is used here for ids for padding reasons. */ 
struct virtq_used_elem { 
        /* Index of start of used descriptor chain. */ 
        le32 id; 
        /* 
         * The number of bytes written into the device writable portion of 
         * the buffer described by the descriptor chain. 
         */ 
        le32 len; 
};
</code></pre>
<p>Source: <a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-540008">2.7.8 The Virtqueue Used Ring</a></p>
</blockquote>
<p><code>Used Ring</code>は<code>Avaialble Ring</code>の逆、つまり「ホストからゲストへ」通知したいDescriptorを指定するために利用する。</p>
<ul>
<li><code>flags</code>は割り込みの一時的な抑制などに利用されるフラグ</li>
<li><code>idx</code>は<code>ring</code>で一番新しいエントリのインデックス</li>
<li><code>ring</code>はRingの本体。Descriptorの番号を保持している</li>
<li><code>used_event</code>も割り込みの抑制に利用されるデータであるが、こちらは後述する<code>Used Ring</code>のエントリがこの値で指定したindexに書き込まれるまで通知が不要であることをデバイスに通知するできるというもの</li>
</ul>
<p>ホストからゲストに通知を返す場合も、リプライに対応するデータをゲストに返却すべく、Descriptorを利用してデータの在処をゲストに伝える。
そして、<code>Used Ring</code>の<code>ring</code>にDescriptorのindexを格納し、idxの値は<code>ring</code>の最新のindexを指すように更新した上でゲストに処理を戻す。</p>
<p>ただし、<code>Avaiable Ring</code>とは少し異なり<code>ring</code>の要素が構造（<code>virtq_used_elem</code>）を伴っている</p>
<ul>
<li><code>id</code>はDescriptorチェーンの先頭エントリ (virtq_avail.idxと同じ)</li>
<li><code>len</code>は<code>id</code>で指すDescriptorチェーンに対して、ホスト側で行われたI/Oの総量などを格納する。</li>
</ul>
<p>これまで説明してきた内容を一枚の図にまとめると以下のようになる。</p>
<div align="center">
<img src="./03_figs/virtqueue_desc_avail_used_flow.svg", width=100%>
</div>
<p>以上が、Virtqueueの実装に必要な知識である</p>
<h4 id="virtqueue-implementation-on-toyvmm"><a class="header" href="#virtqueue-implementation-on-toyvmm">Virtqueue implementation on ToyVMM</a></h4>
<p>ToyVMMでは<code>queue.rs</code>にVirtqueueの実装を行っている。</p>
<p><code>Descriptor Table</code>、<code>Available Ring</code>、<code>Used Ring</code>のGuest Memory上の具体的なアドレスなどは、Guest VM起動時のGuest側のDevice Driverとのやりとりによって設定されることになる。
この一連のやり取りについては後ほど実際にゲストからのIO要求を覗き見ながら確認するが、ここではこの事実だけ把握されたい。</p>
<p>ToyVMM側はそれぞれの先頭アドレスの位置とVirtioの仕様を元にアドレスアクセスを行っていく必要がある。
基本的にはDescriptorのエントリ単位で処理を行うことが想定される（このエントリの先に具体的なデータのアドレスが格納されている）
このデータ処理に伴って<code>Available Ring</code>や<code>Used Ring</code>を更新する形になる。</p>
<p>さて具体的にコードを参照しながら解説を勧めていく。
<code>Queue</code>という構造体がToyVMMにおける<code>Virtqueue</code>を表現する構造体である。定義としては以下のようになっている。</p>
<pre><code class="language-Rust">#[derive(Clone)]
/// A virtio queue's parameters
pub struct Queue {
    /// The maximal size in elements offered by the device
    max_size: u16,

    /// The queue size in elements the driver selected
    pub size: u16,

    /// Indicates if the queue is finished with configuration
    pub ready: bool,

    /// Guest physical address of descriptor table
    pub desc_table: GuestAddress,

    /// Guest physical address of the available ring
    pub avail_ring: GuestAddress,

    /// Guest physical address of the used ring
    pub used_ring: GuestAddress,

    next_avail: Wrapping&lt;u16&gt;,
    next_used: Wrapping&lt;u16&gt;,
}
</code></pre>
<p>Virtqueuesに必要な、<code>Descriptor Table</code>, <code>Available Ring</code>, <code>Used Ring</code>の定義が確認できるだろう。それぞれゲストメモリ上のアドレス値を指し示しており、このはゲストのDevice Driverとのやり取りで初期化される部分である。
前述したとおりToyVMMからみるとそれぞれのデータはあくまでゲストの物理メモリでしかなく、これに対してVirtioの仕様に基づいたアドレスアクセスを伴っていく。</p>
<p>では具体的なアドレスアクセスについて見ていく。
ToyVMMでは<code>Avail Ring</code>の状態から<code>Descriptor</code>を取得する一連の流れを&quot;VirtqueueのIteration&quot;として隠蔽しており、Virtqueueを利用した実際のデバイスの実装においては以下のようなコードを記載する形になっている。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// queue : 'Queue' struct
// desc_chain: 'DescriptorChain' struct
for desc_chain in queue.iter(mem) {
  // 'desc_chain' contrains the 'addr', 'len', 'flags' and 'next' values of the descriptor
  // behind the iteration, the data related to 'queue.avail_ring' is adjusted.
}
<span class="boring">}
</span></code></pre></pre>
<p>iterationの裏で何が起きているかを説明していこう。
まず、<code>Queue</code>構造体は以下のようなiter関数を実装しており<code>AvailIter</code>構造体を作成している。<code>AvailIter</code>の作成にあたり、<code>GuestMemory</code>と<code>avail_ring</code>の先頭アドレスから<code>Available Ringの</code>最新の<code>idx</code>を取得している。</p>
<pre><code class="language-Rust">/// A consuming iterator over all available descriptor chain heads offered by the driver
pub fn iter&lt;'a, 'b&gt;(&amp;'b mut self, mem: &amp;'a GuestMemoryMmap) -&gt; AvailIter&lt;'a, 'b&gt; {
    ... // validation codes
    let queue_size = self.actual_size();
    let avail_ring = self.avail_ring;

    // Access the 'idx' fields of available ring
    // skip 2byte (= u16 / 'flags' member) from avail_ring address
    // and get 2byte (= u16 / 'idx' member that represents the newest index of avail_ring) from that address.
    let index_addr = mem.checked_offset(avail_ring, 2).unwrap();
    let last_index: u16 = mem.read_obj(index_addr).unwrap();

    AvailIter {
        mem,
        desc_table: self.desc_table,
        avail_ring: self.avail_ring,
        next_index: self.next_avail,
        last_index: Wrapping(last_index),
        queue_size,
        next_avail: &amp;mut self.next_avail,
    }
}
</code></pre>
<p>上記の通り<code>iter</code>関数を呼び出すことで<code>AvailIter</code>が返却されるが、イテレーション処理ではこの<code>AvailIter</code>が実装している<code>next</code>関数が呼び出される。
<code>next</code>関数は<code>Option&lt;T&gt;</code>を返却するように実装し、値がある場合はその値がイテレーション時の要素になり、<code>None</code>を返すとイテレーション終了になる。
さて、では<code>AvailIter</code>の<code>next</code>関数の実装をみてみる。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>impl&lt;'a, 'b&gt; Iterator for AvailIter&lt;'a, 'b&gt; {
  type Item = DescriptorChain&lt;'a&gt;;

  fn next(&amp;mut self) -&gt; Option&lt;Self::Item&gt; {
    if self.next_index == self.last_index {
      return None;
    }

    // Access the avail_ring element indicated by self.next_index
    // skip 4byte (=u16 x 2 / 'flags' member and 'idx' member from avail_ring)
    // Because of the Ring, calculate modulo (= self.next_index % self.queue_size)
    // and the result of modulo calculation, convert to 2bytes order
    // (because the unit of avail_ring's element is 2byte)
    let offset = (4 + (self.next_index.0 % self.queue_size) * 2) as usize;
    // Calculate the next desc_index address from avail_ring address.
    let avail_addr = match self.mem.checked_offset(self.avail_ring, offset) {
      Some(a) =&gt; a,
      None =&gt; return None,
    };
    // Get the next desc_index value from avail_ring.
    let desc_index: u16 = self.mem.read_obj(avail_addr).unwrap();

    self.next_index += Wrapping(1);

    let ret = DescriptorChain::checked_new(self.mem, self.desc_table, self.queue_size, desc_index);
    if ret.is_some() {
      *self.next_avail += Wrapping(1);
    }
    ret
  }
}
<span class="boring">}
</span></code></pre></pre>
<p><code>self.last_index</code>が<code>iter</code>関数で取得した「現在の<code>avail_ring</code>における最新要素のindex（= ここまで処理したい、という値）」であり、<code>self.next_index</code>が「これまで処理してきたindexの値」になる。
この<code>next</code>の中で<code>self.next_index</code>の値をインクリメントしていくため、Iterationをしていく中でいずれ<code>self.next_index</code>と<code>self.last_index</code>の値が一致する（= 必要なところまで処理しきる）ところまで進みNoneが帰ることで繰り返しが終了する、というのが大枠である。</p>
<p>さらにこの関数のなかで、<code>self.next_index</code>が指す<code>avail_ring</code>上の要素（= descriptorのindexを指している）を取り出し、それを利用して<code>DescriptorChain::checked_new</code>を呼び出し、その値を返却している。</p>
<p>この<code>checked_new</code>関数では関数に渡した上記のindex値から具体的にその要素のアドレスを割り出してアドレスアクセスを行い、<code>desciptor</code>が指しているデータのアドレス(<code>addr</code>)やデータ長(<code>len</code>)、フラグ(<code>flags</code>)、次のdesciptorの番号(<code>next</code>)を取り出したうえで、<code>DescriptorChain</code>構造体を構成する。</p>
<pre><code class="language-Rust">fn checked_new(
    mem: &amp;GuestMemoryMmap,
    desc_table: GuestAddress,
    queue_size: u16,
    index: u16,
) -&gt; Option&lt;DescriptorChain&gt; {
    if index &gt;= queue_size {
        return None;
    }

    // The size of each element of descriptor table is 16 bytes
    // - le64 addr  = 8byte
    // - le32 len   = 4byte
    // - le16 flags = 2byte
    // - le16 next  = 2byte
    // So, the calculation of the offset of the address
    // indicated by desc_index is 'index * 16'
    let desc_head = match mem.checked_offset(desc_table, (index as usize) * 16) {
        Some(a) =&gt; a,
        None =&gt; return None,
    };
    // These reads can't fail unless Guest memory is hopelessly broken
    let addr = GuestAddress(mem.read_obj(desc_head).unwrap());
    mem.checked_offset(desc_head, 16)?;
    let len = mem.read_obj(desc_head.unchecked_add(8)).unwrap();
    let flags: u16 = mem.read_obj(desc_head.unchecked_add(12)).unwrap();
    let next: u16 = mem.read_obj(desc_head.unchecked_add(14)).unwrap();
    let chain = DescriptorChain {
        mem,
        desc_table,
        queue_size,
        ttl: queue_size,
        index,
        addr,
        len,
        flags,
        next,
    };
    if chain.is_valid() {
        Some(chain)
    } else {
        None
    }
}
</code></pre>
<p><code>next</code>関数の帰り値として上記で作成した<code>DesciptorChain</code>を返却しているので、ループ内部ではDescriptorの情報にアクセスする際はIteration処理で取得できる<code>DescriptorChain</code>構造体の該当するメンバにアクセスするだけで良い形になっている。</p>
<p>なお、ここでは説明を省略するが、Iteration処理の中で<code>next</code>が指すDescriptorの取得が必要になるケースが想定される。
これに対応するために<code>DescriptorChain</code>構造体は<code>next_descriptor</code>関数を実装しており、この関数を呼び出すだけで良い。</p>
<p>ここまであまり触れてこなかったが、<code>Used Ring</code>についてもHost側で更新する必要がある。
ただしこちらについては難しい処理はほとんど不要で、以下のような関数を定義しておき必要に応じて呼び出しているだけの実装で基本的に事足りている。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>/// Puts an available descriptor head into the used ring for use by the guest
pub fn add_used(&amp;mut self, mem: &amp;GuestMemoryMmap, desc_index: u16, len: u32) {
    if desc_index &gt;= self.actual_size() {
        // TODO error
        return;
    }
    let used_ring = self.used_ring;
    let next_used = (self.next_used.0 % self.actual_size()) as u64;

    // virtq_used structure has 4 byte entry before `ring` fields, so skip 4 byte.
    // And each ring entry has 8 bytes, so skip 8 * index.
    let used_elem = used_ring.unchecked_add(4 + next_used * 8);
    // write the descriptor index to virtq_used_elem.id
    mem.write_obj(desc_index, used_elem).unwrap();
    // write the data length to the virtq_used_elem.len
    mem.write_obj(len, used_elem.unchecked_add(4)).unwrap();

    // increment the used index that is the last processed in host side.
    self.next_used += Wrapping(1);

    // This fence ensures all descriptor writes are visible before the index update is.
    fence(Ordering::Release);
    mem.write_obj(self.next_used.0, used_ring.unchecked_add(2))
        .unwrap();
<span class="boring">}
</span></code></pre></pre>
<p>次節で説明するvirtio-net device、及びその次に説明するvirtio-blk deviceの実装において、上記で示したqueueのiterationを利用して具体的なIOの実装を行っているのでこの前提を覚えておいてほしい。</p>
<h3 id="irqfd-ioeventfdによるゲストホスト間の軽量な通知の実装"><a class="header" href="#irqfd-ioeventfdによるゲストホスト間の軽量な通知の実装">irqfd, ioeventfdによるゲスト・ホスト間の軽量な通知の実装</a></h3>
<p>Virtqueueの実装についてここまで話してきたが、ここではもう一つVirtqueueに関係する重要な話をしよう。
それはVirtqueueを利用したホストとゲスト間のやり取りに必要になる「通知」の機能である。
VirtioではVirtqueueにデータを詰めたのち、ゲストからホスト、またはホストからゲストに通知を行う仕組みが必要になる。
この通知はどのように実現されているのか、という話を抑えておく必要がある。</p>
<p>端的にいえば、GuestとHostの間での通知は<code>ioeventfd</code>と<code>irqfd</code>の仕組みを利用することで実現している。
これはともに<code>KVM API</code>によって提供される仕組みである。</p>
<p>まずGuestからHostに対する通知であるが、これは<code>ioeventfd</code>を利用して実装する。
<code>ioeventfd</code>はGuest VMで発生したPIO/MMIOによるメモリへの書き込みをeventfdの通知に変化する仕組みである。
KVM APIとしては<code>KVM_IOEVENTFD</code>を利用し、引数に通知を受けるeventfdやMMIOのアドレスを渡すことで、この引数のMMIOアドレスへの書き込みが、同じく引数のeventfdへの通知に変換される。
これによってKVM APIを呼び出したソフトウェア（= ToyVMM）側で、eventfdからゲストからの通知を受け取ることができる。
この仕組みはイベント通知の効率を向上させるための仕組みであり、従来のポーリングや割り込みハンドラによる方式に比べ軽量な実装になっている。</p>
<div align="center">
<img src="./03_figs/ioeventfd.svg" width="50%">
</div>
<p>次にHostからGuestに対する通知だが、これは<code>irqfd</code>の仕組みを利用する。
<code>irqfd</code>はこれまでの実装でも既に利用しているが、<code>KVM_IRQFD</code>を利用し、引数に通知に利用するeventfdと対応して発火させたいGuestのirq番号を渡すことで、ToyVMM側でのeventfdへのwriteが、Guestのirqへのハードウェア割り込みに変換される。</p>
<div align="center">
<img src="./03_figs/irqfd.svg" width="50%">
</div>
<p>上記のようなKVM APIをベースとする通知機能を利用し、ゲストとホストの間での通知を実現している。具体的な使い方は次の「MMIO Transportの実装」の説明で触れることとする。</p>
<h3 id="mmio-transportの実装"><a class="header" href="#mmio-transportの実装">MMIO Transportの実装</a></h3>
<p>ここからはMMIO Transportの実装について説明していく。
<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-1650002">Virtio Over MMIO</a>が公式の仕様なので必要に応じてこちらを参照されたい。</p>
<p>MMIO TransportはPCIサポートのない仮想環境などで簡単に利用できる方式である。Firecrackerでは基本的にMMIO Transportのみサポートしているようである。MMIO Transportは特定のメモリ領域へのRead/Writeを通してデバイス操作を行う方式である。</p>
<p>MMIO TransportはPCIのような汎用のDevice discovery仕組みを利用できない。
そのため、MMIOにおけるDevice discoveryは<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-1660001">MMIO Device Discovery</a>で説明のある通り、ゲストOSに対してそのメモリマップされたデバイスの位置と割り込み位置について提供する必要がある。
具体的にはDevice Treeに記載する方式が公式では書かれているが、それ以外に<a href="https://docs.kernel.org/admin-guide/kernel-parameters.html">カーネル起動時のコマンドライン引数に埋め込む方法</a>が存在している。Guest VM起動時にこのコマンドライン引数はToyVMMから動的に調整できるため今回は後者の方式をとることとした。</p>
<p>この方式では以下のような書式に従って記述することでMMIO deviceのdiscoveryを行うようにGuest VMに情報を提供することができる。</p>
<pre><code class="language-bash">(format)
virtio_mmio.device=&lt;size&gt;@&lt;baseaddr&gt;:&lt;irq&gt;

(example)
virtio_mmio.device=4K@0xd0000000:5
</code></pre>
<p>この場合、Guest VMは0xd0000000のアドレスを起点として、決まったオフセット（Register位置）にRead/Writeを実施することでデバイスの初期化や設定を実施する。
<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-1670002">MMIO Device Register Layout</a>に記載してあるのがその全てである。</p>
<p>ToyVMMの観点からみると、それぞれのRegisterに対してRead/Writeが発生した際に、上記のSpecに従った処理が実施されることを保証しなければならず、これを実装するというのがMMIO Transportの実装の中核である。
基本的には上記のMMIOの領域に対するIOは<code>KVM_EXIT_MMIO</code>として処理が渡ってくるので、これをうまくハンドルすれば良く、デバイスの初期化などは基本的にこの流れで処理する。</p>
<p>一方で、これまで議論してきたVirtqueueを通したI/Oのための通知については、ioeventfd, irqfdを使って取り回すことにしたい。
MMIO Transportではベースアドレスからのoffsetが0x050の位置にWriteすることが、デバイスに対して「処理すべきデータがバッファに存在する」ということを通知する処理に該当する。
つまり、このアドレスと、Eventfdを<code>KVM_IOEVENTFD</code>によって紐づけ、更に実装したいVirtio Deviceの処理をこのEventfdをハンドルする形で記述すれば、eGuestで発生したNotify（MMIOへの書き込み）が、直接Eventfdへの割り込みとして通知され、それに従いデバイスを処理するコードがかける。</p>
<p>さらに、コマンドライン経由でゲストに渡す情報としてIRQの情報を提示しているため、ゲストは指定されたIRQに対して割り込みが入った場合に、対応するデバイスのハンドラを起動するようにセットアップを進めることになる。<br />
逆にGuest VMに対して見せたVirtioデバイスに割り込みをかけたい時（Guest VMに処理を移譲したい場合）には、このIRQに対して割り込みをかけることができれば良いということになる。
つまり、Eventfdを作成して上記のIRQ番号とともに<code>KVM_IRQFD</code>で登録し、以降はToyVMM側でこのEventfdに対してwriteすることで割り込みがかけられる。</p>
<p>以上の話をまとめたのが以下の図でありToyVMMはこの実装を行っている。</p>
<div align="center">
<img src="./03_figs/mmio_transport.svg" width="80%">
</div>
<h4 id="mmio-transport---mmio-device-register-layoutに対応する実装"><a class="header" href="#mmio-transport---mmio-device-register-layoutに対応する実装">MMIO Transport - MMIO Device Register Layoutに対応する実装</a></h4>
<p>MMIO Transportの実装は<code>mmio.rs</code>に存在している。
<code>MmioTransport</code>は<a href="./02-5_serial_console_implementation.html">Serial Console implementation</a>で触れたI/O Busを表現する構造体と同様に、<code>BusDevice</code> Traitを実装しており、これは<code>Bus</code>構造体に登録される。<code>KVM_EXIT_MMIO</code>に対応する<code>VcpuExit::MmioRead</code>、<code>VcpuExit::MmioWrite</code>といったMMIO領域へのI/Oを、従来の<code>VcpuExit::IoIn</code>、<code>VcpuExit::IoOut</code>と同じように処理できるようにしている。
従って<code>MmioTransport</code>が<code>BusDevice</code>を満たすために実装している<code>read</code>、<code>write</code>関数に具体的なレジスタアクセスに対する処理、つまりデバイスのエミュレーション処理が記載してある。当然ながらこれは<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-1670002">MMIO Device Register Layout</a>の仕様に従って処理するような実装がなされている。参考程度に<code>read</code>の実装の一部を以下に掲載する</p>
<pre><code class="language-Rust">impl BusDevice for MmioTransport {
    // OASIS: MMIO Device Register Layout
    #[allow(clippy::bool_to_int_with_if)]
    fn read(&amp;mut self, offset: u64, data: &amp;mut [u8]) {
        match offset {
            0x00..=0xff if data.len() == 4 =&gt; {
                let v = match offset {
                    0x0 =&gt; MMIO_MAGIC_VALUE,
                    0x04 =&gt; MMIO_VERSION,
                    0x08 =&gt; self.device.device_type(),
                    0x0c =&gt; VENDOR_ID,
                    0x10 =&gt; {
                        self.device.features(self.features_select)
                            | if self.features_select == 1 { 0x1 } else { 0x0 }
                    }
                    0x34 =&gt; self.with_queue(0, |q| q.get_max_size() as u32),
                    0x44 =&gt; self.with_queue(0, |q| q.ready as u32),
                    0x60 =&gt; self.interrupt_status.load(Ordering::SeqCst) as u32,
                    0x70 =&gt; self.driver_status,
                    0xfc =&gt; self.config_generation,
                    _ =&gt; {
                        println!(&quot;unknown virtio mmio register read: 0x{:x}&quot;, offset);
                        return;
                    }
                };
                LittleEndian::write_u32(data, v);
            }
            0x100..=0xfff =&gt; self.device.read_config(offset - 0x100, data),
            _ =&gt; {
                // WARN!
                println!(
                    &quot;invalid virtio mmio read: 0x{:x}:0x{:x}&quot;,
                    offset,
                    data.len()
                );
            }
        }
    }
</code></pre>
<p><code>read</code>, <code>write</code>をそれぞれ詳細に説明すると膨大な量になるためここでは説明を割愛するが、見たとおり素直な実装になっているため、仕様を見ながらソースコードを確認すれば容易に理解できるだろう。</p>
<p>この部分の処理はGuest VMの起動時に生じるVirtio deviceに対するDevice Driverからの初期化シーケンスで実施される初期化や設定などで呼び出されることになるため、ここにデバッグコードを仕込むことで、ゲストから発生するデバイス初期化シーケンスを観察することができる。
以降では、実際にGuest VMが起動するタイミングで実施されるやり取りを観察しつつ、OASISの仕様と照らし合わせることで理解の助けとする。</p>
<h4 id="guest-vmの起動シーケンスでのmmio-device初期化処理の観察"><a class="header" href="#guest-vmの起動シーケンスでのmmio-device初期化処理の観察">Guest VMの起動シーケンスでのMMIO Device初期化処理の観察</a></h4>
<p>Guest OSにはVirtioのデバイスドライバ（ゲスト側のドライバ）が存在しており、これが仕様通りにVirtioデバイスの初期化をすることを期待する。
今回のMMIOベースの実装では起動時のカーネルコマンドラインで指定したvirtioデバイスのMMIOレンジの情報をもとに、Guest VMがR/Wをかけるはずであり、Hypervisor側としてはVMExitが発生して適切にトラップする必要があるコード部に該当するため、デバッグコードを仕込むことは容易である。
実際にデバッグコードを仕込んで、ゲストOSの起動シーケンス中にMMIOの領域に対して発生したR/Wを観測してみる。</p>
<p>具体的な処理の流れをみる前に、仕様上での初期化処理について整理しておく。
以降の議論では<code>virtio-net</code>がネットワークデバイスの初期化しようとする処理を題材としている。
デバイスの初期化の仕様は、MMIO Transportにおけるデバイスの初期化（<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-1700003">MMIO-specific Initialization And Device Operation</a>）、汎用的なデバイスの初期化（<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-1060003">General Initialization And Device Operation</a>）、デバイス特有の初期化に分かれる。
これらを合わせると仕様上は概ね以下のような流れになる。</p>
<ol>
<li>Magic Numberの読み出し。Device ID、Device Type、Vendor ID等の読み出し</li>
<li>Deviceのリセット</li>
<li>ACKNOWLEDGEステータスビットを設定</li>
<li>Device feature bitsを読み出し、OSとドライバが解釈できるfeature bitsをデバイスに設定。</li>
<li>FEATURES_OKステータスビットを設定</li>
<li>デバイス固有の設定を実施する（Virtqueueの検出や設定の書き込みなど）</li>
<li>DRIVER_OKステータスビットを設定し、この時点でデバイスはLiveの状態になる。</li>
</ol>
<p>さて、上記を念頭においてでは実際のGuest VM起動時の処理について確認してみよう。
以下では、デバッグコードを仕込んだ上でゲストOSを起動した際に、デバッグコードにより出力された情報に対して、私が説明のためにコメントを付け加えたものである。</p>
<pre><code class="language-bash"># offset 0x00 からmagic numberを読み出し
# Little Endianなので元の値は 116,114,105,118
# 116(10) = 74(16)
# 114(10) = 72(16)
# 105(10) = 69(16)
# 118(10) = 76(16)
# 従って、0x74726976 (magic number)
MmioRead: addr = 0xd0000000, data = [118, 105, 114, 116]
# offset 0x04 から device id(0x02)を読み出し
MmioRead: addr = 0xd0000004, data = [2, 0, 0, 0]
# offset 0x08 から device typeを(net = 0x01)読み出し
MmioRead: addr = 0xd0000008, data = [1, 0, 0, 0]
# offset 0x0c から vendor id (virtio vendor id = 0x00) を読み出し
MmioRead: addr = 0xd000000c, data = [0, 0, 0, 0]

# この辺りはDevice Initializationのフェーズ(3.1.1 Driver Requirements: Device Initialization)
# offset 0x70(= Status) に0を書き込み -&gt; デバイスステータスをリセット
MmioWrite: addr = 0xd0000070, data = [0, 0, 0, 0]
# offset 0x70 から読み込み。今デバイスはリセットされている
MmioRead: addr = 0xd0000070, data = [0, 0, 0, 0]
# offset 0x70 に0x01 = ACKNOWLEDGE をセット。
MmioWrite: addr = 0xd0000070, data = [1, 0, 0, 0]
# offset 0x70 から読み込み。確認か?
MmioRead: addr = 0xd0000070, data = [1, 0, 0, 0]
# offset 0x70(= Status)に0x02 = Device(2)を'加算した値'(1 + 2 = 3)を書き込み
MmioWrite: addr = 0xd0000070, data = [3, 0, 0, 0]

# Device/DriverのFeature bitsに関する処理。
# Deviceは自身が持つ機能セット(feature bits)を提供し、デバイスの初期化時にドライバはこれを読んだ上で、受け入れる機能サブセットをデバイスに指示する。
# 
# まずは、Guest OS上のvirtio device driverがfeature bitsを読みだす処理
# offset 0x14(=DeviceFeatureSel) に0x01をSet(次の処理に関わる)
MmioWrite: addr = 0xd0000014, data = [1, 0, 0, 0]
# offset 0x10(=DeviceFeatures)にたいしてRead.
# 読みだすのはDeviceFeatures bitだが、(DeviceFeatureSel * 32) + 31 bitsを返却する
# 今DeviceFeatureSel=1なので、DeviceFeatures bitsのうち、64~32 bitsを返却する。
# virtio-netだと、DeviceFeatureSel=0x0000_0001_0000_4c83 (64bit)なので、
# 先頭0x0000_0001がLittle Endiandで取り出される。
MmioRead: addr = 0xd0000010, data = [1, 0, 0, 0]
# offset 0x14(=DeviceFeatureSel)に0x00をSet(次の処理に関わる)
MmioWrite: addr = 0xd0000014, data = [0, 0, 0, 0]
# offset 0x10(=DeviceFeatures)にたいしてRead.
# 今DeviceFeatureSel=0なので、DeviceFeatures bitsのうち、31~0 bitsを返却する。
# virtio-netだと、DeviceFeatureSel=0x0000_0001_0000_4c83 (64bit)なので、
# 0x0000_4c83がLittle Endiandで取り出される。以下、値の確認
# Little Endianを戻す: 0,0,76,131
# 76(10) = 4c
# 131(10) = 83
# 0x00004c83 -&gt; avail_features(0x100004c83)のうち、VIRTIO_F_VERSION_1で立てているbit(0x100000000)を無視したbit
# つまり、
# * virtio_net_sys::VIRTIO_NET_F_GUEST_CSUM
# * virtio_net_sys::VIRTIO_NET_F_CSUM
# * virtio_net_sys::VIRTIO_NET_F_GUEST_TSO4
# * virtio_net_sys::VIRTIO_NET_F_GUEST_UFO
# * virtio_net_sys::VIRTIO_NET_F_HOST_TSO4
# * virtio_net_sys::VIRTIO_NET_F_HOST_UFO
# のfeature bitの情報を返却している
MmioRead: addr = 0xd0000010, data = [131, 76, 0, 0]
# feature bitsの読み出しはここまでで、ここからは受け入れる機能サブセットをデバイスに指示する処理
# 処理としては読み出し時と似たような感じで、DriverFeatureSel bitに0x00/0x01を書き込んだ上で、DriverFeatureに設定したいfeature bitsを書き込んでいく
# まず、offset 0x24(DriverFeatureSel/activate guest feature) に対して、0x01をSet(= acked_featuresに0x01を設定)
MmioWrite: addr = 0xd0000024, data = [1, 0, 0, 0]
# offset 0x20(DriverFeatures)に対して0x01(= 0x0000_0001, 先に読み出した値の片方)をSet（DriverFeatureSelに0x01が設定されているので、
# 32bit shiftが起きて、0x0000_0001_0000_0000 が実際は設定される）
MmioWrite: addr = 0xd0000020, data = [1, 0, 0, 0]
# offset 0x24(DeviceFeatureSel/activate guest feature)に対して、0x00をSet(= acked_featuresに0x00を設定)
MmioWrite: addr = 0xd0000024, data = [0, 0, 0, 0]
# offset 0x20(DeviceFeatures)に対して、0x0000_4c83（先に読み出した値の片方）をSet（DriverFeatureSelに0x00が設定されているので、そのまま加算する形で書き込む）
MmioWrite: addr = 0xd0000020, data = [131, 76, 0, 0]
# ここまでで、Feature bitsの処理が完了する。

# offset 0x70(= Status)を読み込み -&gt; 直近で0x03が指定されているので、0x03が返却されるのはよい。
MmioRead: addr = 0xd0000070, data = [3, 0, 0, 0]
# offset 0x70(= Status)に0x08 = FEATURES_OK(8)を'加算した値'(3 + 8 = 11)を書き込み
MmioWrite: addr = 0xd0000070, data = [11, 0, 0, 0]
# offset 0x70(= Status)から読み込み。当然11が返る
MmioRead: addr = 0xd0000070, data = [11, 0, 0, 0]

# device-specific setupはここからになる(4.2.3.2 Virtqueue Configuration)
# offset 0x30(=QueueSel)に 0x00 をSet(self.queue_select)
MmioWrite: addr = 0xd0000030, data = [0, 0, 0, 0]
# offset 0x44(QueueReady)を読み込んで、まだreadyではないので0x0が返却され、これは期待通り。
MmioRead: addr = 0xd0000044, data = [0, 0, 0, 0]
# offset 0x34(QueueNumMax)を読み込んで、queueのサイズを確認。
MmioRead: addr = 0xd0000034, data = [0, 1, 0, 0]
# offset 0x38(QueueNum)に先ほど読み込んだQueueNumをセット(q.size)
MmioWrite: addr = 0xd0000038, data = [0, 1, 0, 0]

# Virtual queue's 'descriptor' area 64bit long physical address
# offset 0x80(QueueDescLow = lo(q.desc_table) / lower 32bits of the address)に、
# QueueSelレジスタで選択したqueue（0）のdiscriptor領域の位置をSet -&gt; 122,209,64,0 -&gt; 0x7ad14000
MmioWrite: addr = 0xd0000080, data = [0, 64, 209, 122]
# 同上。但し0x84(QueueDescHigh = hi(q.desc_table) / higher 32 bits of the address)の残りの部分をSet.
MmioWrite: addr = 0xd0000084, data = [0, 0, 0, 0]
# 二つ合わせると、0x0000_0000_7ad1_4000 (q.desc_table) が先頭アドレス

# Virtual queue's 'driver' area 64bit log physical address
# offset 0x90(QueueDeviceLow = lo(q.avail_ring) / lower 32bits of the address)に、
# QueueSelレジスタで選択したqueue(0)のdriver領域(avail_ring)の位置をSet -&gt; 122,209,80,0 -&gt; 0x7ad15000
MmioWrite: addr = 0xd0000090, data = [0, 80, 209, 122]
# 同上。但し0x94(QueueDeviceHigh = hi(q.avail_ring) / higher 32 bits of the address)の残りの部分をSet.
MmioWrite: addr = 0xd0000094, data = [0, 0, 0, 0]
# 二つ合わせると、0x0000_0000_7ad1_5000 (q.avail_ring)
# q.desc_tableのアドレスレンジ: q.avail_ring - q.desc_table = 0x1000 = 512(10)

# Virtual queue's 'device' area 64bit long physical address
# offset 0xa0(QueueDeviceLow = lo(q.used_ring) / lower 32bits of the address)に、
# QueueSelレジスタで選択したqueue(0)のdevice領域 (q.used_ring) の位置をSet -&gt; 122,209,96.0 -&gt; 0x7ad16000
MmioWrite: addr = 0xd00000a0, data = [0, 96, 209, 122]
# 同上。但し0xa4（QueueDeviceHigh = hi(q.used_ring) / higher 32bits of the address）の残り部分をSet.
MmioWrite: addr = 0xd00000a4, data = [0, 0, 0, 0]
# 二つ合わせると、0x0000_0000_7ad1_6000 (q.used_ring)
# q.avail_ringのアドレスレンジ: q.used_ring - q.avail_ring = 0x1000 = 512(10)

# offset 0x44 (QueueReady = q.ready) に対して0x1を書き込んでReadyにしている
MmioWrite: addr = 0xd0000044, data = [1, 0, 0, 0]

# 上記と同じ流れをもう片方のqueue(1)でも実施している
MmioWrite: addr = 0xd0000030, data = [1, 0, 0, 0]
MmioRead: addr = 0xd0000044, data = [0, 0, 0, 0]
MmioRead: addr = 0xd0000034, data = [0, 1, 0, 0]
MmioWrite: addr = 0xd0000038, data = [0, 1, 0, 0]
MmioWrite: addr = 0xd0000080, data = [0, 128, 196, 122]
MmioWrite: addr = 0xd0000084, data = [0, 0, 0, 0] # q.desc_table = 0x0000_0000_7ad1_8000
MmioWrite: addr = 0xd0000090, data = [0, 144, 196, 122]
MmioWrite: addr = 0xd0000094, data = [0, 0, 0, 0] # q.avail_ring = 0x0000_0000_7ad1_9000
MmioWrite: addr = 0xd00000a0, data = [0, 160, 196, 122]
MmioWrite: addr = 0xd00000a4, data = [0, 0, 0, 0] # q.used_ring = 0x0000_0000_7ad1_a000
MmioWrite: addr = 0xd0000044, data = [1, 0, 0, 0]
# ここまでで、device-specificなセットアップ（virtio-net向けの2つのqueueのセットアップ）が完了

# offset 0x70(= Status)を読み込み -&gt; 0x11が書いてあるのでそれが返却される
MmioRead: addr = 0xd0000070, data = [11, 0, 0, 0]
# offset 0x70(= Status)に0x04 = DRIVER_OK(4)を'加算した値'(11 + 4 = 15)を書き込み
MmioWrite: addr = 0xd0000070, data = [15, 0, 0, 0]
# offset 0x70(= Status) を読み込んで値確認かな?
MmioRead: addr = 0xd0000070, data = [15, 0, 0, 0]
# ここまでで、Device Initializationのフェーズ(3.1.1 Driver Requirements: Device Initialization)は完了する
</code></pre>
<p>冷静に一つ一つ解釈していくと、仕様通りの挙動になっていることがわかる。
デバイス固有の設定読み出しや書き込みについては、<code>MmioTransport</code>初期化時に紐付けている<code>VirtioDevice</code>の実装から取り出している。逆にいうと<code>VirtioDevice</code> Traitはこのような処理に対し必要な情報の提供を行うための実装を要求しているわけである。
また、初期化の流れでoffset=0x70へのMMIO Writeが何度か登場するが、これは初期化シーケンスが進む毎に更新されるステータスの書き込みに該当する。このステータス更新が完了する（<code>ACKNOWLEDGE</code>-&gt; <code>DRIVER</code>-&gt; <code>FEATURES_OK</code>-&gt; <code>DRIVER_OK</code>の遷移）ことをToyVMM側で確認できると、<code>DRIVER_OK</code>のステータス更新後に<code>activate</code>関数を呼び出し、具体的なDeviceのアクティベーション処理（epollやそのhandlerの設定など）を実施する。このアクティベーション処理も実際には各Deviceの実装に委ねられている。</p>
<h3 id="summary"><a class="header" href="#summary">Summary</a></h3>
<p>本節ではToyVMMにおける<code>Virtio</code>の仕組みについて詳細に解説してきた。
以降の節では、本節で述べていない実際のデバイスの実装について、具体的にNetwork Device、及びBlock Deviceの実装を紹介する。
更には、このVirtioの仕組みに則って実装したコードで実際に特定のI/Oが実行できることも合わせて確認していく。</p>
<h3 id="reference-1"><a class="header" href="#reference-1">Reference</a></h3>
<ul>
<li><a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=virtio">OASIS</a></li>
<li><a href="https://syuu1228.github.io/howto_implement_hypervisor/">ハイパーバイザの作り方</a></li>
<li><a href="https://docs.kernel.org/virt/kvm/api.html">The Definitive KVM (Kernel-based Virtual Machine) API Documentation</a></li>
<li><a href="https://github.com/qemu/qemu/">QEMU</a></li>
<li><a href="https://blogs.oracle.com/linux/post/introduction-to-virtio">Introduction to VirtIO</a></li>
<li><a href="https://www.redhat.com/ja/blog/virtqueues-and-virtio-ring-how-data-travels">Virtqueues and virtio ring: How the data travels</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implement-virtio-net-device"><a class="header" href="#implement-virtio-net-device">Implement virtio-net device</a></h1>
<p>本節では具体的なVirtio Deviceの実装としてNetwork Deviceの実装を行っていくことにする。
仕様はVirtioと同様OASISの公式で公開されている<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-2170001">Network Device</a>に記載があるが、本実装はこの仕様に完全に一致している訳ではないので注意されたい。
本節はこれまでに出てきた概念は特に説明なしに利用するため、これまでの節を読んでいない場合は本節の内容を読み進める前に必ず確認されたい。</p>
<h3 id="virtio-netの仕組み"><a class="header" href="#virtio-netの仕組み">virtio-netの仕組み</a></h3>
<p>virtio-net`では本来3種類のVirtqueueを利用する。それぞれ、送信キュー、受信キュー、コントロールキューと呼ばれる。
送信キューはゲストからホストに対してのデータ送信、受信キューはホストからゲストへのデータ送信に利用される。
コントロールキューはゲストからホストに対してNICの設定などを行うためのキューであり、NICのプロミスキャスモードの設定、ブロードキャスト受信やマルチキャスト受信の有効・無効化設定などを行うことに利用できる。
今回は説明と実装を最低限に留めるため、コントロールキューについてはひとまず実装を省略しているので注意されたい。
また、Virtqueuesの数は仕様上スケールさせることが認められているが、実装の単純化のためそれも行っていない。</p>
<p>以降では、より詳細な仕組みについて実装ベースで説明していく。</p>
<h3 id="network-deviceの実装詳細"><a class="header" href="#network-deviceの実装詳細">Network Deviceの実装詳細</a></h3>
<p><code>virtio-net</code>の実装は<code>net.rs</code>に存在している。
以降では、初期化処理段階、初期化後の段階に分けて図示、解説を行っていく。</p>
<p>以下の図は、主に初期化処理にフォーカスしたNetwork Deviceの詳細図である。</p>
<div align="center">
<img src="./03_figs/net-device_init_activate.svg", width="80%">
</div>
<p><code>Net</code>構造体は<code>VirtioDevice</code> Traitを実装しており、<code>MmioTransport</code>に紐づけられるような仕組みになっている。
前節で述べた通り、MMIO Transportの初期化処理におけるデバイス特有の処理に関してはこの<code>Net</code>構造体の実装に依存する。</p>
<p>例えば、初期化時に<code>Device Type</code>の問い合わせが発生するが、これに対して<code>Net</code>デバイスの場合は<code>0x01</code>を返すことが<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-2160005">仕様によって決まっており</a>、<code>Net</code>構造体では以下のような実装を行っている。</p>
<pre><code class="language-Rust">impl VirtioDevice for Net {
    fn device_type(&amp;self) -&gt; u32 {
        // types::NETWORK_CARD:u32 = 0x01
        types::NETWORK_CARD
    }
    ...
}
</code></pre>
<p>同様に、<code>Device Feature</code>の問い合わせについてもそのデバイスに特有の値を返すように実装する形になっている。
また初期化処理の中で、Guest OS側でVirtqueueの<code>Descriptor Table</code>、<code>Available Ring</code>、<code>Used Ring</code>の初期化が行われ、それぞれのアドレスについて通知されるためそれをQueue毎に格納しておき、実際の処理の中でアドレスを参照できるようにしておく。<br />
初期化のステップが完了してStatusが特定の値に更新されると、ToyVMM側ではDeviceに実装した<code>activate</code>関数を実行し必要な処理を完了させる。
<code>Net</code>デバイスの場合はこの<code>activate</code>の中で、各種<code>file descriptor</code>を<code>epoll</code>で登録し、<code>epoll</code>に対してトリガがかかった際に実行するハンドラ(<code>NetEpollHandler</code>)のセットアップを実施している。<br />
<code>Net</code>デバイスはI/Oのエミュレーションするにあたりホスト側にTapデバイスを作成し、Virtqueue経由でゲストから受け取ったデータをTapデバイスに書き出すことで<code>Tx</code>を、Tapデバイスに対して着信したデータをVirtqueueに書き込んでゲストに通知することで<code>Rx</code>を実現する。<br />
<code>epoll</code>に登録する<code>file descriptor</code>は合計4つで、<code>tap</code>デバイスの<code>fd</code>、Tx用Virtqueueの通知に利用する<code>eventfd</code>、Rx用Virtqueueの通知に利用する<code>eventfd</code>、及び予期しない状況に陥った場合に停止させるための<code>eventfd</code>である。</p>
<p>次に、初期化後の状況にフォーカスしたNetwork Deviceの詳細図を以下に示す。</p>
<div align="center">
<img src="./03_figs/net-device_activated.svg", width="100%">
</div>
<p><code>epoll</code>に登録した<code>file descriptor</code>のいずれかにトリガがかかった時、<code>NetEpollHandler</code>をディスパッチしてハンドラ処理を実行する。
<code>NetEpollHandler</code>ではどの<code>file descriptor</code>が発火したかのEVENTに応じて実施する処理が変化する。この処理の詳細については後述する。
いずれにせよ<code>NetEpollHandler</code>に実装されている処理の中でVirtqueueを参照しI/Oエミュレーションを実行している。</p>
<p>また、一つ重要な事実として、<code>KVM_EXIT_MMIO</code>をベースとしたDeviceの初期化処理はvCPUを処理するスレッドの中で呼び出される処理であり、ToyVMMにおいてはメインスレッドとは別のスレッドで実施されている。
一方でI/Oを実行するスレッドはvCPUを処理するスレッドとは別（現時点ではこちらをメインスレッドで処理）にしたいため、初期化した<code>NetEpollHandler</code>をメインスレッドに送るためにChannelを利用している。<br />
こうすることで、Guest VMの起動やCPUエミュレーション処理を別スレッドで進めつつデバイスのI/Oが処理できるようになる。</p>
<p>上述したとおり、Host-Guest間での通信は基本的にVirtqueueに紐づいたEventfdやTapデバイスのfdの発火を起点として<code>NetEpollHandler</code>が担当することになる。
以降では、Tx/Rxそれぞれのケースでどのように処理が実行されていくかについてより詳細に説明をしていく。</p>
<h4 id="tx-guest---host"><a class="header" href="#tx-guest---host">Tx (Guest -&gt; Host)</a></h4>
<p>まずはGuest -&gt; Host方向の通信(Tx)について実装を見ながら詳細を説明する。
改めて、Txの場合は<code>Descriptor Table</code>、<code>Available Ring</code>、<code>Used Ring</code>はそれぞれ以下のように機能する</p>
<ul>
<li><code>Descriptor Table</code> : Guestが送信しようとしているデータを指すようなDescriptorが格納されている</li>
<li><code>Available Ring</code> : 送信データを指すDescriptorのindexを格納しており、これをHostが読み取って必要なDescriptorを辿って処理する</li>
<li><code>Used Ring</code> : Host側で処理が完了したDescriptorのindexを格納し、Guestはこれを読み取って処理済みDescriptorを回収する</li>
</ul>
<p>さてTxのケースの具体的な処理について実装をもとに説明してみる。
Txは、Guest（Guestのデバイスドライバ）がパケットを準備し、<code>QueueNotify</code>に対してWriteが走ることでToyVMMに制御が映る。</p>
<p>具体的に、Guestではまず以下のような処理が行われることが期待される。</p>
<ol>
<li>先頭Descriptorのaddrにデータのアドレス、lenにデータ長を代入</li>
<li>Available Ringのidxが指すAvailable Ringの空きエントリに、1のDescriptor indexを代入</li>
<li>Available Ringのidxの値をインクリメント</li>
<li>未処理データがあることをホストに通知するために、MMIOの<code>QueueNotify</code>へ書き込み</li>
</ol>
<p>ここからは処理がホスト側（ToyVMM）に移ってくる。
MMIOの<code>QueueNotify</code>への書き込みはによって発火したEventFdは<code>epoll</code>の監視によって拾い上げられ<code>NetEpollHandle</code>のハンドラ処理、具体的には<code>TX_QUEUE_EVENT</code>に対応する処理が実行される。</p>
<div align="center">
<img src="./03_figs/net-device_tx_1.svg", width="100%">
</div>
<p>実装的には<code>process_tx</code>関数が呼ばれることになる。</p>
<p><code>process_tx</code>では以下のような形で処理が進んでいく。ぜひコードを確認しながら以降の解説を見てみてほしい。</p>
<ol>
<li>必要な変数等の初期化</li>
</ol>
<ul>
<li><code>frame[0u8; 65562]</code> : Guest側で用意されたデータをコピーするバッファ。</li>
<li><code>used_desc_heads[0u16; 256]</code> : 処理済みの<code>Descriptor</code>のindexを格納し、最後に<code>used_ring</code>へ値をセットするためのデータ。</li>
<li><code>used_count</code> : Guest側のデータをどこまで読み出したか保存しておくカウンタ</li>
</ul>
<ol start="2">
<li>TX用のVirtqueueをiterationして、停止するまで3~5の処理を繰り返す。(VirtqueueのIterationについては前節の説明を参照されたい)</li>
<li><code>Descriptor</code>が指すデータ情報(<code>addr</code>の先のデータ)を読み出してバッファに積み込み、もし<code>next</code>が指す<code>Descriptor</code>があればそれをたどってデータを読みだす。 </li>
<li>読み出したデータをtapに書き込む</li>
<li>処理した<code>Descriptor</code>（<code>Available Ring</code>が指しているDescriptor）のindexを<code>used_desc_heads</code>に保存する。</li>
<li>3~5のiterationで<code>used_desc_heads</code>に格納した処理済み<code>Descriptor</code>の情報<code>used_ring</code>に書き込む。</li>
<li>irqに紐づく<code>eventfd</code>に対してwriteすることでGuestに対して割り込みをかけ処理をGuestに移譲する。</li>
</ol>
<div align="center">
<img src="./03_figs/net-device_tx_2.svg", width="100%">
</div>
<p>以降の処理はGuest側（device driver）に移譲され、以下のように処理されることが期待される。</p>
<ol>
<li><code>Used Ring</code>のidxを確認し、既に処理したindex位置との差分がある場合は、その差分を埋めるように<code>Used Ring</code>の要素(<code>Descriptor index</code>)を確認しする。</li>
<li><code>Desciptor index</code>が指す<code>Descriptor</code>はホストで処理されたものになるため、空き<code>Descriptor</code>チェーンに戻し記録していたDescriptor番号を更新する。</li>
<li>1~2の処理を<code>Used Ring</code>のidxの値と記録しているindex位置の値に差分がなくなるまで繰り返す。</li>
</ol>
<p>以上でTxの処理が完了する。</p>
<h4 id="rx-host---guest"><a class="header" href="#rx-host---guest">Rx (Host -&gt; Guest)</a></h4>
<p>次にHost -&gt; Guest方向の通信(Rx)について実装を見ながら詳細を説明する。
Rxの場合は<code>Descriptor Table</code>、<code>Available Ring</code>、<code>Used Ring</code>はそれぞれ以下のように機能する。</p>
<ul>
<li><code>Descriptor Table</code> : Tapから受け取ったデータをGuestが参照できるよう、受信データを指すようなDescriptorが格納されている。</li>
<li><code>Available Ring</code> : Guest側で処理が完了した空きDescriptorの受け渡しに利用される。</li>
<li><code>Used Ring</code> : 受信データを指すDescriptorのindexを格納しており、これをGuestが読み取って必要なDescriptorをたどって処理する。</li>
</ul>
<p>Txの場合と比較して、ちょうど<code>Available Ring</code>と<code>Used Ring</code>の役割が逆転していることがわかるだろう。</p>
<p>Rxの場合はTxとは異なり二種類のイベントをトリガとする。
１つ目はTapデバイスからの受信パケット、もう一つはGuestからのRx用Virtqueueの処理完了通知である。
Rxはこの二種類のイベントトリガをうまく取り回す必要がありTxの場合に比べて煩雑になる。</p>
<p>以降ではまず基本的なRxの処理フローを説明した上で、考慮しなければいけない協調動作についても少し触れることにする。</p>
<h5 id="rxの処理の基本的なフロー"><a class="header" href="#rxの処理の基本的なフロー">Rxの処理の基本的なフロー</a></h5>
<p>ホストはTapで受信したデータをRx用のVirtqueueに詰めてゲストに知らせることになる。
そのためにはまずRx用のVirtqueueについて、どの位置にデータを詰めればいいかなどの基本的なセットアップがなされていないといけない。
（ToyVMMからみた時、あくまでVirtqueueの各要素はゲストのメモリアドレスでしかなく、Virtqueueの仕様に基づいたメモリアクセスによって必要な処理を行っていたことを思い出してほしい）</p>
<p>翻って、Rxの場合Guestでは以下のような処理が行われることが期待される。</p>
<ol>
<li>Descriptorチェイン等の初期化後、空きDescriptorチェーンの先頭番号をAvailable Ringのidxが指す空きエントリに代入</li>
<li>Available Ringのidxの値をインクリメント。</li>
<li>ホストに通知するために、MMIOの<code>QueueNotify</code>へ書き込み</li>
</ol>
<p>ホスト側では、Rx Virtqueueの通知をゲストから受け取ると「Rx用のデータを詰めるための領域へのアドレスアクセスの準備が整った」と解釈することができる。</p>
<div align="center">
<img src="./03_figs/net-device_rx_1.svg", width="100%">
</div>
<p>さて、ここでTapデバイスにパケットが届いたとする。Tap用の<code>file descriptor</code>のトリガを検知することで、<code>NetEpollHandler</code>がディスパッチされ<code>RX_TAP_EVENT</code>のイベント処理を実施する。
この処理は基本的には<code>process_rx</code>関数を呼び出す。ある条件に従ってはその限りではないが、この条件については後ほど記載する。</p>
<p><code>process_rx</code>では以下のような形で処理が進んでいく。ぜひコードを確認しながら以降の解説を見てみてほしい。</p>
<ol>
<li><code>process_rx</code>ではTapに届いた可能な限りのframeを処理するため、Tapにreadをかけデータが取得できなくなるまで2以降の処理をループする</li>
<li>Tapに対するreadが成功した場合は、読み出すことができたデータサイズを<code>self.rx_count</code>に格納しておき、単一frameを処理する関数である　<code>rx_single_frame</code>を呼び出す。</li>
<li><code>rx_single_frame</code>ではまず<code>Available Ring</code>から先頭エントリを取得し、そのエントリが指す空き<code>Descriptor</code>チェーンの先頭を取り出す。</li>
<li>受信した単一フレームが<code>Descriptor</code>が指す単一のエントリ（<code>desc.addr</code>, <code>desc.len</code>）に格納できるかこの時点では未知であるため、サイズを計算しながらデータを格納していく。</li>
</ol>
<ul>
<li>Tapが受信した単一フレームが<code>Descriptor</code>が指す単一エントリに格納できない場合は、<code>Descriptor</code>の<code>next</code>の値からチェーンをたどっていきながら可能な限り格納していく。</li>
<li>準備できているすべてのチェーンを使用しても格納できない状況も想定される。これについては後述する。</li>
</ul>
<ol start="5">
<li>Rx用Virtqueueの<code>Used Ring</code>の値を更新する。この時、Rxデータを格納したDescriptorのindex情報と、格納した合計データ量の情報を格納する。</li>
<li>irqに紐づく<code>eventfd</code>に対してwriteすることでGuestに対して割り込みをかけ処理をGuestに移譲する。</li>
</ol>
<p>以下の図は、実際にTapから受信したデータを<code>Avaialble Ring</code>を使いながら<code>Descriptor</code>のチェーンに書きこんでいるイメージ図である。</p>
<div align="center">
<img src="./03_figs/net-device_rx_2.svg", width="100%">
</div>
<p>Tapからの受信データを書き終えてたら、<code>Used Ring</code>の更新を行いゲストの仮想デバイス（vNIC）に対して割り込みを入れる。</p>
<div align="center">
<img src="./03_figs/net-device_rx_3.svg", width="100%">
</div>
<p>ゲスト側では、<code>Used Ring</code>のindexを確認し、新しいエントリが指している<code>Descriptor</code>を参照することでRxデータを取得・処理していく。
また、その他必要な処理を行った上で、<code>Availble Ring</code>を更新して改めてデータが受け付けられる状態になったことをホストに通知する。</p>
<h5 id="rx用virtqueueの準備が整っていない状況でtapのトリガが入った場合"><a class="header" href="#rx用virtqueueの準備が整っていない状況でtapのトリガが入った場合">Rx用Virtqueueの準備が整っていない状況でTapのトリガが入った場合</a></h5>
<p>Tapがパケットを受信した際にVirtqueueの準備ができていないケースが想定される。
この場合、Tapからデータを取り出したとしても格納する先の情報を得ることができないため処理を進めることができない。</p>
<p>これに対応するために、Virtqueueの準備が完了するまでTapデバイスの処理を遅らせる機構を備える必要があり、ToyVMMのコード上では<code>deferred_rx</code>というフラグでこれを制御している。</p>
<p>このフラグが立っている場合はToyVMMのRxに関連する処理は以下の戦略に従う</p>
<ul>
<li><code>RX_QUEUE_EVENT</code>が発火した場合、つまりゲストからRx Virtqueueの受信準備が整ったという通知がくると、即座にTapデバイスからデータを取得し処理を進める。もしこれで処理が完了したらフラグを折る。</li>
<li><code>TAP_RX_EVENT</code>が発火した場合、ひとまず処理が進められるかRx Virtqueueの状況を確認しに行く。処理が進められる場合は処理を進め、処理が完了した場合フラグを折る。処理が進められない場合や、Virtqueueに詰められるデータ量が受信データ量に対して小さい場合などはフラグは折らず改めてVirtqueueの受信処理が整うのを待機する。</li>
</ul>
<h5 id="tapによる受信が準備できているrx用virtqueueで収まらない場合"><a class="header" href="#tapによる受信が準備できているrx用virtqueueで収まらない場合">Tapによる受信が準備できているRx用Virtqueueで収まらない場合</a></h5>
<p>もう一つのケースとして、上記でも軽く記載しているが、Tapで受信したデータ量が準備済みのVirtqueueに乗り切らないケースが想定される。
このケースも基本的には戦略は同じであり、<code>deferred_rx</code>のフラグ処理によって次のVirtqueueの準備が整うまで処理を一時中断し、準備が整ったら処理を再開するよう実装している。</p>
<h3 id="virtio-netの動作確認"><a class="header" href="#virtio-netの動作確認">virtio-netの動作確認</a></h3>
<p>実装した<code>Virtio</code>の機構、及びNetwork Deviceによってホストとゲストの間で通信ができるか試してみる。
以下はGuest内部で<code>ip addr</code>コマンドを実行した結果である。<code>eth0</code>が仮想NICとして認識されている。</p>
<pre><code>localhost:~# ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 02:32:22:01:57:33 brd ff:ff:ff:ff:ff:ff
    inet6 fe80::32:22ff:fe01:5733/64 scope link
       valid_lft forever preferred_lft forever
</code></pre>
<p>ホスト側も確認してみる。
ToyVMMではTapデバイスをホスト側に作成しているので、IPアドレス（<code>192.168.0.10/24</code>）を付与する</p>
<pre><code class="language-bash">140: vmtap0: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000
    link/ether c6:69:6d:65:05:cf brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.10/24 brd 192.168.0.255 scope global vmtap0
       valid_lft forever preferred_lft forever
</code></pre>
<p>合わせてGuest側にIPアドレスを付与する。ここではHostと同じサブネットレンジのアドレスを適当に付与している。</p>
<pre><code>localhost:~# ip addr add 192.168.0.11/24 dev eth0
</code></pre>
<p>準備が整ったので、Guestの中からHostのTapインターフェイスのIPに向けてpingを実行してみると以下の通り返答がある。</p>
<pre><code>localhost:~# ping -c 5 192.168.0.10
PING 192.168.0.10 (192.168.0.10): 56 data bytes
64 bytes from 192.168.0.10: seq=0 ttl=64 time=0.394 ms
64 bytes from 192.168.0.10: seq=1 ttl=64 time=0.335 ms
64 bytes from 192.168.0.10: seq=2 ttl=64 time=0.334 ms
64 bytes from 192.168.0.10: seq=3 ttl=64 time=0.321 ms
64 bytes from 192.168.0.10: seq=4 ttl=64 time=0.330 ms

--- 192.168.0.10 ping statistics ---
5 packets transmitted, 5 packets received, 0% packet loss
round-trip min/avg/max = 0.321/0.342/0.394 ms
</code></pre>
<p>逆にHostからGuestの<code>virtio-net</code>インターフェイスのIPに向けてpingを実行してみても同様に返答がある。</p>
<pre><code class="language-bash">[mmichish@mmichish ~]$ ping -c 5 192.168.0.11
PING 192.168.0.11 (192.168.0.11) 56(84) bytes of data.
64 bytes from 192.168.0.11: icmp_seq=1 ttl=64 time=0.410 ms
64 bytes from 192.168.0.11: icmp_seq=2 ttl=64 time=0.366 ms
64 bytes from 192.168.0.11: icmp_seq=3 ttl=64 time=0.385 ms
64 bytes from 192.168.0.11: icmp_seq=4 ttl=64 time=0.356 ms
64 bytes from 192.168.0.11: icmp_seq=5 ttl=64 time=0.376 ms

--- 192.168.0.11 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4114ms
rtt min/avg/max/mdev = 0.356/0.378/0.410/0.028 ms
</code></pre>
<p>ICMPによる簡単な確認だけだけであるが、問題なく通信ができることまで確認できた！</p>
<h3 id="reference-2"><a class="header" href="#reference-2">Reference</a></h3>
<ul>
<li><a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=virtio">OASIS</a></li>
<li><a href="https://syuu1228.github.io/howto_implement_hypervisor/">ハイパーバイザの作り方</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="implement-virtio-blk-device"><a class="header" href="#implement-virtio-blk-device">Implement virtio-blk device</a></h1>
<p>ここではゲストの<code>virtio-blk</code>が利用するBlock deviceの実装を行っていくことにする。
仕様はVirtioと同様OASISの公式で公開されている<a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-2170002">Block Device</a>に記載があるが、本実装はこの仕様に完全に一致している訳ではないので注意されたい。</p>
<p>本節これまでに出てきた概念は特に説明なしに利用するため、これまでの節を読んでいない場合は本節の内容を読み進める前に必ず確認されたい。<br />
更に、本説では<code>virtio-net</code>の実装部分や説明と重複する内容は適宜省略しているため、本節を確認する前に必ず前節の<a href="./03-3_virtio-net.html">Implement virtio-net device</a>を一読してもらいたい。</p>
<h3 id="virtio-blkの仕組み"><a class="header" href="#virtio-blkの仕組み">virtio-blkの仕組み</a></h3>
<p><code>virtio-blk</code>では単一のVirtqueueを利用してゲストからのDISK Read/Writeを表現する。
<code>virtio-net</code>とは異なり外部要因（Tapから受信）が介在せず、あくまでGuestからのI/O要求で駆動するため、Virtqueueの数が最低一つで動作するわけである。
当然こちらもVirtqueueの数は仕様上スケールさせることが認められているが、実装の単純化のために今回は行っていない。</p>
<p>以降では、より詳細な仕組みについて実装ベースで説明していくい。</p>
<h3 id="virtio-blkの実装詳細"><a class="header" href="#virtio-blkの実装詳細">virtio-blkの実装詳細</a></h3>
<p><code>virtio-blk</code>の実装は<code>block.rs</code>に存在している
各種構造体の役割と関係は以下の図のようになっている。</p>
<div align="center">
<img src="./03_figs/blk-device_activated.svg", width="100%">
</div>
<p>これまで述べてきたとおり具体的な実装は各デバイスの実装に依存しているが、それは<code>VirtioDevice</code> Traitによって抽象化されているため、各種デバイスの細部の仕組み以外はすべて<code>virtio-net</code>で示したものと同様に動作する。
そのため、上図もBlock Deviceの内部詳細が少し異なるくらいでそれ以外についてNet Deviceと全く同様になっている。</p>
<p>初期化時の<code>Device Type</code>の問い合わせや、<code>Features</code>の問い合わせなどは<code>Block</code>デバイスの具体的な実装で応答し、<code>Net</code>デバイスと同様にGuestアドレス上のQueueのアドレス位置などが設定・提供され、初期化ステップ完了とともに、<code>activate</code>関数が実行される。
<code>Block</code>デバイスの場合も<code>activate</code>関数の中で<code>Net</code>デバイスと同様に各種<code>file descriptor</code>を<code>epoll</code>で登録し、<code>epoll</code>に対してトリガがかかった際に実行するハンドラ(<code>BlockEpollHandler</code>)のセットアップを実施している。<br />
<code>Block</code>デバイスではI/Oエミュレーションするにあたり、ホスト側のファイル（<code>BlockDevice</code>として操作するもの）をOpenし、それに対してゲストから要求のあったRead/Writeを実施していく形になる。<br />
<code>epoll</code>に登録する<code>file descriptor</code>は<code>Virtqueue</code>用の<code>eventfd</code>、及び予期しない状況に陥った場合に停止させるための<code>eventfd</code>の合計２つである。
<code>Net</code>デバイスのケースと比較して、Tapデバイスがファイルに変わったこと、eventfdの数の変化に伴いハンドラが実行するEVENTの数が変わったこと以外に変化がないのが見て取れるだろう。</p>
<p><code>Block</code>デバイスの場合、単一のVirtqueueに紐づくeventfdの発火のみが動作起点になるため、以降ではこの処理を確認していくこととする。</p>
<h4 id="virtio-blkにおけるioリクエスト"><a class="header" href="#virtio-blkにおけるioリクエスト">virtio-blkにおけるI/Oリクエスト</a></h4>
<p>実装の説明に入る前に<code>virtio-blk</code>におけるI/Oリクエストについて説明する。</p>
<p>すでに述べたとおり、<code>virtio-blk</code>はゲストからのI/O要求は単一のVirtqueueを介してやり取りする。
一方で、ゲストから発生するI/O要求はかなり雑に考えても<code>Read</code>/<code>Write</code>の二種類が想定でき、それぞれのケースで処理するべき内容は大きく異なるはずである。
ホスト側ではこの要求の違いをどのように認識し、実際のI/Oをエミュレートすればいいかという疑問が当然発生することになる。</p>
<p>これを説明するためには、<code>virtio-blk</code>によってどのように<code>Descriptor Table</code>が利用されるかを理解する必要がある。
まず、ゲスト側のドライバがVirtqueueに詰めるデータは以下の構造のものになる。</p>
<blockquote>
<pre><code>struct virtio_blk_req { 
        le32 type; 
        le32 reserved; 
        le64 sector; 
        u8 data[]; 
        u8 status; 
};
</code></pre>
<p>Source: <a href="https://docs.oasis-open.org/virtio/virtio/v1.2/cs01/virtio-v1.2-cs01.html#x1-2850006">Block Device: Device Operaiton</a></p>
</blockquote>
<p>実際はこれが<code>Descriptor Table</code>に以下ような3つのエントリとして作成され、それぞれのエントリが<code>next</code>によってチェーンを構成する形になっている。</p>
<p>一つ目の<code>Descriptor</code>エントリは、<code>type</code>、<code>reserved</code>、<code>sector</code>の3つのデータを格納しているアドレスを指し、二つ目の<code>Descriptor</code>エントリは、dataが書かれている領域の先頭アドレスを、三つ目の<code>Descriptor</code>エントリは、<code>status</code>が書かれている領域の先頭アドレスを指すような構造になる。</p>
<div align="center">
<img src="./03_figs/virtio-blk_virtqueue.svg", width="50%">
</div>
<p>特に<code>type</code>にはI/Oの種類（<code>read</code>や<code>write</code>、それ以外のI/O要求）を示しており、ここの値を確認することでホスト側は振る舞いを変更する必要がある。</p>
<p><code>read</code>の場合、２つ目の<code>Descriptor</code>が指すエントリはホスト側が実際のDiskから読み込んだデータを格納すべきアドレス領域として利用できる。
<code>sector</code>の値から読み取るべきセクタ位置を特定し、そこから必要な量のデータ（２つ目の<code>Descriptor</code>の<code>desc.len</code>の値）を読み込む。</p>
<p><code>write</code>の場合、2つ目の<code>Descriptor</code>が指すエントリにはDiskに書き込むべきデータが格納されているため、データを読み出した上で<code>sector</code>の値で指定されているDiskのセクタ位置に書き込みを行う。</p>
<p>３つ目の<code>Descriptor</code>エントリには正常にI/Oのエミュレーションが完了したか、もしくは失敗したかなどを表現するステータス情報を書き込む。</p>
<p>上記のように、Disk I/Oの種別と、そのI/Oに対して必要なデータやバッファがVirtqueueを介して提供されるため、ホスト側はこれを仕様に従って解釈し、適切にI/Oをエミュレートする責務を追うことになる。</p>
<h4 id="toyvmmによるdisk-ioの実装"><a class="header" href="#toyvmmによるdisk-ioの実装">ToyVMMによるDisk I/Oの実装</a></h4>
<p>ゲストから発生したDisk I/Oリクエストについて実装を見ながら詳細を説明していく。
ホスト側の処理以外は基本<code>Net</code> Deviceの<code>Tx</code>のケースと挙動が同じになるので<code>QueueNotify</code>でホスト側に処理が移譲されたところから説明していく。</p>
<p>MMIOの<code>QueueNotify</code>への書き込みはによって発火したEventFdは<code>epoll</code>の監視によって拾い上げられ<code>BlockEpollHandle</code>のハンドラ処理、具体的には<code>QUEUE_AVAIL_EVENT</code>に対応する処理が実行される。
実際は<code>process_queue</code>関数が呼び出され、この返り値が<code>true</code>である場合には<code>signal_used_queue</code>関数が呼ばれる。
後者の<code>signal_used_queue</code>はゲストに対して割り込みを入れているだけなので詳細に確認すべき処理は前者の<code>process_queue</code>関数である。</p>
<p><code>process_queue</code>では以下のような形で処理が進んでいく。ぜひコードを確認しながら以降の解説を見てほしい。</p>
<ol>
<li>必要な変数の初期化</li>
</ol>
<ul>
<li><code>used_desc_heads[(u16, u32), 256]</code> : 処理済みの<code>Descriptor</code>のindexとデータ長を格納するデータ。<code>process_queue</code>の処理の最後にこの値を元に<code>used_ring</code>へ値をセットする。</li>
<li><code>used_count</code> : GuestからのI/O要求をどこまで処理したかを格納しておくカウンタ</li>
</ul>
<ol start="2">
<li>Virtqueueをiterationして、停止するまでX~Yの処理を繰り返す</li>
<li><code>Available Ring</code>が指す<code>Descriptor</code>を取り出し、<code>virtio-blk</code>の仕様に従ってパースし<code>Request</code>構造体を作成する。</li>
</ol>
<ul>
<li><code>Request</code>構造体にはパースした結果（リクエスト種別、セクタ情報、データアドレス、データ長、ステータスアドレス）が格納されている</li>
</ul>
<ol start="4">
<li><code>execute</code>関数を呼び出し、<code>Request</code>構造体の内容から実施すべきI/Oリクエストを実施する。</li>
</ol>
<ul>
<li>I/Oに成功した際、Readの場合は読み込んだデータ長を返却し、writeなどそれ以外は0を返却する。この値は<code>used_ring</code>に書き込む値として利用する。</li>
</ul>
<ol start="5">
<li>I/Oに成功したか失敗したかをstatusアドレスに書き込み、<code>used_ring</code>に必要な情報を書き込む</li>
<li>上記の処理で一つ以上のリクエストを処理した場合は関数の戻り値として<code>true</code>を返却する。</li>
</ol>
<p>以下の図はGuestからのI/O要求がReadだった場合の処理の図解である。</p>
<div align="center">
<img src="./03_figs/blk-device_read.svg", width="100%">
</div>
<p>また、以下の図がGuestからのI/O要求がWriteだった場合の処理の図解である。</p>
<div align="center">
<img src="./03_figs/blk-device_write.svg", width="100%">
</div>
<h3 id="virtio-blkの動作確認"><a class="header" href="#virtio-blkの動作確認">virtio-blkの動作確認</a></h3>
<p>ここでは実際の動作確認として、もはやinitrd.imgを使わず、Firecracker同様Ubuntuのrootfsイメージを利用して、UbuntuのOSを起動するようにしてみよう。<br />
<code>virtio-blk</code>向けBlockDeviceの実装できたことにより、UbuntuのrootfsイメージをVMの<code>/dev/vda</code>として認識させることができるようになったため、VMのカーネルのcmdlineの値に<code>root=/dev/vda</code>を指定すればこのUbuntuイメージからOSを起動することができるはずである。</p>
<pre><code># Run ToyVMM with kernel and rootfs (no initrd.img)
$ sudo -E cargo run -- boot_kernel -k vmlinux.bin -r ubuntu-18.04.ext4                                                       [22:46:37]
...
warning: `toyvmm` (bin &quot;toyvmm&quot;) generated 4 warnings
    Finished dev [unoptimized + debuginfo] target(s) in 0.02s
     Running `target/debug/toyvmm boot_kernel -k vmlinux.bin -r ubuntu-18.04.ext4`
[    0.000000] Linux version 4.14.174 (@57edebb99db7) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) #2 SMP Wed Jul 14 11:47:24 UTC 2021
[    0.000000] Command line: console=ttyS0 reboot=k panic=1  root=/dev/vda virtio_mmio.device=4K@0xd0000000:5 virtio_mmio.device=4K@0xd0001000:6
...

# Instead of Alpine rootfs (initrd.img), Ubuntu rootfs is used and startup.
Welcome to Ubuntu 18.04.1 LTS!

...

Ubuntu 18.04.1 LTS 7e47bb8f2f0a ttyS0

# Please type root/root and login!

7e47bb8f2f0a login: root
Password:
Last login: Mon Aug 14 13:28:29 UTC 2023 on ttyS0
Welcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.14.174 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.

# You can verify that the launched VM is ubuntu-based.
root@7e47bb8f2f0a:~# uname -r
4.14.174
root@7e47bb8f2f0a:~# cat /etc/os-release
NAME=&quot;Ubuntu&quot;
VERSION=&quot;18.04.1 LTS (Bionic Beaver)&quot;
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=&quot;Ubuntu 18.04.1 LTS&quot;
VERSION_ID=&quot;18.04&quot;
HOME_URL=&quot;https://www.ubuntu.com/&quot;
SUPPORT_URL=&quot;https://help.ubuntu.com/&quot;
BUG_REPORT_URL=&quot;https://bugs.launchpad.net/ubuntu/&quot;
PRIVACY_POLICY_URL=&quot;https://www.ubuntu.com/legal/terms-and-policies/privacy-policy&quot;
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic

# And you can also find that this VM mount /dev/vda as rootfs.

root@7e47bb8f2f0a:~# lsblk
NAME MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vda  254:0    0  384M  0 disk /


root@7e47bb8f2f0a:~# ls -lat /
total 36
drwxr-xr-x 12 root root   360 Aug 14 13:47 run
drwxr-xr-x 11 root root  2460 Aug 14 13:46 dev
dr-xr-xr-x 12 root root     0 Aug 14 13:46 sys
drwxrwxrwt  7 root root  1024 Aug 14 13:46 tmp
dr-xr-xr-x 57 root root     0 Aug 14 13:46 proc
drwxr-xr-x  2 root root  3072 Jul 20  2021 sbin
drwxr-xr-x  2 root root  1024 Dec 16  2020 home
drwxr-xr-x 48 root root  4096 Dec 16  2020 etc
drwxr-xr-x  2 root root  1024 Dec 16  2020 lib64
drwxr-xr-x  2 root root  5120 May 28  2020 bin
drwxr-xr-x 20 root root  1024 May 13  2020 .
drwxr-xr-x 20 root root  1024 May 13  2020 ..
drwxr-xr-x  2 root root  1024 May 13  2020 mnt
drwx------  4 root root  1024 Apr  7  2020 root
drwxr-xr-x  2 root root  1024 Apr  3  2019 srv
drwxr-xr-x  6 root root  1024 Apr  3  2019 var
drwxr-xr-x 10 root root  1024 Apr  3  2019 usr
drwxr-xr-x  9 root root  1024 Apr  3  2019 lib
drwx------  2 root root 12288 Apr  3  2019 lost+found
drwxr-xr-x  2 root root  1024 Aug 21  2018 opt
</code></pre>
<p>上記の通り、VMは/dev/vdaとして渡したUbuntu OSを起動し、ログイン後にUbuntuベースのOSになっていることや意図通りrootfsをマウントしていることがわかる。<br />
さらに、これまでのinitrd.imgではRAM上にrootfsが展開されていたので揮発性があったが、今回はDISKとして永続化されているrootfsをベースに起動しているため、一度VM内部で作成したファイルは再度VMを起動した際も確認することができる。</p>
<pre><code># Create sample file (hello.txt) in first VM boot and reboot.

root@7e47bb8f2f0a:~# echo &quot;HELLO UBUNTU&quot; &gt; ./hello.txt
root@7e47bb8f2f0a:~# cat hello.txt
HELLO UBUNTU
root@7e47bb8f2f0a:~# reboot -f
Rebooting.

# After second boot, you can also find 'hello.txt'.  

Ubuntu 18.04.1 LTS 7e47bb8f2f0a ttyS0

7e47bb8f2f0a login: root
Password:
Last login: Mon Aug 14 13:57:27 UTC 2023 on ttyS0
Welcome to Ubuntu 18.04.1 LTS (GNU/Linux 4.14.174 x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage

This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
root@7e47bb8f2f0a:~# cat hello.txt
HELLO UBUNTU
</code></pre>
<p><code>virtio-net</code>/<code>virtio-blk</code>両方のDeviceの実装を終え、かなりシンプルではあるが必要な機能を最小限有したVMを作成するに至ったと言えよう</p>
<h3 id="reference-3"><a class="header" href="#reference-3">Reference</a></h3>
<ul>
<li><a href="https://www.oasis-open.org/committees/tc_home.php?wg_abbrev=virtio">OASIS</a></li>
<li><a href="https://syuu1228.github.io/howto_implement_hypervisor/">ハイパーバイザの作り方</a></li>
<li><a href="https://blog.bobuhiro11.net/2022/04-12-gokvm6.html">自作VMMのvirtio-blk対応</a></li>
<li><a href="https://wiki.osdev.org/Virtio#Block_Device_Packets">OSDev.org - Virtio#Block_Device_Packets</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="smp-and-cpu-specific-configuration"><a class="header" href="#smp-and-cpu-specific-configuration">SMP and CPU specific configuration</a></h1>
<p>このセクションではVMMのSMP（Symmetric MultiProcessing）対応について主題として議論する
副題としてIntel/AMD特有の設定についても触れることとする</p>
<p>これまで仮想マシンに割り当てることのできるvCPUの数は1つだった。
SMPの対応を行うことで、仮想マシンに複数のCPUを割り当てて、複数プロセッサでの並行処理に対応させたい。
この詳細については <a href="./04-1_smp_symmetric_multiprocessing.html">04-1. SMP - Symmetric MultuProcessing</a>に詳細を記載することとする。</p>
<p>また、SMPの実装に合わせてコードを変更するついでに、ベンダーの異なる各CPU（Intel・AMD）に対して独自の設定ができるような実装にリファインする。
共通の設定については <a href="./04-2_common_CPU_configuration.html">04-2. Common CPU configuration</a>、Intel CPUに関する設定については <a href="./04-3_Intel_CPU_specific_configuration.html">04-3. Intel CPU specific configuration</a>、AMD CPUに関する設定については <a href="./04-4_AMD_CPU_specific_configuration.html">04-4. AMD CPU specific configuration</a>で議論することとするが、これらのCPU設定については必要に応じて設定をする類のものが多いこともあり、ベストエフォートで記載を進めることさせてもらう。
そのため、およびについては不定期に更新が入るものと読者には想定してもらいたい。</p>
<p>改めて、本セクションの各トピックは次のようになっている</p>
<ul>
<li><a href="./04-1_smp_symmetric_multiprocessing.html">04-1. SMP - Symmetric MultuProcessing</a></li>
<li><a href="./04-2_common_CPU_configuration.html">04-2. Common CPU configuration</a></li>
<li><a href="./04-3_Intel_CPU_specific_configuration.html">04-3. Intel CPU specific configuration</a></li>
<li><a href="./04-4_AMD_CPU_specific_configuration.html">04-4. AMD CPU specific configuration</a></li>
</ul>
<p>本資料では、上述したとおり不定期に更新が入る想定にしているため、ベースとしているコミットナンバーについては各トピックの話題毎にそれぞれ記載することとする。</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="smp---symmetric-multuprocessing"><a class="header" href="#smp---symmetric-multuprocessing">SMP - Symmetric MultuProcessing</a></h1>
<p>本資料は全て以下のコミットナンバーをベースとしている</p>
<ul>
<li>ToyVMM : 2795702addc5fe3d7d42f13a82c5487985237fc8</li>
<li>Firecracker : 0c3c9fc6a5a7d0815a310c5d2e7624a0fe2118ac</li>
</ul>
<p>また、議論のはじめにSMP実装の導入にあたって参考にさせてもらった素晴らしい資料について紹介しておく。
Nobuhiro Miki氏による<a href="https://blog.bobuhiro11.net/2021/11-25-gokvm3.html">KVMを使った自作VMMのSMP対応</a>は本実装を行うにあたりFirecrackerのソースコードと合わせて大変参考にさせていただいた資料である。
Nobuhiro Miki氏はGolangによるVMM実装である<a href="https://github.com/bobuhiro11/gokvm">gokvm</a>の作者でもあるため、もしGolangでVMMを実装することに興味がある方は確認してみると良いだろう。
本資料でも上記の資料の内容は一部参照させていただく。</p>
<p>本資料ではまず、SMPの話に入る前の基礎知識として、割り込み処理、割り込みコントローラの基礎について簡単に復習する。
その後にSMPについて詳細に議論していく。</p>
<h2 id="割り込みinterruptの復習"><a class="header" href="#割り込みinterruptの復習">割り込み（Interrupt）の復習</a></h2>
<h3 id="割り込みとは"><a class="header" href="#割り込みとは">割り込みとは</a></h3>
<p>割り込みとは、CPUが処理をしているプログラムの実行に、その名の通り「割り込んで」別のプログラムを実行させる仕組みである。
CPUはこの割り込みを受け取ると、実行しているプログラムを一旦停止し、その割り込みに対応する処理を実行する。この処理は基本的に事前定義されており、これを割り込みハンドラと呼ぶ。CPUは割り込みハンドラの処理を終了すると、停止していたプログラムを再開する。</p>
<p>割り込みには「ソフトウェア割り込み」と「ハードウェア割り込み」が存在するが、ここでは特に後者に着目する。基本的に以降「割り込み」と記載した場合は「ハードウェア割り込み」を指すものと思ってほしい。</p>
<p>割り込みについては検索すると多くの資料がでてくるため、もし読者が割り込みについて基礎知識を習得していない場合は自身で調べてみてほしい。</p>
<h3 id="割り込みコントローラ"><a class="header" href="#割り込みコントローラ">割り込みコントローラ</a></h3>
<p>CPUの割り込み端子の数はCPUの種類、実装によって異なるものの、少なくとも一つ、もしくは複数用意される。
一方で周辺機器は基本的に複数存在するため、割り込み要求もそれらのデバイス数に合わせて増えていくことになる。</p>
<p>例えば実装として、一つの割り込みハンドラのみを用意し、全ての周辺機器からの割り込みについてそのハンドラで処理をするという形式にすると、ハンドラの実装の中でどの周辺機器からの割り込み要求かを判別する処理が必要になる。</p>
<p>PC/AT互換機などでは基本的に、ハードウェアとして複数のIRQ端子を用意し、割り込み要因毎に異なるハンドラに処理を移すことができるような形をとっており、この機能を持った回路のことを割り込みコントローラと呼ぶ。</p>
<h4 id="pic-programmable-interrupt-controller"><a class="header" href="#pic-programmable-interrupt-controller">PIC (Programmable Interrupt Controller)</a></h4>
<p>割り込みコントローラの代表的な例として、ここでは<a href="https://ja.wikipedia.org/wiki/Intel_8259">Intel 8259</a>を題材として取り上げる。
Intel 8259はProgrammable Interrupt  Controller（PIC）の一種であり、マルチプレクサの機能、CPUに割り込みをかけるために、複数の周辺機器からの割り込み処理を一つの割り込み出力に束ねる機能を持っている。この割り込みコントローラはIntel 8080時代から使用されているコントローラである。</p>
<p>Intel 8259は単体で8つのデバイスからの割り込み要求入力（IRQ0~7）に加え、一本の割り込み出力（INTR）、割り込みACK（INTA）、割り込みレベルや割り込みベクタオフセット用の通信ライン（D0-D7）を備えている。
更に、8259には3つのレジスタがあり、、、と霧がなくなるため、ひとまずはこのあたりで話を止めておく。
8259は大変有名なPICであり、調べるとたくさんの資料が出てくるので、詳細は各自調べてほしい。</p>
<p>以下の図は、よく取られるPICを2つカスケード接続してCPUに接続する構成である。
この構成では最大15個のデバイスから割り込みを受け付けることができる。</p>
<blockquote>
<img src="./04_figs/PIC.jpg">
<p>Reference: <a href="https://www.valinux.co.jp/technologylibrary/document/linux/interrupts0001/">Linux/x86_64の割り込み処理 第一回 割り込みコントローラ</a></p>
</blockquote>
<p>上記の図はあくまで単一CPUに対しての構成例であった。
さて、ではこれをベースにマルチプロセッサシステムを考えた場合に割り込みはどう考えればよいだろうか？</p>
<p>対処が必要な点としては、CPUが複数になる一方で周辺機器の数はかわらないということで、CPUが単一の場合は全ての割り込みをそのPCUに渡せば良いだけだったため話が単純だったが、複数CPUになると「どの割り込みをどのCPUで処理させるのか？」という新たな問題が浮上してくることになり、これに対応できるような仕組みが必要になってきた。
これが次に紹介し、また本資料の主題でもあるSMPとも深く関係のあるAPICである。</p>
<h4 id="apic-advanced-programmable-interrupt-controller"><a class="header" href="#apic-advanced-programmable-interrupt-controller">APIC (Advanced Programmable Interrupt Controller)</a></h4>
<p>APICはIntelが開発したx86アーキテクチャにおける割り込みコントローラであり、マルチプロセッサシステムに対応するものになっている。</p>
<p>まず特徴的なのはAPICに２つ種類があることである。
単一のCPUにそれぞれ存在しているLocal APIC（LAPIC）とデバイスの割り込みを受け付けるI/O APICである。
LAPICとI/O APICはAPIC Busと呼ばれる線でつながっており、APIC間で割り込みの送受信を行えるようになっている。
PICでは単一CPUに対してデバイスの割り込みラインが直接つながっていたことがマルチプロセッサ化対応の上での問題だったため、PICの機能を分割しI/Oを受けるPICとCPUに対して割り込みをいれるPICを分けたような形、と考えると割と順当な変化と言えよう。</p>
<blockquote>
<img src="./04_figs/APIC.jpg">
<p>Reference: <a href="https://www.valinux.co.jp/technologylibrary/document/linux/interrupts0001/">Linux/x86_64の割り込み処理 第一回 割り込みコントローラ</a></p>
</blockquote>
<p>LAPICは、自身のCPUに対して届いた割り込みを受信し、自身のCPUに対して実際に割り込みをかける役割、及び他CPUに割り込みを送出したりする（IPI: InterProcessor Interrupt）役割を持っている。
I/O APICは当然ながらデバイスからの割り込みを受けて特定のLAPICにベクタなどを指定して送出する役割を持っている。I/O APICは増設することも可能になっている。</p>
<p>さて、ここまで簡単に割り込みの復習をしてきたが、つまるところマルチプロセッサ構成の場合はこのLAPIC・I/O APICなどの構成に関する理解が重要になってくるわけである。</p>
<h2 id="smp-symmetric-multiprocessing"><a class="header" href="#smp-symmetric-multiprocessing">SMP (Symmetric MultiProcessing)</a></h2>
<p>本題に入って、ToyVMMのマルチプロセッサ対応について話していく。</p>
<p>まずはマルチプロセッサ対応の話をする上で避けることのできないAPICに関わる話題として割り込みモードの話をし、その後に実際のマルチプロセッサ対応の根幹部分であるMP Configurationの話に移って行くことにする。</p>
<h3 id="intel-multiprocessor-specification-v14"><a class="header" href="#intel-multiprocessor-specification-v14">Intel MultiProcessor Specification v1.4</a></h3>
<p>今回の資料はマルチプロセッサシステムの仕様書である<a href="https://pdos.csail.mit.edu/6.828/2008/readings/ia32/MPspec.pdf">Intel MultiProcessor Specification v1.4</a>に従った内容になっている。
以降の内容は上記の資料から必要最低限の情報をピックアップしたような形になっているため、より詳細を知りたい場合はこのSpecを参照されたい。</p>
<h3 id="pic-mode-virtual-wire-mode-and-symmetric-io-mode"><a class="header" href="#pic-mode-virtual-wire-mode-and-symmetric-io-mode">PIC Mode, Virtual Wire Mode and Symmetric I/O Mode</a></h3>
<p>MP specificationではAPICに対して以下の割り込みモードを定義している</p>
<ul>
<li>PIC Mode : 全てのAPICコンポーネントをバイパスし、システムをシングルプロセッサモードで動作させる。</li>
<li>Virtual Wire Mode : APICをVirtual Wireとして利用するが、それ以外はPIC Modeと同様に動作させる。</li>
<li>Symmetric I/O Mode : 複数プロセッサによるシステム動作を有効にする。</li>
</ul>
<p>PIC ModeとVirtual Wire ModeはPC/AT 互換性を保つための割り込みモードであり、これらのいずれかについてシステムは実装している必要がある。
これらのモードでは、APICを標準のPICである8259A相当のPICと組み合わせて使用することにより、uniprocessor PC/ATと完全なDOS互換性が提供される。</p>
<p>一方、Symmetric I/O ModeはPIC Mode、もしくはVirtual Wire Modeに追加で実装されるものである。MP Operating Systemはいずれかのモード(PIC Mode or Virtual Wire Mode)で起動し、その後マルチプロセッサモードに移行するとSymmetric I/O Modeに切り替わる。</p>
<h4 id="pic-mode"><a class="header" href="#pic-mode">PIC Mode</a></h4>
<p>PIC ModeではBSP(Bootstrap Processor）に到達する割り込み信号がMaster PIC（図中にある8259A-EQUIVALENT PICS）から来るか、Local APICからくるかを制御するIMCR（Interrupt Mode Configuuration Register）が存在しており、必要に応じて適切にAPICコンポーネントをバイパスできるような仕組みになっている。
図中の点線のラインをみると、Master PICからの割り込みラインがBSPに繋がるような構成になっており、これをIMCRでうまく制御するため、実質的に旧来の割り込み形式と同じ構成で動かすことができるわけである。
従って、PC/ATと同じハードウェア割り込みコンフィギュレーションを採用でき、互換性があるような形を保てている。</p>
<blockquote>
<img src="./04_figs/PIC_Mode.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 3-2. PIC Mode</p>
</blockquote>
<p>Symmetric I/O Modeに遷移する前に、BIOSもしくはOSがIMCRの設定を変更しPIC Modeから切り替える必要がある。
今回はどちらかというと次に説明するVirtual Wire Modeの話が重要なので、PIC Modeにおける具体的なIMCRへの書き込み等に関する話についてはここでは省略する。気になる場合はSpecを参照してもらいたい。</p>
<h4 id="virtual-wire-mode"><a class="header" href="#virtual-wire-mode">Virtual Wire Mode</a></h4>
<p>Virtual Wire ModeはPIC Modeとは異なり、下図のようにPICの割り込みラインがBSPのLocal APICのLINTIN0（Local Interrupt 0）を介してBSPへと配送される。
おそらく、直接的な配線ではなくLocal APICを通した仮想的な配線がなされるように設定をすることから、&quot;Virtual Wire&quot; Modeという名前がついているのだろうと思われる。</p>
<p>Virtual Wire Modeを実現するために、Local APICの設定をする必要がある。
すなわち、LINTIN0のピンはExtINT（External Interrupt）としてプログラムされ、8259-EQUIVALENT PICが外部割り込みコントローラとして機能することを指定する必要がある。
この場合は図からもわかるとおり、Virtual Wire Modeにおける割り込みは8259-EQUIVALENT PICからBSPに創出されるため、このMode中にはI/O APICは利用されない。</p>
<blockquote>
<img src="./04_figs/Virtual_Wire_Mode_LocalAPIC.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 3-3. Virtual Wire Mode via Local APIC</p>
</blockquote>
<p>一方で、下図に示すようにI/O APICをプログラムしてVirtual Wire Modeを実現する方式も存在するようである。
この場合、割り込み信号はI/O APICとLocal APICの両方を通過してBSPに配送されるような形になる。</p>
<blockquote>
<img src="./04_figs/Virtual_Wire_Mode_IOAPIC.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 3-3. Virtual Wire Mode via I/O APIC</p>
</blockquote>
<h4 id="symmetric-io-mode"><a class="header" href="#symmetric-io-mode">Symmetric I/O Mode</a></h4>
<p>複数プロセッサでの動作を有効にするために、PIC ModeもしくはVirtual Wire Modeを経て、Symmetric I/O Modeに遷移する。
当然ながらこのモードでは少なくとも一つ以上のI/O APICが必要になり、I/O割り込みはI/O APICによって生成される。8259割り込みラインはマスクされるか、混合モードでI/O APICと一緒に動作することになる。</p>
<blockquote>
<img src="./04_figs/Symmetric_IO_Mode.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 3-5. Symmetric I/O Mode</p>
</blockquote>
<p>APIC I/Oユニットはgeneral-purpose interrupt inputを持っており、異なる動作モードで個別にプログラムできるようになっている。
I/O APIC interrupt lineの割り当てはシステムの実装に固有になっており、当然カスタムも可能である。
デフォルトの実装についてはSpecの5章、カスタムの方法についてはSpecの4章を参考にされたい。ここでは一旦省略とするが後ほどのMP Configurationの話のなかで、カスタムする話が一部出てくることになる。</p>
<h4 id="virtual-wire-mode-setup-in-toyvmm"><a class="header" href="#virtual-wire-mode-setup-in-toyvmm">Virtual Wire Mode setup in ToyVMM.</a></h4>
<p>これまでの話をToyVMMの実装に絡めて考えてみる。</p>
<p>基本的に、PIC ModeやVirtual Wire Mode周りについてはBIOSが処理を行う領域であるため、ToyVMMとしてはこれらを適切に実装する必要がある。
SpecのAppendix. Aにはありがたいことに「System BIOS Programming Guidelines」という記載が存在しており、これを一部参考にさせてもらった。
ここではその内容のうち「A.3 Programming the APIC for Virtual Wire Mode」を参考に、ToyVMMに対してVirtual Wire Modeで割り込みモードを動作するように設定を入れていくことにした。</p>
<p>Virtual Wire Modeではまず、BSPのLocal APICの設定を行って&quot;virtual wire&quot;として動作させ、8259A-EQUIVALENT PICからのCPU割り込みをBSPまでバイパスするように設定する必要がある。</p>
<p>これには、以下の設定を実施する必要があるらしい。</p>
<ul>
<li>BSPのLocal APICのLINT0をEdge-Triggered ExitINT delivery modeに設定</li>
<li>BSPのLocal APICのLINT1をLevel-Triggered NMI delivery modeに設定</li>
</ul>
<p>各Local APICのLINTはどこに存在していて、どのように書き込みなどを実行すればよいかといった詳細については、Specの「Example A-1. Programming Local APIC for Virtual Wire Mode」にも記載があるが、より詳細の理解を伴うために、「<a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html">Intel 64 and IA-32 Architectures Software Developer's Manual Combined Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D, and 4</a>」も参照した。</p>
<p>このIntel SDMの資料の「11.4 LOCAL APIC」と「11.5 HANDLING LOCAL INTERRUPTS」にLocal APIC、及びLocal Interruptに関するより詳細な仕様の記載がある。
流石に上記の資料は斜め読みしかしていないが、必要な情報としては概ね以下の２つ図にまとまっていた。</p>
<blockquote>
<img src="./04_figs/Local_APIC_Register_Address_Map.png">
<p>Reference: Intel SDM - Table 11-1. Local APIC Register Address Map</p>
<img src="./04_figs/Local_Vector_Table.png">
<p>Reference: Intel SDM - Local Vector Table (LVT)</p>
</blockquote>
<p>Table 11-1を見てわかるとおり、Local APICのLINT0は0xFEE00350、LINT1は0xFEE00360に配置されているため、ここをベースとしてderivery modeの変更を加えることになる。
更に、Figure 11-8を見るとそれぞれのLINTアドレスをベースに8bitオフセットした位置から3bitでDelivery Modeが表現されることがわかる。NMIを設定する場合は0b100、ExtINTを設定する場合は0b111を書き込むと良さそうだ。</p>
<p>また、Edge/Level Triggerを設定する場合は、それぞれのLINTのアドレスの15bit目を変更することで制御できるようだ。
しかし、どうもBIOSのサンプルコードもFirecrackerのコードもこの部分に関しては特にいじっていないように見える。
今回は一旦Firecrackerを参考にLINT0, LINT1のDelivery Modeを変更するのみとした。</p>
<p>ToyVMMでの実装を以下に示す。これはFirecrackerの実装をそのまま引用させてもらっている。
以下のコードでは、LINT0、LINT1のアドレスオフセットを0x350、0x360として処理しているように見えるが、これは<code>vcpu.get_lapic</code>で取得してきた値（これは0xFEE00000が開始アドレスで、そこからLocal APIC分だけ読み出している値）に対して処理をしているからである。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>// Defines poached from apicdef.h kernel header
const APIC_LVT0: usize = 0x350;
const APIC_LVT1: usize = 0x360;
const APIC_MODE_NMI: u32 = 0x4; // 0x4 = 0b100
const APIC_MODE_EXTINT: u32 = 0x7; // 0x7 = 0b111

fn get_klapic_reg(klapic: &amp;kvm_lapic_state, reg_offset: usize) -&gt; u32 {
    let range = reg_offset..reg_offset + 4;
    let reg = klapic.regs.get(range).expect(&quot;get_klapic_reg range&quot;);
    byte_order::read_le_i32(reg) as u32
}

fn set_klapic_reg(klapic: &amp;mut kvm_lapic_state, reg_offset: usize, value: u32) {
    let range = reg_offset..reg_offset + 4;
    let reg = klapic.regs.get_mut(range).expect(&quot;set_klapic_reg range&quot;);
    byte_order::write_le_i32(reg, value as i32)
}

fn set_apic_delivery_mode(reg: u32, mode: u32) -&gt; u32 {
    // !0x700 = 0b1000_1111_1111
    // (AND)    0bRRRR_RRRR_RRRR (reg)
    // --------------------------
    //          0bR000_RRRR_RRRR
    // (OR)     0b0MMM_0000_0000 (mode)
    // --------------------------
    //        = 0bRMMM_RRRR_RRRR (Write mode bits to 8~11 bit)
    ((reg) &amp; !0x700) | ((mode) &lt;&lt; 8)
}

// Configure LAPICs. LAPIC0 set for external interrupts. LAPIC1 is set for NMI
pub fn set_lint(vcpu: &amp;VcpuFd) -&gt; Result&lt;(), InterruptError&gt; {
    let mut klapic = vcpu.get_lapic().map_err(InterruptError::GetLapic)?;
    let lvt_lint0 = get_klapic_reg(&amp;klapic, APIC_LVT0);
    set_klapic_reg(
        &amp;mut klapic,
        APIC_LVT0,
        set_apic_delivery_mode(lvt_lint0, APIC_MODE_EXTINT),
    );
    let lvt_lint1 = get_klapic_reg(&amp;klapic, APIC_LVT1);
    set_klapic_reg(
        &amp;mut klapic,
        APIC_LVT1,
        set_apic_delivery_mode(lvt_lint1, APIC_MODE_NMI),
    );
    vcpu.set_lapic(&amp;klapic).map_err(InterruptError::SetLapic)
}
<span class="boring">}
</span></code></pre></pre>
<p>ここまでの議論でBSPのLAPICの設定としてVirtual Wire Modeの設定を実施する話をした。
以降では、SMPの根幹であるMP Configuration Tableについての議論を行う。</p>
<h4 id="mp-floating-pointer-table"><a class="header" href="#mp-floating-pointer-table">MP Floating Pointer Table</a></h4>
<p>ここからは具体的にMP Configurationの話に移っていく。</p>
<p>まず第一に、MP準拠システムでは「MP Floating Pointer Structure」の実装が必要になる。
システムがMPの仕様に準拠しているかどうかを判断するために、OSはこのMP Floating Pointer Structure構造体を以下の順序で検索する</p>
<ol>
<li>拡張BIOSデータ領域（EBDA）の最初の1キロバイト</li>
<li>EBDAセグメントが未定義の場合、システム・ベース・メモリの最後の1キロバイト内</li>
<li>In the BIOS ROM address space between 0F0000h and 0FFFFFh.</li>
</ol>
<p>この構造が存在する場合、OSはそのシステムをはMP準拠のシステムであると解釈する。
この構造体は後述する「MP Configuration Table」への物理アドレスのポインタを格納していることを期待する。
MP Configuration Tableへのポインタがない場合はシステムのデフォルト設定が利用されるような挙動になるらしいが、その話はここでは省略する。</p>
<blockquote>
<img src="./04_figs/MP_Floating_Pointer_Structure.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-2. MP Floating Pointer Structure</p>
</blockquote>
<p>以下に、MP Floating Pointer Structureの一部のフィールドについて記載する。
全てのフィールドについての詳細はSpecを参照されたい。</p>
<table><thead><tr><th>Field</th><th>Offset<br>(in bytes:bits)</th><th>Length<br>(in bits)</th><th>Description</th></tr></thead><tbody>
<tr><td>Signature</td><td>0</td><td>32</td><td>検索キーとなる <code>_MP_</code> というASCII文字列</td></tr>
<tr><td>Physical Address Pointer</td><td>4</td><td>32</td><td>MP Configuration Tableの先頭アドレス</td></tr>
<tr><td>Length</td><td>8</td><td>8</td><td>Paragraph(16byte)単位の本テーブル長</td></tr>
<tr><td>Spec Rev</td><td>9</td><td>8</td><td>サポートされるMP Specのバージョン</td></tr>
<tr><td>Checksum</td><td>10</td><td>8</td><td>完全なポインタ構造体のチェックサム</td></tr>
</tbody></table>
<p>ToyVMMでの実装の観点から考えると、ゲストアドレスのEBDAの領域にこの構造を構築すれば良いことになる。
特にLinux Kernelでは、MP Floating Pointer Tableは<code>mpf_intel</code>という名称の構造体で定義されているため、これを利用してゲストOSのメモリにMP Floating Pointer Tableをセットアップすれば良い。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    let mut mpf_intel = mpspec::mpf_intel {
        signature: SMP_MAGIC_IDENT, // _MP_
        physptr: (base_mp.raw_value() + size) as u32, // phys addr for MP Configuration Table
        length: 1,
        specification: 4,
        ..mpspec::mpf_intel::default()
    };
    mpf_intel.checksum = mpf_intel_compute_checksum(&amp;mpf_intel);
    // write mpf_intel to guest memory
    ...
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="mp-configuration-table"><a class="header" href="#mp-configuration-table">MP Configuration Table</a></h4>
<p>MP Configuration Tableは上述した、MP Floating Pointer TableのPhysical Address Pointerの値の先に格納されているデータ構造である。
MP Configuration TableをここではHeaderとEntriesの2種類に分けて議論する。</p>
<p>MP Configuration Table Headerはベースアドレスの最初に構築されるヘッダー部になる。<br />
上記のHeaderに続いて、MP Configuration Table Entryが続くが、このEntryには種類があり、その種類によってエントリの内容はもちろん、エントリ長も変わってくることになる。またエントリの数も任意の数になる。<br />
MP Configuration Table EntriesはBase sectionとExtended sectionに別れていると仕様書に記載があるが、今回の実装ではBase sectionの話しか出てこないので後者は省略する。</p>
<h4 id="mp-configuration-table-header"><a class="header" href="#mp-configuration-table-header">MP Configuration Table Header</a></h4>
<p>MP Configuration Table Headerは以下のフォーマットになっている</p>
<blockquote>
<img src="./04_figs/MP_Configuration_Table_Header.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-3. MP Configuration Table Header</p>
</blockquote>
<p>以下に、MP Configuration Table Headerの一部のフィールドについて記載する。</p>
<table><thead><tr><th>Field</th><th>Offset<br>(in bytes:bits)</th><th>Length<br>(in bits)</th><th>Description</th></tr></thead><tbody>
<tr><td>Signature</td><td>0</td><td>32</td><td>テーブルの存在を示す <code>PCMP</code> というASCII文字列</td></tr>
<tr><td>Base Table Length</td><td>4</td><td>16</td><td>Configuration tableのバイト長</td></tr>
<tr><td>Spec Rev</td><td>6</td><td>8</td><td>MP Specのバージョン</td></tr>
<tr><td>Checksum</td><td>7</td><td>8</td><td>Base configuration table全体のchecksum</td></tr>
<tr><td>OEM ID</td><td>8</td><td>64</td><td>システムハードウェアの製造会社を示す文字列</td></tr>
<tr><td>PRODUCT ID</td><td>16</td><td>96</td><td>プロダクトファミリを示す文字列</td></tr>
<tr><td>Address of Local APIC</td><td>36</td><td>32</td><td>各CPUがLAPICにアクセスする際のベースアドレス</td></tr>
</tbody></table>
<p>Linux Kernelではこのheaderは<code>mpc_table</code>という構造体に対応しているようである。<br />
Base Table Lengthに格納する値は、その後に続くConfiguration Tableのバイト長さになるが、これは前述したとおりエントリの種類と数が可変のため注意が必要である。ToyVMMの実装としてはこれらのエントリを作成したあとに、エントリ長の情報を埋めたこのHeaderをメモリ上に書き込む実装にしている。<br />
また、Specの「3.1 System Memory Configuration」の節にLocal APICのデフォルトのベースアドレスは<code>0xFEE0_0000</code>と記載があるためこれに従って設定する。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    let mut mpc_table = mpspec::mpc_table {
        signature: MPC_SIGNATURE, // PCMP
        length: table_end.unchecked_offset_from(table_base) as u16,
        spec: MPC_SPEC, // 4
        oem: MPC_OEM, // TOYVMM
        productid: MPC_PRODUCT_ID, // 0
        lapic: APIC_DEFAULT_PHYS_BASE, // 0xfee0_0000
        ..Default::default()
    };
    checksum = checksum.wrapping_add(compute_checksum(&amp;mpc_table));
    mpc_table.checksum = (!checksum).wrapping_add(1) as i8;
    // write mpc_table to guest memory
    ...
}
<span class="boring">}
</span></code></pre></pre>
<h4 id="base-mp-configuration-table-entries"><a class="header" href="#base-mp-configuration-table-entries">Base MP Configuration Table Entries</a></h4>
<p>上記のHeaderに続いて、MP Configuration Table Entryが続くが、このEntryには種類があり、その種類によってエントリの内容はもちろん、エントリ長も変わってくることになる。またエントリの数も任意の数になる。</p>
<p>具体的に、MP Configuration Table Entryには以下の種別が存在する</p>
<table><thead><tr><th>Entry Description</th><th>Entry Type Code</th><th>Length</th><th>Commnets</th></tr></thead><tbody>
<tr><td>Processor</td><td>0</td><td>20</td><td>One entry per processor</td></tr>
<tr><td>Bus</td><td>1</td><td>8</td><td>One entry per bus</td></tr>
<tr><td>I/O APIC</td><td>2</td><td>8</td><td>One entry per I/O APIC</td></tr>
<tr><td>I/O Interrupt Assignment</td><td>3</td><td>8</td><td>One entry per bus interrupt source</td></tr>
<tr><td>Local Interrupt Assignment</td><td>4</td><td>8</td><td>One entry per system interrupt source</td></tr>
</tbody></table>
<p>以降では、それぞれのEntryについての詳細を以下に記載していく。</p>
<p><strong>Processor Entry</strong></p>
<p>Processor entryのフォーマットは以下のような形になっている</p>
<blockquote>
<img src="./04_figs/Processor_Entry.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-4. Processor Entry</p>
</blockquote>
<p>以下に、Processor Entryのフィールドについて記載する</p>
<table><thead><tr><th>Field</th><th>Offset<br>(in bytes:bits)</th><th>Length<br>(in bits)</th><th>Description</th></tr></thead><tbody>
<tr><td>ENTRY TYPE</td><td>0</td><td>8</td><td>Processorエントリを示す0を入れる</td></tr>
<tr><td>LOCAL APIC ID</td><td>1</td><td>8</td><td>そのプロセッサのLocal APIC ID</td></tr>
<tr><td>LOCAL APIC Version</td><td>2</td><td>8</td><td>APIC version register(Bits: 0-7)</td></tr>
<tr><td>CPU Flags:EN</td><td>3:0</td><td>1</td><td>0の場合はプロセッサを使わない</td></tr>
<tr><td>CPU Flags:BP</td><td>3:1</td><td>1</td><td>Bootstrapプロセッサとして設定</td></tr>
<tr><td>CPU Signature STEPPING</td><td>4:0</td><td>4</td><td>SpecのTable 4-5を参照されたい</td></tr>
<tr><td>CPU Signature MODEL</td><td>4:4</td><td>4</td><td>SpecのTable 4-5を参照されたい</td></tr>
<tr><td>CPU Signature FAMILY</td><td>5:0</td><td>4</td><td>SpecのTable 4-5を参照されたい</td></tr>
<tr><td>FEATURE FLAGS</td><td>8</td><td>32</td><td>CPUの機能定義フラグ<br>SpecのTable 4-6を参照されたい</td></tr>
</tbody></table>
<p>Descriptionに記載している通り、CPUの詳細な情報についてはSpecの4-5, 4-6を参照されたい。
Linux KernelではこのEntryは<code>mpc_cpu</code>という構造体に対応しているようである。</p>
<p>ToyVMMのマルチプロセッサ対応にあたって、この<code>mpc_cpu</code>のエントリはCPUの個数分作ることになる。
Local APIC IDにはそのCPUの番号と同じ番号を割り当てることとした。
最初のCPUのセットアップ時のみCPU Flags:BPを立て、そのCPUをBootstrap Processor(BSP)として登録する。
CPU SigunatureとしてはSpecのTable4-5ではFeature processor向けになっているReservedな値を設定しておくことにする。
また、CPU Feature FlagとしてはFPU（On-chip Floating Point Unit）とAPIC（On-chip APIC）を有効にしておく。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    let size = mem::size_of::&lt;mpspec::mpc_cpu&gt;() as u64;
    for cpu_id in 0..num_cpus {
        let mpc_cpu = mpspec::mpc_cpu {
            type_: mpspec::MP_PROCESSOR as u8, // MP_PROCESSOR = 0
            apicid: cpu_id,
            apicver: APIC_VERSION, // 0x14=0b0001_0100
            cpuflag: mpspec::CPU_ENABLED as u8 // CPU_ENABLED = 1
                | if cpu_id == 0 {
                    mpspec::CPU_BOOTPROCESSOR as u8 // CPU_BOOTPROCESSOR = 2
                } else {
                    0
                },
            cpufeature: CPU_STEPPING, // 0b0110_0000_0000
            featureflag: CPU_FEATURE_APIC | CPU_FEATURE_FPU, // 0b0010_0000_0001
            ..Default::default()
        };
        // write mpc_cpu to guest memory
        ...
    }
}
<span class="boring">}
</span></code></pre></pre>
<p><strong>Bus Entry</strong></p>
<p>Bus entryのフォーマットは以下のような形になっている</p>
<blockquote>
<img src="./04_figs/Bus_Entry.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-5. Bus Entry</p>
</blockquote>
<p>以下に、Bus Entryのフィールドについて記載する</p>
<table><thead><tr><th>Field</th><th>Offset<br>(in bytes:bits)</th><th>Length<br>(in bits)</th><th>Description</th></tr></thead><tbody>
<tr><td>ENTRY TYPE</td><td>0</td><td>8</td><td>Busエントリを示す1を入れる</td></tr>
<tr><td>Bus ID</td><td>1</td><td>8</td><td>Bus ID (Bus毎にユニークな値)</td></tr>
<tr><td>Bus Type String</td><td>2</td><td>48</td><td>Busの種類を示す文字列(Table 4-8)</td></tr>
</tbody></table>
<p>Descriptionに記載している通り、Busの種類についてはSpecの4-8を参照されたい。
Linux KernelではこのEntryは<code>mpc_bus</code>という構造体に対応しているようである。
今回、BusはISAの種類のバスを1本のみ準備することした。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    let size = mem::size_of::&lt;mpspec::mpc_bus&gt;() as u64;
    let mpc_bus = mpspec::mpc_bus {
        type_: mpspec::MP_BUS as u8,
        busid: 0,
        bustype: BUS_TYPE_ISA,
    };
    // write mpc_bus to guest memory
    ...
}
<span class="boring">}
</span></code></pre></pre>
<p><strong>I/O APIC Entry</strong></p>
<p>I/O APIC entryのフォーマットは以下のような形になっている</p>
<blockquote>
<img src="./04_figs/IO_APIC_Entry.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-6. I/O APIC Entry</p>
</blockquote>
<p>以下に、I/O APIC Entryのフィールドについて記載する</p>
<table><thead><tr><th>Field</th><th>Offset<br>(in bytes:bits)</th><th>Length<br>(in bits)</th><th>Description</th></tr></thead><tbody>
<tr><td>ENTRY TYPE</td><td>0</td><td>8</td><td>I/O APIC エントリを示す2を入れる</td></tr>
<tr><td>I/O APIC ID</td><td>1</td><td>8</td><td>このI/O APICのID</td></tr>
<tr><td>I/O APIC Version #</td><td>2</td><td>8</td><td>I/O APICのバージョンレジスタのBit (0-7)</td></tr>
<tr><td>I/O APIC FLAGS: EN</td><td>3:0</td><td>1</td><td>0の場合、このI/O APICは利用されない</td></tr>
<tr><td>I/O APIC Address</td><td>4</td><td>32</td><td>このI/O APICのベースアドレス</td></tr>
</tbody></table>
<p>Linux KernelではこのEntryは<code>mpc_ioapic</code>という構造体に対応しているようである。
I/O APIC IDとしては、CPU Entryを作成するときにLAPICに割り振ったIDと被らないように、ID = Num of CPU + 1 とした。
Specの「3.1 System Memory Configuration」の節にI/O APICのデフォルトのベースアドレスは<code>0xFEC0_0000</code>と記載があるためこれに従って設定する。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    let size = mem::size_of::&lt;mpspec::mpc_ioapic&gt;() as u64;
    let mpc_ioapic = mpspec::mpc_ioapic {
        type_: mpspec::MP_IOAPIC as u8,
        apicid: ioapicid,
        apicver: APIC_VERSION, // 0x14=0b0001_0100
        flags: mpspec::MPC_APIC_USABLE as u8, // MPC_APIC_USABLE = 1
        apicaddr: IO_APIC_DEFAULT_PHYS_BASE, // 0xfec0_0000
    };
    // write mpc_ioapic to guest memory
    ...
}
<span class="boring">}
</span></code></pre></pre>
<p><strong>I/O Interrupt Assignment Entry</strong></p>
<p>このエントリは、各I/O APICの割り込み入力に接続されている割り込みソースを示す。
接続されているI/O APICの割り込み入力毎に1つのエントリが存在している。</p>
<blockquote>
<img src="./04_figs/IO_Interrupt_Assignment_Entry.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-7. I/O Interrupt Entry</p>
</blockquote>
<p>以下に、I/O Interrupt Assignment Entryのフィールドについて記載する</p>
<table><thead><tr><th>Field</th><th>Offset<br>(in bytes:bits)</th><th>Length<br>(in bits)</th><th>Description</th></tr></thead><tbody>
<tr><td>ENTRY TYPE</td><td>0</td><td>8</td><td>I/O interrupt エントリを示す3を入れる</td></tr>
<tr><td>Interrupt Type</td><td>1</td><td>8</td><td>以下のInterrupt Type ValueのTableを参照</td></tr>
<tr><td>I/O Interrupt Flag: PO</td><td>2:0</td><td>2</td><td>Polarity of APIC I/O input signals(詳細はSpecを参照)</td></tr>
<tr><td>I/O Interrupt Flag: EL</td><td>2:2</td><td>2</td><td>Trigger of APIC I/O input signals(詳細はSpecを参照)</td></tr>
<tr><td>Source Bus ID</td><td>4</td><td>32</td><td>割り込み信号が来るバスのID</td></tr>
<tr><td>Source Bus IRQ</td><td>4</td><td>32</td><td>Source busからの割り込み信号の識別子</td></tr>
<tr><td>Dest I/O APIC ID</td><td>4</td><td>32</td><td>信号が接続されているI/O APICを識別する</td></tr>
<tr><td>Dest I/O APIC INTIN#</td><td>4</td><td>32</td><td>信号が接続されているINTIN #n ピンを識別する</td></tr>
</tbody></table>
<p>また、Interrupt Typeに指定する、Interrupt Type Valueは以下のテーブルである。</p>
<table><thead><tr><th>Interrupt Type</th><th>Description</th><th>Comments</th></tr></thead><tbody>
<tr><td>0</td><td>INT</td><td>信号はベクタ割り込みである。<br>ベクタはAPICのredirection tableから供給される</td></tr>
<tr><td>1</td><td>NMI</td><td>信号はNon Maskable割り込みである</td></tr>
<tr><td>2</td><td>SMI</td><td>信号はSystem Management割り込みである</td></tr>
<tr><td>3</td><td>ExtINT</td><td>信号はベクタ割り込みである。<br>ベクタはexternal PICから供給される</td></tr>
</tbody></table>
<p>Linux KernelではこのEntryは<code>mpc_intsrc</code>という構造体に対応しているようである。
ここではIRQ 0-23の全てを、そのままのIRQ番号で対応付けるよな設定をI/O APICに対して実施している。
BusとしてはBus Entryで設定したバス番号0（ISAバスとして設定しているもの）を指定し、I/O APICの番号もI/O APIC Entryで設定したものを指定している。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>for i in 0..=u8::try_from(IRQ_MAX).map_err(|_| MptableError::TooManyIrqs)? { // IRQ_MAX = 23
    let size = mem::size_of::&lt;mpspec::mpc_intsrc&gt;() as u64;
    let mpc_intsrc = mpspec::mpc_intsrc {
        type_: mpspec::MP_INTSRC as u8,
        irqtype: mpspec::mp_irq_source_types_mp_INT as u8, // Interrupt Type = 0
        irqflag: mpspec::MP_IRQPOL_DEFAULT as u16, // irqflag = 0
        srcbus: 0,
        srcbusirq: i,
        dstapic: ioapicid,
        dstirq: i,
    };
    // write mpc_intsrc to guest memory
    ...
}
<span class="boring">}
</span></code></pre></pre>
<p><strong>Local Interrupt Assignment Entry</strong></p>
<p>このエントリは、各Local APICのローカル割り込み入力に接続されている割り込みソースを示す。</p>
<blockquote>
<img src="./04_figs/Local_Interrupt_Assignment_Entry.png">
<p>Reference: Intel MultiProcessor Specification v1.4 - Figure 4-8. Local Interrupt Entry</p>
</blockquote>
<p>以下に、Local Interrupt Assignment Entryのフィールドについて記載する</p>
<p>| Field                  | Offset<br>(in bytes:bits) | Length | Description                               |
| Field                  | Offset<br>(in bytes:bits) | Length<br>(in bits) | Description |
|------------------------|--------|--------|--------------------------------------------------------------|
| ENTRY TYPE             | 0      | 8      | Local interrupt エントリを示す4を入れる                      |
| Interrupt Type         | 1      | 8      | 前述したInterrupt Type ValueのTableを参照                    |
| I/O Interrupt Flag: PO | 2:0    | 2      | Polarity of APIC I/O local input signals（詳細はSpecを参照)  |
| I/O Interrupt Flag: EL | 2:2    | 2      | Trigger of APIC I/O local input signals                      |
| Source Bus ID          | 4      | 32     | 割り込み信号が来るバスのID                                   |
| Source Bus IRQ         | 4      | 32     | Source busからの割り込み信号の識別子                         |
| Dest Local APIC ID     | 4      | 32     | 信号が接続されているlocal APICを識別する。<br>0xFFの場合シグナルは全てのlocal APICsに接続される |
| Dest Local APIC INTIN# | 4      | 32     | 信号が接続されているINTIN #n ピンを識別する                  |</p>
<p>Linux KernelではこのEntryは<code>mpc_lintsrc</code>という構造体に対応しているようである。
FirecrackerやCrosVMではここのエントリを以下のように設定していたが、正直この設定については意図が掴みきれていないため今の自分の知識では説明できない。
もし詳細がわかったら追記することとするが、逆に詳しい人はIssueなどで教えてほしい。
実際のところ、このエントリ部分を消してもに問題なくOSは起動するし、複数プロセッサは利用できる。ただ、割り込みに関する設定なので確認が不十分である節は否めない。
ToyVMMではFirecrecker、CrosVMに習ってここは同じ実装を入れておくことにした。</p>
<pre><pre class="playground"><code class="language-rust">
<span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>{
    let size = mem::size_of::&lt;mpspec::mpc_lintsrc&gt;() as u64;
    let mpc_lintsrc = mpspec::mpc_lintsrc {
        type_: mpspec::MP_LINTSRC as u8,
        irqtype: mpspec::mp_irq_source_types_mp_ExtINT as u8,
        irqflag: mpspec::MP_IRQPOL_DEFAULT as u16,
        srcbusid: 0,
        srcbusirq: 0,
        destapic: 0,
        destapiclint: 0,
    };
    // write mpc_intsrc to guest memory
    ...
}
{
    let size = mem::size_of::&lt;mpspec::mpc_lintsrc&gt;() as u64;
    let mpc_lintsrc = mpspec::mpc_lintsrc {
        type_: mpspec::MP_LINTSRC as u8,
        irqtype: mpspec::mp_irq_source_types_mp_NMI as u8,
        irqflag: mpspec::MP_IRQPOL_DEFAULT as u16,
        srcbusid: 0,
        srcbusirq: 0,
        destapic: 0xFF,
        destapiclint: 1,
    };
    // write mpc_intsrc to guest memory
    ...
}
<span class="boring">}
</span></code></pre></pre>
<h3 id="toyvmm-implementation"><a class="header" href="#toyvmm-implementation">ToyVMM Implementation</a></h3>
<p>ToyVMMにおけるMPTableの実装は <code>src/vmm/src/arch/x86_64/mptable.rs</code> に存在している。
このコードを読み解くのは難しくはない。<code>setup_mptable</code>という関数の中でSpecificationにしたがってゲストメモリ上にMPTableの構造を作成していっているだけである。
更に、それぞれのエントリの内容についてはすでに上記で見てきたとおりであり、それを適切なアドレス位置に書き込んでいるだけなのでここでは特に説明することはないだろう。</p>
<p>加えて、各CPU毎にセットアップを実施したり、各CPU毎に<code>KVM_RUN</code>を実行するThreadを生成する必要があるがこれも軽微な修正なので特にここでは言及しない。</p>
<p>実際に上記の実装を終え、ToyVMMの起動時に渡す設定ファイルのCPUの数を増やすと、増やし多分のCPUを利用できるようになった。</p>
<pre><code>$ cat examples/vm_config.json
{
        &quot;boot-source&quot;: {
                &quot;kernel_path&quot;: &quot;./vmlinux.bin&quot;,
                &quot;boot_args&quot;: &quot;console=ttyS0 reboot=k panic=1&quot;
        },
        &quot;drives&quot;: [
                {
                        &quot;path_on_host&quot;: &quot;./ubuntu-18.04.ext4&quot;,
                        &quot;is_root_device&quot;: true
                }
        ],
        &quot;machine-config&quot;: {
                &quot;vcpu_count&quot;: 2,
                &quot;mem_size_mib&quot;: 2048,
                &quot;track_dirty_page&quot;: false
        }
}

# VM起動
$ sudo ./build/release/toyvmm vm run --config examples/vm_config.json

# dmesgを抜粋するとMPTableを読み込んで複数CPUがセットアップできているのを確認できる。
[    0.000000] Intel MultiProcessor Specification v1.4
[    0.000000] MPTABLE: OEM ID: TOYVMM
[    0.000000] MPTABLE: Product ID: 000000000000
[    0.000000] MPTABLE: APIC at: 0xFEE00000
[    0.000000] Processor #0 (Bootup-CPU)
[    0.000000] Processor #1
[    0.000000] IOAPIC[0]: apic_id 3, version 17, address 0xfec00000, GSI 0-23
[    0.000000] Processors: 2
[    0.000000] smpboot: Allowing 2 CPUs, 0 hotplug CPUs

# VM起動後、内部で複数CPUが確認できる
root@7e47bb8f2f0a:~# cat /proc/cpuinfo | grep processor
processor       : 0
processor       : 1
</code></pre>
<h2 id="references-5"><a class="header" href="#references-5">References</a></h2>
<ul>
<li><a href="https://blog.bobuhiro11.net/2021/11-25-gokvm3.html">KVMを使った自作VMMのSMP対応</a></li>
<li><a href="https://pdos.csail.mit.edu/6.828/2008/readings/ia32/MPspec.pdf">Intel MultiProcessor Specification v1.4</a></li>
<li><a href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html">Intel 64 and IA-32 Architectures Software Developer's Manual Combined Volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D, and 4</a></li>
<li><a href="https://mmi.hatenablog.com/entry/2017/03/27/202656">Local APICについて</a></li>
<li><a href="https://mmi.hatenablog.com/entry/2017/04/09/132708">I/O APICについて</a></li>
<li><a href="https://www.valinux.co.jp/technologylibrary/document/linux/interrupts0001/">Linux / x86_64の割り込み処理 第1回 割り込みコントローラ</a></li>
<li><a href="https://events19.linuxfoundation.org/wp-content/uploads/2017/11/A-Major-Overhaul-of-the-APIC-Initialization-and-Vector-Allocation-in-Linux-Kernel-OSS-Dou-Liyang.pdf">Kernel Interrupt: A Major Overhaul</a></li>
<li><a href="https://ja.wikipedia.org/wiki/%E5%89%B2%E3%82%8A%E8%BE%BC%E3%81%BF_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF)">割り込み(コンピュータ)</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="common-cpu-configuration"><a class="header" href="#common-cpu-configuration">Common CPU Configuration</a></h1>
<p>This content will be added and revised as appropriate.</p>
<p>Please check the <a href="./04_smp.html">04. SMP</a> for details.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="intel-cpu-specific-configuration"><a class="header" href="#intel-cpu-specific-configuration">Intel CPU specific configuration</a></h1>
<p>This content will be added and revised as appropriate.</p>
<p>Please check the <a href="./04_smp.html">04. SMP</a> for details.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="amd-cpu-specific-configuration"><a class="header" href="#amd-cpu-specific-configuration">AMD CPU specific configuration</a></h1>
<p>This content will be added and revised as appropriate.</p>
<p>Please check the <a href="./04_smp.html">04. SMP</a> for details.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
